"""
æ–‡ç« ç›¸å…³ API ç«¯ç‚¹
"""
import json
import logging
from datetime import datetime, timedelta
from typing import Optional, List

from fastapi import APIRouter, Depends, HTTPException, Query, Body
from sqlalchemy.orm import Session

from backend.app.db.repositories import ArticleRepository
from backend.app.db.models import Article
from backend.app.core.dependencies import get_database
from backend.app.api.v1.endpoints.settings import require_auth
from backend.app.schemas.article import (
    Article as ArticleSchema,
    ArticleListResponse,
    ArticleUpdate,
)
from backend.app.utils import create_ai_analyzer

logger = logging.getLogger(__name__)

router = APIRouter()


def _normalize_summary(summary_value) -> str:
    """è§„èŒƒåŒ– summary å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
    if isinstance(summary_value, dict):
        return json.dumps(summary_value, ensure_ascii=False)
    elif not isinstance(summary_value, str):
        return str(summary_value) if summary_value else ""
    return summary_value


def _update_article_analysis(article: Article, analysis_result: dict) -> None:
    """æ›´æ–°æ–‡ç« çš„åˆ†æç»“æœ"""
    article.importance = analysis_result.get("importance")
    article.tags = analysis_result.get("tags", [])
    article.target_audience = analysis_result.get("target_audience")
    
    # ä¿å­˜ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœAIåˆ†æè¿”å›äº†title_zhï¼‰
    if analysis_result.get("title_zh"):
        article.title_zh = analysis_result.get("title_zh")
    
    # è§„èŒƒåŒ– summary å­—æ®µ
    article.summary = _normalize_summary(analysis_result.get("summary", ""))
    article.is_processed = True
    article.updated_at = datetime.now()


def _get_article_or_404(db: Session, article_id: int) -> Article:
    """è·å–æ–‡ç« ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æŠ›å‡º404å¼‚å¸¸"""
    article = db.query(Article).filter(Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="æ–‡ç« ä¸å­˜åœ¨")
    return article


def _parse_time_range(time_range: str) -> Optional[datetime]:
    """è§£ææ—¶é—´èŒƒå›´å­—ç¬¦ä¸²"""
    now = datetime.now()
    time_range_map = {
        "ä»Šå¤©": lambda: now.replace(hour=0, minute=0, second=0, microsecond=0),
        "æœ€è¿‘3å¤©": lambda: now - timedelta(days=3),
        "æœ€è¿‘7å¤©": lambda: now - timedelta(days=7),
        "æœ€è¿‘30å¤©": lambda: now - timedelta(days=30),
        "å…¨éƒ¨": lambda: None,
    }
    parser = time_range_map.get(time_range)
    return parser() if parser else None


@router.get("", response_model=ArticleListResponse)
async def get_articles(
    time_range: Optional[str] = Query(None, description="æ—¶é—´èŒƒå›´"),
    sources: Optional[str] = Query(None, description="æ¥æºåˆ—è¡¨ï¼Œé€—å·åˆ†éš”"),
    exclude_sources: Optional[str] = Query(None, description="æ’é™¤çš„æ¥æºåˆ—è¡¨ï¼Œé€—å·åˆ†éš”"),
    importance: Optional[str] = Query(None, description="é‡è¦æ€§åˆ—è¡¨ï¼Œé€—å·åˆ†éš”"),
    category: Optional[str] = Query(None, description="åˆ†ç±»åˆ—è¡¨ï¼Œé€—å·åˆ†éš”"),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    include_details: bool = Query(False, description="æ˜¯å¦åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼ˆæ‘˜è¦ã€å†…å®¹ã€ä¸»é¢˜ã€æ ‡ç­¾ç­‰ï¼‰"),
    db: Session = Depends(get_database),
):
    """è·å–æ–‡ç« åˆ—è¡¨ï¼ˆæ”¯æŒç­›é€‰å’Œåˆ†é¡µï¼‰
    
    é»˜è®¤åªè¿”å›æ ‡é¢˜è¡Œæ˜¾ç¤ºæ‰€éœ€çš„åŸºæœ¬å­—æ®µï¼Œä¸è¿”å›è¯¦ç»†ä¿¡æ¯ä»¥èŠ‚çœç½‘ç»œæµé‡ã€‚
    è¯¦ç»†ä¿¡æ¯åŒ…æ‹¬ï¼šauthor, summary, content, tags, user_notesç­‰ã€‚
    ä»…åœ¨éœ€è¦æ—¶è®¾ç½®include_details=Trueï¼Œæˆ–ä½¿ç”¨/articles/{id}/fieldsæ¥å£æŒ‰éœ€è·å–ã€‚
    """
    # è§£æç­›é€‰å‚æ•°
    time_threshold = _parse_time_range(time_range) if time_range else None
    sources_list = sources.split(",") if sources else None
    exclude_sources_list = exclude_sources.split(",") if exclude_sources else None
    importance_list = importance.split(",") if importance else None
    category_list = category.split(",") if category else None
    
    # å¤„ç†"æœªåˆ†æ"é‡è¦æ€§
    include_unimportance = False
    if importance_list and "æœªåˆ†æ" in importance_list:
        include_unimportance = True
        importance_list = [i for i in importance_list if i != "æœªåˆ†æ"]
    
    # è·å–æ–‡ç« æ€»æ•°ï¼ˆç”¨äºåˆ†é¡µï¼‰
    articles_query = db.query(Article)
    if time_threshold:
        articles_query = articles_query.filter(Article.published_at >= time_threshold)
    if sources_list:
        articles_query = articles_query.filter(Article.source.in_(sources_list))
    if exclude_sources_list:
        articles_query = articles_query.filter(~Article.source.in_(exclude_sources_list))
    if importance_list or include_unimportance:
        if include_unimportance:
            if importance_list:
                articles_query = articles_query.filter(
                    (Article.importance.in_(importance_list)) | (Article.importance == None)
                )
            else:
                articles_query = articles_query.filter(Article.importance == None)
        else:
            articles_query = articles_query.filter(Article.importance.in_(importance_list))
    if category_list:
        articles_query = articles_query.filter(Article.category.in_(category_list))
    
    total = articles_query.count()
    
    # è·å–åˆ†é¡µæ•°æ®
    articles = (
        articles_query
        .order_by(Article.published_at.desc())
        .offset((page - 1) * page_size)
        .limit(page_size)
        .all()
    )
    
    # è½¬æ¢ä¸º Pydantic æ¨¡å‹
    article_schemas = []
    for article in articles:
        # åˆ›å»ºæ–‡ç« å¯¹è±¡
        article_data = ArticleSchema.model_validate(article)
        # å¦‚æœä¸åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼Œæ¸…ç©ºæ‰€æœ‰è¯¦ç»†å­—æ®µ
        if not include_details:
            update_data = {
                'content': None,
                'summary': None,
                'author': None,
                'tags': None,
                'user_notes': None,
                'target_audience': None,
            }
            article_data = article_data.model_copy(update=update_data)
        
        article_schemas.append(article_data)
    
    total_pages = (total + page_size - 1) // page_size
    
    return ArticleListResponse(
        items=article_schemas,
        total=total,
        page=page,
        page_size=page_size,
        total_pages=total_pages,
    )


@router.get("/{article_id}", response_model=ArticleSchema)
async def get_article(
    article_id: int,
    db: Session = Depends(get_database),
):
    """è·å–æ–‡ç« è¯¦æƒ…"""
    article = _get_article_or_404(db, article_id)
    return ArticleSchema.model_validate(article)


@router.post("/batch/basic", response_model=List[ArticleSchema])
async def get_articles_basic(
    article_ids: List[int] = Body(..., description="æ–‡ç« IDåˆ—è¡¨"),
    db: Session = Depends(get_database),
):
    """
    æ‰¹é‡è·å–æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«è¯¦ç»†å­—æ®µï¼ŒèŠ‚çœæµé‡ï¼‰
    
    åªè¿”å›åŸºæœ¬å­—æ®µï¼šid, title, title_zh, url, source, source_id, category, 
    published_at, collected_at, importance, is_processed, is_sent, is_favorited, 
    created_at, updated_at
    
    ä¸è¿”å›è¯¦ç»†å­—æ®µï¼šcontent, summary, author, tags, user_notes, target_audienceç­‰
    """
    if not article_ids:
        return []
    
    articles = db.query(Article).filter(Article.id.in_(article_ids)).all()
    
    # è½¬æ¢ä¸º Pydantic æ¨¡å‹ï¼Œä½†æ¸…ç©ºè¯¦ç»†å­—æ®µ
    article_schemas = []
    for article in articles:
        article_data = ArticleSchema.model_validate(article)
        # æ¸…ç©ºæ‰€æœ‰è¯¦ç»†å­—æ®µ
        update_data = {
            'content': None,
            'summary': None,
            'author': None,
            'tags': None,
            'user_notes': None,
            'target_audience': None,
        }
        article_data = article_data.model_copy(update=update_data)
        article_schemas.append(article_data)
    
    return article_schemas


@router.get("/{article_id}/fields")
async def get_article_fields(
    article_id: int,
    fields: str = Query(..., description="è¦è·å–çš„å­—æ®µï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ï¼šsummary,content,tags,author,user_notes"),
    db: Session = Depends(get_database),
):
    """è·å–æ–‡ç« çš„ç‰¹å®šå­—æ®µï¼ˆç”¨äºæŒ‰éœ€åŠ è½½ï¼‰
    
    æ”¯æŒçš„å­—æ®µï¼š
    - summary: AIæ€»ç»“
    - content: æ–‡ç« å†…å®¹
    - author: ä½œè€…
    - tags: æ ‡ç­¾åˆ—è¡¨
    - user_notes: ç”¨æˆ·ç¬”è®°
    - target_audience: ç›®æ ‡å—ä¼—
    
    è¿”å›æ ¼å¼ï¼š{"summary": "...", "content": "...", "tags": [...], ...}
    
    ç‰¹æ®Šå€¼ "all" å¯ä»¥è·å–æ‰€æœ‰è¯¦ç»†å­—æ®µã€‚
    """
    article = _get_article_or_404(db, article_id)
    
    # å¦‚æœè¯·æ±‚æ‰€æœ‰å­—æ®µ
    if fields.strip().lower() == "all":
        return {
            "summary": article.summary,
            "content": article.content,
            "author": article.author,
            "tags": article.tags,
            "user_notes": article.user_notes,
            "target_audience": article.target_audience,
        }
    
    requested_fields = [f.strip() for f in fields.split(",")]
    result = {}
    
    # æ”¯æŒçš„å­—æ®µæ˜ å°„
    field_mapping = {
        "summary": lambda: article.summary,
        "content": lambda: article.content,
        "author": lambda: article.author,
        "tags": lambda: article.tags,
        "user_notes": lambda: article.user_notes,
        "target_audience": lambda: article.target_audience,
    }
    
    for field in requested_fields:
        if field in field_mapping:
            result[field] = field_mapping[field]()
        else:
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„å­—æ®µ: {field}ã€‚æ”¯æŒçš„å­—æ®µï¼š{', '.join(field_mapping.keys())}ï¼Œæˆ–ä½¿ç”¨ 'all' è·å–æ‰€æœ‰å­—æ®µ"
            )
    
    return result


@router.post("/{article_id}/analyze")
async def analyze_article(
    article_id: int,
    force: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†æï¼ˆå³ä½¿å·²åˆ†æè¿‡ï¼‰"),
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """è§¦å‘æ–‡ç« AIåˆ†æï¼ˆæ”¯æŒé‡æ–°åˆ†æï¼‰
    
    æ³¨æ„ï¼šå¯¹äºé‚®ä»¶ç±»å‹çš„æ–‡ç« ï¼Œä¸ä¼šé‡æ–°é‡‡é›†å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨å·²ä¿å­˜çš„å†…å®¹è¿›è¡ŒAIåˆ†æã€‚
    å…¶ä»–ç±»å‹çš„æ–‡ç« ä¹Ÿä¸ä¼šé‡æ–°é‡‡é›†ï¼Œåªå¯¹å·²ä¿å­˜çš„å†…å®¹é‡æ–°è¿›è¡ŒAIåˆ†æã€‚
    """
    article = _get_article_or_404(db, article_id)
    
    # å¦‚æœå·²åˆ†æä¸”æœªå¼ºåˆ¶é‡æ–°åˆ†æï¼Œè¿”å›æç¤ºä¿¡æ¯
    was_processed = article.is_processed
    if was_processed and not force:
        return {
            "message": "æ–‡ç« å·²åˆ†æï¼Œå¦‚éœ€é‡æ–°åˆ†æè¯·ä½¿ç”¨ force=true å‚æ•°",
            "article_id": article_id,
            "is_processed": True,
        }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé‚®ä»¶ç±»å‹
    is_email = article.category == "email" or article.url.startswith("mailto:")
    
    # å¯¹äºé‚®ä»¶ç±»å‹ï¼Œç¡®ä¿ä¸é‡æ–°é‡‡é›†ï¼Œç›´æ¥ä½¿ç”¨å·²ä¿å­˜çš„å†…å®¹
    # å¯¹äºå…¶ä»–ç±»å‹ï¼Œä¹Ÿä½¿ç”¨å·²ä¿å­˜çš„å†…å®¹ï¼ˆä¸é‡æ–°é‡‡é›†ï¼‰
    if is_email:
        logger.info(f"ğŸ“§ é‚®ä»¶ç±»å‹æ–‡ç« ï¼Œä½¿ç”¨å·²ä¿å­˜çš„å†…å®¹è¿›è¡Œé‡æ–°åˆ†æï¼ˆä¸é‡æ–°é‡‡é›†ï¼‰")
    
    # æ£€æŸ¥å†…å®¹æ˜¯å¦å­˜åœ¨
    if not article.content:
        raise HTTPException(
            status_code=400, 
            detail="æ–‡ç« å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚é‚®ä»¶ç±»å‹æ–‡ç« ä¸ä¼šé‡æ–°é‡‡é›†å†…å®¹ï¼Œè¯·ç¡®ä¿æ–‡ç« å·²æœ‰å†…å®¹ã€‚"
        )
    
    # åˆ›å»ºAIåˆ†æå™¨
    ai_analyzer = create_ai_analyzer()
    if not ai_analyzer:
        raise HTTPException(status_code=400, detail="æœªé…ç½®AIåˆ†æå™¨")
    
    try:
        # è·å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæºé…ç½®äº†ï¼‰
        custom_prompt = None
        if article.source:
            from backend.app.db.models import RSSSource
            source_obj = db.query(RSSSource).filter(
                RSSSource.name == article.source
            ).first()
            if source_obj and source_obj.analysis_prompt:
                custom_prompt = source_obj.analysis_prompt
        
        # æ‰§è¡ŒAIåˆ†æï¼ˆä½¿ç”¨å·²ä¿å­˜çš„å†…å®¹ï¼Œä¸é‡æ–°é‡‡é›†ï¼‰
        analysis_result = ai_analyzer.analyze_article(
            title=article.title,
            content=article.content or "",
            url=article.url,
            source=article.source,
            category=article.category,
            custom_prompt=custom_prompt,
        )
        
        # æ›´æ–°æ–‡ç« åˆ†æç»“æœ
        _update_article_analysis(article, analysis_result)
        
        db.commit()
        
        return {
            "message": "é‡æ–°åˆ†æå®Œæˆ" if was_processed else "åˆ†æå®Œæˆ",
            "article_id": article_id,
            "analysis": analysis_result,
            "is_processed": True,
        }
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@router.delete("/{article_id}")
async def delete_article(
    article_id: int,
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """åˆ é™¤æ–‡ç« """
    success = ArticleRepository.delete_article(db, article_id)
    if not success:
        raise HTTPException(status_code=404, detail="æ–‡ç« ä¸å­˜åœ¨")
    return {"message": "æ–‡ç« å·²åˆ é™¤", "article_id": article_id}


@router.post("/{article_id}/favorite")
async def favorite_article(
    article_id: int,
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """æ”¶è—æ–‡ç« """
    article = _get_article_or_404(db, article_id)
    
    article.is_favorited = True
    article.updated_at = datetime.now()
    db.commit()
    
    return {"message": "æ–‡ç« å·²æ”¶è—", "article_id": article_id, "is_favorited": True}


@router.delete("/{article_id}/favorite")
async def unfavorite_article(
    article_id: int,
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """å–æ¶ˆæ”¶è—æ–‡ç« """
    article = _get_article_or_404(db, article_id)
    
    article.is_favorited = False
    article.updated_at = datetime.now()
    db.commit()
    
    return {"message": "å·²å–æ¶ˆæ”¶è—", "article_id": article_id, "is_favorited": False}


@router.put("/{article_id}", response_model=ArticleSchema)
async def update_article(
    article_id: int,
    article_update: ArticleUpdate,
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """æ›´æ–°æ–‡ç« """
    article = _get_article_or_404(db, article_id)
    
    # æ›´æ–°å­—æ®µï¼ˆåªæ›´æ–°æä¾›çš„å­—æ®µï¼‰
    update_data = article_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(article, field, value)
    
    article.updated_at = datetime.now()
    db.commit()
    db.refresh(article)
    
    return ArticleSchema.model_validate(article)


@router.post("/collect", response_model=ArticleSchema)
async def collect_article_from_url(
    url: str = Query(..., description="è¦é‡‡é›†çš„æ–‡ç« URL"),
    db: Session = Depends(get_database),
    _current_user: str = Depends(require_auth),
):
    """ä»URLæ‰‹åŠ¨é‡‡é›†æ–‡ç« ï¼Œå¹¶ç«‹å³è¿›è¡ŒAIåˆ†æ"""
    from backend.app.services.collector.web_collector import WebCollector
    from urllib.parse import urlparse
    
    # éªŒè¯URLæ ¼å¼
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„URLæ ¼å¼")
    except Exception:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„URLæ ¼å¼")
    
    # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
    existing_article = db.query(Article).filter(Article.url == url).first()
    if existing_article:
        raise HTTPException(status_code=409, detail=f"æ–‡ç« å·²å­˜åœ¨ (ID: {existing_article.id})")
    
    # ä½¿ç”¨WebCollectoré‡‡é›†æ–‡ç« 
    collector = WebCollector()
    article_data = collector.fetch_single_article(url)
    
    if not article_data:
        raise HTTPException(status_code=500, detail="é‡‡é›†æ–‡ç« å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®")
    
    # è·å–å½“å‰æ—¶é—´ä½œä¸ºé‡‡é›†æ—¶é—´
    now = datetime.now()
    
    # ä¼˜å…ˆä½¿ç”¨è§£æå‡ºçš„å‘å¸ƒæ—¶é—´ï¼Œå¦‚æœè§£æä¸å‡ºæ¥åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
    published_at = article_data.get("published_at")
    if not published_at:
        published_at = now
    
    # ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“
    try:
        new_article = Article(
            title=article_data.get("title", url),
            url=article_data.get("url", url),
            content=article_data.get("content", ""),
            source=article_data.get("source", "æ‰‹åŠ¨é‡‡é›†-webé¡µé¢"),
            category=article_data.get("category", "æ‰‹åŠ¨é‡‡é›†-webé¡µé¢"),
            author=article_data.get("author"),  # å¦‚æœè§£æä¸å‡ºæ¥å°±æ˜¯ None
            published_at=published_at,  # ä¼˜å…ˆä½¿ç”¨è§£æå‡ºçš„æ—¥æœŸï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
            collected_at=now,
        )
        
        db.add(new_article)
        db.flush()  # åˆ·æ–°ä»¥è·å–IDï¼Œä½†ä¸æäº¤
        
        # ç«‹å³è¿›è¡ŒAIåˆ†æ
        ai_analyzer = create_ai_analyzer()
        if ai_analyzer:
            try:
                # è·å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæºé…ç½®äº†ï¼‰
                custom_prompt = None
                if new_article.source:
                    from backend.app.db.models import RSSSource
                    source_obj = db.query(RSSSource).filter(
                        RSSSource.name == new_article.source
                    ).first()
                    if source_obj and source_obj.analysis_prompt:
                        custom_prompt = source_obj.analysis_prompt
                
                analysis_result = ai_analyzer.analyze_article(
                    title=new_article.title,
                    content=new_article.content or "",
                    url=new_article.url,
                    custom_prompt=custom_prompt,
                )
                
                # æ›´æ–°æ–‡ç« åˆ†æç»“æœ
                _update_article_analysis(new_article, analysis_result)
            except Exception as e:
                # AIåˆ†æå¤±è´¥ä¸å½±å“æ–‡ç« ä¿å­˜ï¼Œåªè®°å½•é”™è¯¯
                logger.warning(f"âš ï¸  AIåˆ†æå¤±è´¥: {e}")
                new_article.is_processed = False
        else:
            # å¦‚æœæ²¡æœ‰é…ç½®AIåˆ†æå™¨ï¼Œæ ‡è®°ä¸ºæœªåˆ†æ
            new_article.is_processed = False
        
        db.commit()
        db.refresh(new_article)
        
        return ArticleSchema.model_validate(new_article)
    except Exception as e:
        db.rollback()
        if "UNIQUE constraint failed" in str(e):
            # å¹¶å‘æƒ…å†µä¸‹å¯èƒ½å·²å­˜åœ¨
            existing = db.query(Article).filter(Article.url == url).first()
            if existing:
                return ArticleSchema.model_validate(existing)
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æ–‡ç« å¤±è´¥: {str(e)}")

