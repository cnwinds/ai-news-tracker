"""
é€šç”¨ç½‘é¡µé‡‡é›†å™¨
æ”¯æŒé€šè¿‡CSSé€‰æ‹©å™¨é…ç½®æ–‡ç« æå–è§„åˆ™
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
import re

from backend.app.services.collector.base_collector import BaseCollector

logger = logging.getLogger(__name__)


class WebCollector(BaseCollector):
    """é€šç”¨ç½‘é¡µé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»ç½‘é¡µè·å–æ–‡ç« 

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«:
                - url: ç½‘ç«™URL
                - name: æºåç§°
                - article_selector: æ–‡ç« åˆ—è¡¨çš„CSSé€‰æ‹©å™¨
                - title_selector: æ ‡é¢˜çš„CSSé€‰æ‹©å™¨
                - link_selector: é“¾æ¥çš„CSSé€‰æ‹©å™¨
                - date_selector: æ—¥æœŸçš„CSSé€‰æ‹©å™¨
                - content_selector: å†…å®¹çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - author_selector: ä½œè€…çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - max_articles: æœ€å¤§æ–‡ç« æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        url = config.get("url")
        name = config.get("name", "Unknown")
        article_selector = config.get("article_selector")
        max_articles = config.get("max_articles", 20)

        if not url or not article_selector:
            logger.error(f"âŒ {name}: ç¼ºå°‘å¿…è¦çš„é…ç½® (url æˆ– article_selector)")
            return []

        try:
            logger.info(f"ğŸŒ æ­£åœ¨è·å–ç½‘é¡µ: {url}")

            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            articles = []
            article_elements = soup.select(article_selector)

            for i, element in enumerate(article_elements[:max_articles]):
                article = self._parse_article_element(element, config, name)
                if article:
                    # å¦‚æœé…ç½®äº†ä»è¯¦æƒ…é¡µè·å–å®Œæ•´å†…å®¹ï¼Œåˆ™è®¿é—®è¯¦æƒ…é¡µ
                    if config.get("fetch_full_content") and article.get("url"):
                        full_data = self._fetch_article_details(article["url"], config)
                        if full_data:
                            if full_data.get("content"):
                                article["content"] = full_data["content"]
                            if full_data.get("author") and not article.get("author"):
                                article["author"] = full_data["author"]
                            if full_data.get("published_at") and not article.get("published_at"):
                                article["published_at"] = full_data["published_at"]
                    articles.append(article)

            logger.info(f"âœ… {name}: æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æç½‘é¡µå¤±è´¥ {url}: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯Webé…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸

        Returns:
            (is_valid, error_message) å…ƒç»„
        """
        if not config.get("url"):
            return False, "Webé…ç½®ä¸­ç¼ºå°‘urlå­—æ®µ"
        if not config.get("article_selector"):
            return False, "Webé…ç½®ä¸­ç¼ºå°‘article_selectorå­—æ®µ"
        return True, None

    def _parse_article_element(self, element: Any, config: Dict[str, Any], source_name: str) -> Dict[str, Any]:
        """
        è§£æå•ä¸ªæ–‡ç« å…ƒç´ 

        Args:
            element: BeautifulSoupå…ƒç´ 
            config: é…ç½®å­—å…¸
            source_name: æºåç§°

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            title_selector = config.get("title_selector")
            link_selector = config.get("link_selector")
            date_selector = config.get("date_selector")
            content_selector = config.get("content_selector")
            description_selector = config.get("description_selector")
            author_selector = config.get("author_selector")

            title = ""
            url = ""
            published_at = None
            author = ""
            content = ""

            if title_selector:
                title_elem = element.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)

            if link_selector:
                if link_selector == "self":
                    url = element.get("href", "")
                else:
                    link_elem = element.select_one(link_selector)
                    if link_elem:
                        url = link_elem.get("href", "")
                if url and not url.startswith("http"):
                    base_url = config.get("url")
                    url = self._resolve_url(url, base_url)

            if date_selector:
                date_elem = element.select_one(date_selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    published_at = self._parse_date(date_text)

            if author_selector:
                author_elem = element.select_one(author_selector)
                if author_elem:
                    author = author_elem.get_text(strip=True)

            if content_selector:
                content_elem = element.select_one(content_selector)
                if content_elem:
                    content = self.html_to_markdown(str(content_elem))

            if description_selector and not content:
                desc_elem = element.select_one(description_selector)
                if desc_elem:
                    content = self.html_to_markdown(str(desc_elem))

            if not title or not url:
                logger.warning(f"âš ï¸  æ–‡ç« ç¼ºå°‘æ ‡é¢˜æˆ–URL: {title[:50] if title else 'N/A'}")
                return None

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source_name,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
            return None

    def _resolve_url(self, url: str, base_url: str) -> str:
        """
        è§£æç›¸å¯¹URLä¸ºç»å¯¹URL

        Args:
            url: å¯èƒ½æ˜¯ç›¸å¯¹çš„URL
            base_url: åŸºç¡€URL

        Returns:
            ç»å¯¹URL
        """
        if url.startswith("//"):
            return "https:" + url
        elif url.startswith("/"):
            from urllib.parse import urljoin
            return urljoin(base_url, url)
        else:
            return url

    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²

        Args:
            date_text: æ—¥æœŸæ–‡æœ¬

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        if not date_text:
            return None

        month_names = {
            "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
            "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12
        }

        date_patterns = [
            (r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}", "month_day_year"),
            r"\d{4}-\d{2}-\d{2}",
            r"\d{4}/\d{2}/\d{2}",
            r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",
            r"\d{2}-\d{2}-\d{4}",
            r"\d{2}/\d{2}/\d{4}",
        ]

        for pattern in date_patterns:
            if isinstance(pattern, tuple):
                pattern_str, format_type = pattern
            else:
                pattern_str = pattern
                format_type = "default"

            match = re.search(pattern_str, date_text)
            if match:
                date_str = match.group(0)
                try:
                    if format_type == "month_day_year":
                        parts = date_str.replace(",", "").split()
                        month = month_names.get(parts[0])
                        day = int(parts[1])
                        year = int(parts[2])
                        return datetime(year, month, day)
                    elif "å¹´" in date_str:
                        parts = re.split(r"[å¹´æœˆæ—¥]", date_str)
                        return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                    elif "-" in date_str:
                        parts = date_str.split("-")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                    elif "/" in date_str:
                        parts = date_str.split("/")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                except Exception:
                    continue

        return None

    def _fetch_article_details(self, url: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»è¯¦æƒ…é¡µè·å–æ–‡ç« çš„å®Œæ•´å†…å®¹ã€ä½œè€…å’Œæ—¥æœŸ

        Args:
            url: æ–‡ç« URL
            config: é…ç½®å­—å…¸

        Returns:
            åŒ…å« content, author, published_at çš„å­—å…¸
        """
        try:
            logger.debug(f"ğŸ“„ æ­£åœ¨è·å–è¯¦æƒ…é¡µå†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            result = {}

            # è·å–å†…å®¹
            content_selector = config.get("content_selector")
            if content_selector:
                content_elem = soup.select_one(content_selector)
                if content_elem:
                    result["content"] = self.html_to_markdown(str(content_elem))
            else:
                # ä½¿ç”¨é»˜è®¤é€‰æ‹©å™¨
                content_selectors = [
                    'article .entry-content',
                    'article',
                    '.article-content',
                    '.post-content',
                    '.entry-content',
                    '.content',
                    'main article',
                    '[role="article"]',
                    '.blog-post-content',
                ]

                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        result["content"] = self.html_to_markdown(str(elements[0]))
                        if len(result["content"]) > 500:
                            break

                if not result.get("content") or len(result.get("content", "")) < 500:
                    for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                        tag.decompose()
                    result["content"] = self.html_to_markdown(str(soup))

            # è·å–ä½œè€…
            author_selector = config.get("author_selector")
            if author_selector:
                author_elem = soup.select_one(author_selector)
                if author_elem:
                    result["author"] = author_elem.get_text(strip=True)

            # è·å–æ—¥æœŸï¼ˆå¦‚æœåˆ—è¡¨é¡µæ²¡æœ‰è·å–åˆ°ï¼‰
            date_selector = config.get("date_selector")
            if date_selector:
                date_elem = soup.select_one(date_selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    if date_text:
                        result["published_at"] = self._parse_date(date_text)
                # ä¹Ÿå°è¯•ä» time æ ‡ç­¾çš„ datetime å±æ€§è·å–
                if not result.get("published_at"):
                    time_elem = soup.select_one("time[datetime]")
                    if time_elem:
                        datetime_attr = time_elem.get("datetime")
                        if datetime_attr:
                            try:
                                result["published_at"] = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                            except:
                                pass

            return result

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–è¯¦æƒ…é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æè¯¦æƒ…é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return {}

    def _is_error_page(self, content: str, soup: BeautifulSoup) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢ï¼ˆå¦‚éœ€è¦JavaScriptã€è®¿é—®è¢«æ‹’ç»ç­‰ï¼‰

        Args:
            content: é¡µé¢æ–‡æœ¬å†…å®¹
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            å¦‚æœæ˜¯é”™è¯¯é¡µé¢è¿”å›True
        """
        # å¸¸è§é”™è¯¯é¡µé¢çš„ç‰¹å¾
        error_indicators = [
            "JavaScript is not available",
            "JavaScript is disabled",
            "Please enable JavaScript",
            "Enable JavaScript to continue",
            "Access Denied",
            "Something went wrong",
            "let's give it another shot",
            "privacy related extensions may cause issues",
        ]

        content_lower = content.lower()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯æç¤º
        for indicator in error_indicators:
            if indicator.lower() in content_lower:
                logger.warning(f"âš ï¸  æ£€æµ‹åˆ°é”™è¯¯é¡µé¢: '{indicator}'")
                return True

        # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().lower()
            for indicator in error_indicators:
                if indicator.lower() in title_text:
                    logger.warning(f"âš ï¸  é¡µé¢æ ‡é¢˜æ˜¾ç¤ºé”™è¯¯: '{indicator}'")
                    return True

        # æ£€æŸ¥é¡µé¢å†…å®¹è¿‡çŸ­ï¼ˆå¯èƒ½æ˜¯é”™è¯¯é¡µé¢ï¼‰
        if len(content.strip()) < 200:
            logger.warning(f"âš ï¸  é¡µé¢å†…å®¹è¿‡çŸ­ ({len(content.strip())} å­—ç¬¦)ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢")
            return True

        return False

    def fetch_full_content(self, url: str) -> str:
        """
        è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

        Args:
            url: æ–‡ç« URL

        Returns:
            å®Œæ•´å†…å®¹æ–‡æœ¬
        """
        try:
            result = self._fetch_article_details(url, {})

            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
            content = result.get("content", "")
            if content:
                # ä½¿ç”¨BeautifulSoupæ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
                soup = BeautifulSoup(content, "html.parser") if "<html" in content.lower() or "<body" in content.lower() else None
                if soup:
                    page_text = soup.get_text()
                    if self._is_error_page(page_text, soup):
                        logger.warning(f"âš ï¸  URLè¿”å›é”™è¯¯é¡µé¢ï¼Œè·³è¿‡: {url}")
                        return ""

            return content
        except Exception as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""

    def fetch_single_article(self, url: str) -> Optional[Dict[str, Any]]:
        """
        æ™ºèƒ½æå–å•ä¸ªURLçš„æ–‡ç« å†…å®¹ï¼ˆç”¨äºæ‰‹åŠ¨é‡‡é›†ï¼‰

        Args:
            url: æ–‡ç« URL

        Returns:
            æ–‡ç« å­—å…¸ï¼ŒåŒ…å« title, url, content, author, published_at ç­‰
        """
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨é‡‡é›†æ–‡ç« : {url}")
            
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, "html.parser")
            
            # æå–æ ‡é¢˜
            title = ""
            title_selectors = [
                'h1.entry-title',
                'h1.post-title',
                'h1.article-title',
                'article h1',
                'main h1',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    # æ ‡é¢˜é€šå¸¸æ˜¯çº¯æ–‡æœ¬ï¼Œä¸éœ€è¦Markdownè½¬æ¢
                    title = title_elem.get_text(strip=True)
                    if title and len(title) > 5:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                        break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»titleæ ‡ç­¾è·å–
            if not title or len(title) < 5:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    # ç§»é™¤å¸¸è§çš„åç¼€ï¼ˆå¦‚ " - Site Name"ï¼‰
                    title = re.sub(r'\s*[-|]\s*.*$', '', title)
            
            if not title:
                title = url  # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨URLä½œä¸ºæ ‡é¢˜
            
            # æå–å†…å®¹
            content = ""
            content_selectors = [
                'article .entry-content',
                'article .post-content',
                'article .article-content',
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main article',
                '[role="article"]',
                '.blog-post-content',
                '.post-body',
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = self.html_to_markdown(str(elements[0]))
                    if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œå°è¯•ä»mainæ ‡ç­¾è·å–
            if not content or len(content) < 500:
                main_elem = soup.find('main')
                if main_elem:
                    # ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰
                    for tag in main_elem.find_all(['nav', 'aside', 'script', 'style', 'header', 'footer']):
                        tag.decompose()
                    content = self.html_to_markdown(str(main_elem))
            
            # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œå°è¯•ä»bodyè·å–ï¼ˆç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼‰
            if not content or len(content) < 500:
                for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style', 'noscript']):
                    tag.decompose()
                body = soup.find('body')
                if body:
                    content = self.html_to_markdown(str(body))
            
            # æå–ä½œè€…
            author = ""
            author_selectors = [
                '.author',
                '.post-author',
                '.article-author',
                '[rel="author"]',
                '.by-author',
                'meta[name="author"]',
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    if selector.startswith('meta'):
                        author = author_elem.get('content', '')
                    else:
                        author = author_elem.get_text(strip=True)
                    if author:
                        break
            
            # æå–å‘å¸ƒæ—¥æœŸ
            published_at = None
            
            # å°è¯•ä»timeæ ‡ç­¾çš„datetimeå±æ€§è·å–
            time_elem = soup.select_one("time[datetime]")
            if time_elem:
                datetime_attr = time_elem.get("datetime")
                if datetime_attr:
                    try:
                        published_at = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                    except:
                        pass
            
            # å°è¯•ä»metaæ ‡ç­¾è·å–
            if not published_at:
                meta_date = soup.select_one('meta[property="article:published_time"]')
                if meta_date:
                    datetime_attr = meta_date.get("content")
                    if datetime_attr:
                        try:
                            published_at = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                        except:
                            pass
            
            # å°è¯•ä»å¸¸è§çš„é€‰æ‹©å™¨è·å–æ—¥æœŸæ–‡æœ¬
            if not published_at:
                date_selectors = [
                    '.published',
                    '.post-date',
                    '.article-date',
                    '.date',
                    'time',
                ]
                for selector in date_selectors:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        if date_text:
                            published_at = self._parse_date(date_text)
                            if published_at:
                                break
            
            logger.info(f"âœ… æˆåŠŸé‡‡é›†æ–‡ç« : {title[:50]}...")
            
            return {
                "title": title,
                "url": url,
                "content": content,
                "source": "æ‰‹åŠ¨é‡‡é›†-webé¡µé¢",
                "author": author if author else None,
                "published_at": published_at,
                "category": "æ‰‹åŠ¨é‡‡é›†-webé¡µé¢",
            }
            
        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å¤±è´¥ {url}: {e}")
            import traceback
            traceback.print_exc()
            return None

