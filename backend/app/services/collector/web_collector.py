"""
é€šç”¨ç½‘é¡µé‡‡é›†å™¨
æ”¯æŒé€šè¿‡CSSé€‰æ‹©å™¨é…ç½®æ–‡ç« æå–è§„åˆ™
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
import re

logger = logging.getLogger(__name__)


class WebCollector:
    """é€šç”¨ç½‘é¡µé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»ç½‘é¡µè·å–æ–‡ç« 

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«:
                - url: ç½‘ç«™URL
                - name: æºåç§°
                - article_selector: æ–‡ç« åˆ—è¡¨çš„CSSé€‰æ‹©å™¨
                - title_selector: æ ‡é¢˜çš„CSSé€‰æ‹©å™¨
                - link_selector: é“¾æ¥çš„CSSé€‰æ‹©å™¨
                - date_selector: æ—¥æœŸçš„CSSé€‰æ‹©å™¨
                - content_selector: å†…å®¹çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - author_selector: ä½œè€…çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - max_articles: æœ€å¤§æ–‡ç« æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        url = config.get("url")
        name = config.get("name", "Unknown")
        article_selector = config.get("article_selector")
        max_articles = config.get("max_articles", 20)

        if not url or not article_selector:
            logger.error(f"âŒ {name}: ç¼ºå°‘å¿…è¦çš„é…ç½® (url æˆ– article_selector)")
            return []

        try:
            logger.info(f"ğŸŒ æ­£åœ¨è·å–ç½‘é¡µ: {url}")

            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            articles = []
            article_elements = soup.select(article_selector)

            for i, element in enumerate(article_elements[:max_articles]):
                article = self._parse_article_element(element, config, name)
                if article:
                    # å¦‚æœé…ç½®äº†ä»è¯¦æƒ…é¡µè·å–å®Œæ•´å†…å®¹ï¼Œåˆ™è®¿é—®è¯¦æƒ…é¡µ
                    if config.get("fetch_full_content") and article.get("url"):
                        full_data = self._fetch_article_details(article["url"], config)
                        if full_data:
                            if full_data.get("content"):
                                article["content"] = full_data["content"]
                            if full_data.get("author") and not article.get("author"):
                                article["author"] = full_data["author"]
                            if full_data.get("published_at") and not article.get("published_at"):
                                article["published_at"] = full_data["published_at"]
                    articles.append(article)

            logger.info(f"âœ… {name}: æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æç½‘é¡µå¤±è´¥ {url}: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _parse_article_element(self, element: Any, config: Dict[str, Any], source_name: str) -> Dict[str, Any]:
        """
        è§£æå•ä¸ªæ–‡ç« å…ƒç´ 

        Args:
            element: BeautifulSoupå…ƒç´ 
            config: é…ç½®å­—å…¸
            source_name: æºåç§°

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            title_selector = config.get("title_selector")
            link_selector = config.get("link_selector")
            date_selector = config.get("date_selector")
            content_selector = config.get("content_selector")
            description_selector = config.get("description_selector")
            author_selector = config.get("author_selector")

            title = ""
            url = ""
            published_at = None
            author = ""
            content = ""

            if title_selector:
                title_elem = element.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)

            if link_selector:
                if link_selector == "self":
                    url = element.get("href", "")
                else:
                    link_elem = element.select_one(link_selector)
                    if link_elem:
                        url = link_elem.get("href", "")
                if url and not url.startswith("http"):
                    base_url = config.get("url")
                    url = self._resolve_url(url, base_url)

            if date_selector:
                date_elem = element.select_one(date_selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    published_at = self._parse_date(date_text)

            if author_selector:
                author_elem = element.select_one(author_selector)
                if author_elem:
                    author = author_elem.get_text(strip=True)

            if content_selector:
                content_elem = element.select_one(content_selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)

            if description_selector and not content:
                desc_elem = element.select_one(description_selector)
                if desc_elem:
                    content = desc_elem.get_text(strip=True)

            if not title or not url:
                logger.warning(f"âš ï¸  æ–‡ç« ç¼ºå°‘æ ‡é¢˜æˆ–URL: {title[:50] if title else 'N/A'}")
                return None

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source_name,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
            return None

    def _resolve_url(self, url: str, base_url: str) -> str:
        """
        è§£æç›¸å¯¹URLä¸ºç»å¯¹URL

        Args:
            url: å¯èƒ½æ˜¯ç›¸å¯¹çš„URL
            base_url: åŸºç¡€URL

        Returns:
            ç»å¯¹URL
        """
        if url.startswith("//"):
            return "https:" + url
        elif url.startswith("/"):
            from urllib.parse import urljoin
            return urljoin(base_url, url)
        else:
            return url

    def _parse_date(self, date_text: str) -> Optional[datetime]:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²

        Args:
            date_text: æ—¥æœŸæ–‡æœ¬

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        if not date_text:
            return None

        month_names = {
            "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
            "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12
        }

        date_patterns = [
            (r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}", "month_day_year"),
            r"\d{4}-\d{2}-\d{2}",
            r"\d{4}/\d{2}/\d{2}",
            r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",
            r"\d{2}-\d{2}-\d{4}",
            r"\d{2}/\d{2}/\d{4}",
        ]

        for pattern in date_patterns:
            if isinstance(pattern, tuple):
                pattern_str, format_type = pattern
            else:
                pattern_str = pattern
                format_type = "default"

            match = re.search(pattern_str, date_text)
            if match:
                date_str = match.group(0)
                try:
                    if format_type == "month_day_year":
                        parts = date_str.replace(",", "").split()
                        month = month_names.get(parts[0])
                        day = int(parts[1])
                        year = int(parts[2])
                        return datetime(year, month, day)
                    elif "å¹´" in date_str:
                        parts = re.split(r"[å¹´æœˆæ—¥]", date_str)
                        return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                    elif "-" in date_str:
                        parts = date_str.split("-")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                    elif "/" in date_str:
                        parts = date_str.split("/")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                except Exception:
                    continue

        return None

    def _fetch_article_details(self, url: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»è¯¦æƒ…é¡µè·å–æ–‡ç« çš„å®Œæ•´å†…å®¹ã€ä½œè€…å’Œæ—¥æœŸ

        Args:
            url: æ–‡ç« URL
            config: é…ç½®å­—å…¸

        Returns:
            åŒ…å« content, author, published_at çš„å­—å…¸
        """
        try:
            logger.debug(f"ğŸ“„ æ­£åœ¨è·å–è¯¦æƒ…é¡µå†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            result = {}

            # è·å–å†…å®¹
            content_selector = config.get("content_selector")
            if content_selector:
                content_elem = soup.select_one(content_selector)
                if content_elem:
                    result["content"] = content_elem.get_text(separator=" ", strip=True)
            else:
                # ä½¿ç”¨é»˜è®¤é€‰æ‹©å™¨
                content_selectors = [
                    'article .entry-content',
                    'article',
                    '.article-content',
                    '.post-content',
                    '.entry-content',
                    '.content',
                    'main article',
                    '[role="article"]',
                    '.blog-post-content',
                ]

                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        result["content"] = elements[0].get_text(separator=" ", strip=True)
                        if len(result["content"]) > 500:
                            break

                if not result.get("content") or len(result.get("content", "")) < 500:
                    for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                        tag.decompose()
                    result["content"] = soup.get_text(separator=" ", strip=True)

            if result.get("content"):
                result["content"] = " ".join(result["content"].split())

            # è·å–ä½œè€…
            author_selector = config.get("author_selector")
            if author_selector:
                author_elem = soup.select_one(author_selector)
                if author_elem:
                    result["author"] = author_elem.get_text(strip=True)

            # è·å–æ—¥æœŸï¼ˆå¦‚æœåˆ—è¡¨é¡µæ²¡æœ‰è·å–åˆ°ï¼‰
            date_selector = config.get("date_selector")
            if date_selector:
                date_elem = soup.select_one(date_selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    if date_text:
                        result["published_at"] = self._parse_date(date_text)
                # ä¹Ÿå°è¯•ä» time æ ‡ç­¾çš„ datetime å±æ€§è·å–
                if not result.get("published_at"):
                    time_elem = soup.select_one("time[datetime]")
                    if time_elem:
                        datetime_attr = time_elem.get("datetime")
                        if datetime_attr:
                            try:
                                result["published_at"] = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                            except:
                                pass

            return result

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–è¯¦æƒ…é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æè¯¦æƒ…é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return {}

    def fetch_full_content(self, url: str) -> str:
        """
        è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

        Args:
            url: æ–‡ç« URL

        Returns:
            å®Œæ•´å†…å®¹æ–‡æœ¬
        """
        try:
            result = self._fetch_article_details(url, {})
            return result.get("content", "")
        except Exception as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""

