"""
APIæ•°æ®é‡‡é›†å™¨ï¼ˆarXiv, Hugging Faceç­‰ï¼‰
"""
import requests
import feedparser
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import logging
import time

from backend.app.services.collector.base_collector import BaseCollector

logger = logging.getLogger(__name__)


class ArXivCollector(BaseCollector):
    """arXivè®ºæ–‡é‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "http://export.arxiv.org/api/query"
    
    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»arXivè·å–è®ºæ–‡ï¼ˆå®ç°BaseCollectoræ¥å£ï¼‰

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - query: æŸ¥è¯¢æ¡ä»¶ (å¦‚: cat:cs.AI)
                - max_results: æœ€å¤§ç»“æœæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤20ï¼‰

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        query = config.get("query")
        max_results = config.get("max_results", 20)
        
        if not query:
            raise ValueError("ArXivé…ç½®ä¸­ç¼ºå°‘queryå­—æ®µ")
        
        return self.fetch_papers(query, max_results)
    
    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯ArXivé…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸

        Returns:
            (is_valid, error_message) å…ƒç»„
        """
        if not config.get("query"):
            return False, "ArXivé…ç½®ä¸­ç¼ºå°‘queryå­—æ®µ"
        return True, None

    def fetch_papers(self, query: str, max_results: int = 20) -> List[Dict[str, Any]]:
        """
        ä»arXivè·å–è®ºæ–‡

        Args:
            query: æŸ¥è¯¢æ¡ä»¶ (å¦‚: cat:cs.AI)
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“š æ­£åœ¨è·å–arXivè®ºæ–‡: {query}")

            params = {
                "search_query": query,
                "start": 0,
                "max_results": max_results,
                "sortBy": "submittedDate",
                "sortOrder": "descending",
            }

            response = requests.get(self.base_url, params=params, timeout=self.timeout)
            response.raise_for_status()

            # è§£æAtom feed
            feed = feedparser.parse(response.content)

            papers = []
            for entry in feed.entries:
                paper = self._parse_arxiv_entry(entry)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡arXivè®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–arXivè®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_arxiv_entry(self, entry: Any) -> Dict[str, Any]:
        """è§£æarXivè®ºæ–‡æ¡ç›®"""
        try:
            # æå–ä½œè€…
            authors = ", ".join([author.name for author in entry.authors[:5]]) if hasattr(entry, "authors") else ""

            # æå–æ‘˜è¦ï¼ˆè®ºæ–‡æ‘˜è¦å†…å®¹ï¼‰
            summary = entry.get("summary", "")

            # arXiv ID
            entry_id = entry.get("id", "")
            arxiv_id = entry_id.split("/abs/")[-1] if "/abs/" in entry_id else ""
            
            # HTML é¡µé¢åœ°å€ï¼ˆç¡®ä¿æ˜¯ /abs/ æ ¼å¼ï¼‰
            html_url = entry_id
            if arxiv_id and "/abs/" not in html_url:
                # å¦‚æœ entry.id ä¸æ˜¯ /abs/ æ ¼å¼ï¼Œæ„å»º HTML URL
                html_url = f"http://arxiv.org/abs/{arxiv_id}"

            # PDFé“¾æ¥
            pdf_url = entry.get("link", "").replace("/abs/", "/pdf/") + ".pdf" if "/abs/" in entry.get("link", "") else ""
            if arxiv_id and not pdf_url:
                # å¦‚æœæ— æ³•ä» link è·å–ï¼Œæ ¹æ® arxiv_id æ„å»º PDF URL
                pdf_url = f"http://arxiv.org/pdf/{arxiv_id}.pdf"

            # å‘å¸ƒæ—¶é—´
            # feedparserè¿”å›çš„æ—¶é—´æ˜¯UTCæ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                # åˆ›å»ºUTCæ—¶é—´å¯¹è±¡
                utc_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
                local_tz = timezone(timedelta(hours=8))
                published_at = utc_time.astimezone(local_tz).replace(tzinfo=None)

            return {
                "title": entry.get("title", ""),
                "url": html_url,  # ä½¿ç”¨ HTML é¡µé¢åœ°å€ä½œä¸ºä¸» URL
                "content": summary,  # å°†è®ºæ–‡æ‘˜è¦ä½œä¸ºæ–‡ç« å†…å®¹ï¼Œç”¨äºåç»­AIåˆ†æå’Œä¸­æ–‡æ€»ç»“
                "source": "arXiv",
                "author": authors,
                "published_at": published_at,
                "category": "paper",
                "metadata": {
                    "arxiv_id": arxiv_id,
                    "html_url": html_url,  # æ˜ç¡®æ·»åŠ  HTML é¡µé¢åœ°å€
                    "pdf_url": pdf_url,
                    "primary_category": entry.get("arxiv_primary_category", {}).get("term", ""),
                    "categories": [tag.term for tag in entry.tags] if hasattr(entry, "tags") else [],
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æarXivè®ºæ–‡å¤±è´¥: {e}")
            return None


class HuggingFaceCollector(BaseCollector):
    """Hugging Faceè®ºæ–‡é‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "https://huggingface.co/api/papers"
    
    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–è¶‹åŠ¿è®ºæ–‡ï¼ˆå®ç°BaseCollectoræ¥å£ï¼‰

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - max_results: æœ€å¤§æ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤20ï¼‰

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        limit = config.get("max_results", 20)
        return self.fetch_trending_papers(limit)
    
    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯HuggingFaceé…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸

        Returns:
            (is_valid, error_message) å…ƒç»„
        """
        return True, None

    def fetch_trending_papers(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–è¶‹åŠ¿è®ºæ–‡

        Args:
            limit: æœ€å¤§æ•°é‡

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ”¥ æ­£åœ¨è·å–Hugging Faceè¶‹åŠ¿è®ºæ–‡")

            url = f"{self.base_url}/trending"
            params = {"limit": limit}

            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()

            data = response.json()

            papers = []
            for item in data:
                paper = self._parse_hf_paper(item)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡Hugging Faceè¶‹åŠ¿è®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–Hugging Faceè®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_hf_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æHugging Faceè®ºæ–‡"""
        try:
            # æå–ä½œè€…
            authors = item.get("authors", [])
            author_str = ", ".join(authors[:5]) + ("..." if len(authors) > 5 else "")

            # å‘å¸ƒæ—¶é—´
            # Hugging Face APIè¿”å›çš„æ—¶é—´æ˜¯UTCæ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
            published_at = None
            if item.get("publishedAt"):
                try:
                    # è§£æUTCæ—¶é—´
                    utc_time = datetime.fromisoformat(item["publishedAt"].replace("Z", "+00:00"))
                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
                    local_tz = timezone(timedelta(hours=8))
                    published_at = utc_time.astimezone(local_tz).replace(tzinfo=None)
                except:
                    pass

            return {
                "title": item.get("title", ""),
                "url": item.get("paperUrl", ""),
                "content": item.get("summary", item.get("abstract", "")),
                "source": "Hugging Face",
                "author": author_str,
                "published_at": published_at,
                "category": "paper",
                "metadata": {
                    "hf_id": item.get("id", ""),
                    "likes": item.get("likesCount", 0),
                    "models": item.get("models", []),
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æHFè®ºæ–‡å¤±è´¥: {e}")
            return None


class PapersWithCodeCollector(BaseCollector):
    """Papers with Codeé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "https://paperswithcode.com/api/v1"
    
    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–è¶‹åŠ¿è®ºæ–‡ï¼ˆå®ç°BaseCollectoræ¥å£ï¼‰

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - max_results: æœ€å¤§æ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤20ï¼‰

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        limit = config.get("max_results", 20)
        return self.fetch_trending_papers(limit)
    
    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯PapersWithCodeé…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸

        Returns:
            (is_valid, error_message) å…ƒç»„
        """
        return True, None

    def fetch_trending_papers(self, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–è¶‹åŠ¿è®ºæ–‡"""
        try:
            logger.info(f"ğŸ“ˆ æ­£åœ¨è·å–Papers with Codeè¶‹åŠ¿è®ºæ–‡")

            url = f"{self.base_url}/papers/"
            params = {"ordering": "-stars", "page_size": limit}

            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()

            data = response.json()

            papers = []
            for item in data.get("results", []):
                paper = self._parse_pwc_paper(item)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡Papers with Codeè®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–Papers with Codeå¤±è´¥: {e}")
            return []

    def _parse_pwc_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æPapers with Codeè®ºæ–‡"""
        try:
            # æå–ä½œè€…
            authors = item.get("authors", [])
            author_str = ", ".join([a["name"] for a in authors[:5]]) + ("..." if len(authors) > 5 else "")

            return {
                "title": item.get("title", ""),
                "url": f"https://paperswithcode.com/paper/{item['id']}",
                "content": item.get("abstract", ""),
                "source": "Papers with Code",
                "author": author_str,
                "published_at": None,
                "category": "paper",
                "metadata": {
                    "stars": item.get("stars", 0),
                    "tasks": [t["name"] for t in item.get("tasks", [])],
                    "methods": [m["name"] for m in item.get("methods", [])],
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æPWCè®ºæ–‡å¤±è´¥: {e}")
            return None
