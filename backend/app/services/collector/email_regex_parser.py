"""
åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„é‚®ä»¶è§£æå™¨
ç”¨äºè§£æTLDRç­‰æ–°é—»é‚®ä»¶ï¼Œæ›¿ä»£å¤§æ¨¡å‹åˆ†æ
"""
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class EmailRegexParser:
    """
    åŸºäºæ­£åˆ™çš„é‚®ä»¶è§£æå™¨
    ä½¿ç”¨å¯é…ç½®çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥æå–æ–‡ç« 
    """

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            config: è§£æé…ç½®ï¼ŒåŒ…å«æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å’Œæå–è§„åˆ™
        """
        self.config = config
        self.rules = config.get("regex_rules", {})

    def parse(self, content: str, content_type: str = "plain") -> List[Dict[str, Any]]:
        """
        è§£æé‚®ä»¶å†…å®¹ï¼Œæå–æ–‡ç« åˆ—è¡¨

        Args:
            content: é‚®ä»¶å†…å®¹ï¼ˆHTMLæˆ–çº¯æ–‡æœ¬ï¼‰
            content_type: å†…å®¹ç±»å‹ ("html" æˆ– "plain")

        Returns:
            æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡æ–‡ç« åŒ…å« title, url, content ç­‰å­—æ®µ
        """
        if content_type == "html":
            return self._parse_html(content)
        else:
            return self._parse_plain_text(content)

    def _parse_plain_text(self, content: str) -> List[Dict[str, Any]]:
        """è§£æçº¯æ–‡æœ¬æ ¼å¼çš„é‚®ä»¶"""
        articles = []

        # æ­¥éª¤1: æå–é“¾æ¥æ˜ å°„ï¼ˆä» Links: éƒ¨åˆ†ï¼‰
        link_map = self._extract_links_plain(content)
        logger.info(f"ğŸ“ æå–åˆ° {len(link_map)} ä¸ªé“¾æ¥æ˜ å°„")

        # æ­¥éª¤2: åˆ†å‰²æ–‡ç« å—
        article_blocks = self._split_article_blocks_plain(content)
        logger.info(f"ğŸ“¦ æ‰¾åˆ° {len(article_blocks)} ä¸ªæ–‡ç« å—")

        # æ­¥éª¤3: ä»æ¯ä¸ªæ–‡ç« å—ä¸­æå–æ–‡ç« ä¿¡æ¯
        for block in article_blocks:
            article = self._extract_article_from_block(block, link_map)
            if article:
                articles.append(article)

        logger.info(f"âœ… æˆåŠŸè§£æ {len(articles)} ç¯‡æ–‡ç« ")
        return articles

    def _parse_html(self, content: str) -> List[Dict[str, Any]]:
        """
        è§£æHTMLæ ¼å¼çš„é‚®ä»¶

        ç­–ç•¥ï¼š
        1. ç›´æ¥ä»HTMLä¸­æå–æ‰€æœ‰æ–‡ç« ï¼ˆæ ‡é¢˜ + URL + æ‘˜è¦ï¼‰
        2. ä¸ä¾èµ–çº¯æ–‡æœ¬ä¸­çš„å¼•ç”¨æ ‡è®°
        """
        articles = []

        try:
            soup = BeautifulSoup(content, 'html.parser')

            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()

            # æ­¥éª¤1: ä»HTMLä¸­ç›´æ¥æå–æ–‡ç« 
            articles = self._extract_articles_from_html(soup)
            logger.info(f"ğŸ”— ä»HTMLæå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

            if not articles:
                logger.warning("âš ï¸  HTMLä¸­æœªæ‰¾åˆ°æ–‡ç« ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬è§£æ")
                text = soup.get_text()
                return self._parse_plain_text(text)

        except Exception as e:
            logger.error(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            # å›é€€åˆ°çº¯æ–‡æœ¬è§£æ
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text()
            return self._parse_plain_text(text)

        return articles

    def _extract_articles_from_html(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """
        ç›´æ¥ä»HTMLä¸­æå–æ–‡ç« 

        TLDR HTMLç»“æ„ï¼š
        - æ–‡ç« æ ‡é¢˜åœ¨<span>æˆ–<a>æ ‡ç­¾ä¸­
        - æ ‡é¢˜åŒ…å« "minute read" æˆ– "(sponsor)" ç­‰æ ‡è¯†
        - URLåœ¨ç›¸é‚»çš„<a>æ ‡ç­¾ä¸­
        - æ–‡ç« å†…å®¹åœ¨æ ‡é¢˜åçš„<p>æˆ–<div>æ ‡ç­¾ä¸­

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = []

        try:
            # æ‰¾åˆ°æ‰€æœ‰<a>æ ‡ç­¾
            for a_tag in soup.find_all('a', href=True):
                href = a_tag.get('href')

                # æå–çœŸå®URL
                real_url = self._extract_real_url(href)
                if not real_url:
                    continue

                # è·å–é“¾æ¥æ–‡æœ¬
                link_text = a_tag.get_text(strip=True)

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                if not self._is_article_link(link_text, real_url):
                    continue

                # æ¸…ç†æ ‡é¢˜
                clean_title = self._clean_html_title(link_text)
                if not clean_title:
                    continue

                # æŸ¥æ‰¾æ–‡ç« å†…å®¹
                content = self._extract_article_content_from_html(a_tag)

                if not content:
                    logger.debug(f"âš ï¸  æœªæ‰¾åˆ°å†…å®¹: {clean_title[:50]}")
                    continue

                articles.append({
                    "title": clean_title,
                    "url": real_url,
                    "content": content,
                    "metadata": {
                        "ref_id": None,
                        "original_title_line": link_text
                    }
                })

                logger.debug(f"âœ… æå–æ–‡ç« : {clean_title[:50]}...")

        except Exception as e:
            logger.error(f"âŒ æå–HTMLæ–‡ç« å¤±è´¥: {e}")

        return articles

    def _extract_article_content_from_html(self, a_tag) -> str:
        """
        ä»HTMLä¸­æå–æ–‡ç« å†…å®¹

        TLDR HTMLç»“æ„ï¼š
        <span>
            <a href="..."><strong>æ ‡é¢˜ (minute read)</strong></a>
            <br><br>
            <span>æ–‡ç« å†…å®¹...</span>
        </span>

        ç­–ç•¥ï¼š
        1. æ‰¾åˆ°<a>æ ‡ç­¾çš„çˆ¶çº§<span>
        2. åœ¨çˆ¶çº§ä¸­æŸ¥æ‰¾é™¤<a>ä»¥å¤–çš„å…¶ä»–<span>å…ƒç´ 
        3. æå–æ–‡æœ¬å†…å®¹
        """
        content_parts = []

        try:
            # æ‰¾åˆ°<a>æ ‡ç­¾çš„çˆ¶çº§<span>å…ƒç´ 
            parent_span = a_tag.find_parent('span')
            if not parent_span:
                # å¦‚æœæ²¡æœ‰çˆ¶çº§<span>ï¼Œå°è¯•æŸ¥æ‰¾çˆ¶çº§<td>æˆ–<div>
                parent = a_tag.find_parent(['td', 'div', 'p'])
                if not parent:
                    return ""
            else:
                parent = parent_span

            # åœ¨çˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾æ‰€æœ‰å­å…ƒç´ 
            for child in parent.descendants:
                if child.name == 'span' and child != a_tag.parent:
                    # è·å–æ–‡æœ¬
                    text = child.get_text(strip=True)

                    # æ’é™¤åŒ…å«"minute read"çš„spanï¼ˆè¿™äº›æ˜¯æ ‡é¢˜spanï¼‰
                    if 'minute read' in text.lower():
                        continue

                    # æ’é™¤åŒ…å«<a>æ ‡ç­¾çš„spanï¼ˆå³æ ‡é¢˜æ‰€åœ¨çš„spanï¼‰
                    if child.find('a'):
                        continue

                    # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–åŒ…å«å¹¿å‘Šçš„æ–‡æœ¬
                    if text and len(text) > 20:
                        if not any(skip in text.lower() for skip in ['sponsored by', 'try now for free', 'apply here']):
                            content_parts.append(text)

            # å¦‚æœåœ¨spanä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾çˆ¶å…ƒç´ çš„æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
            if not content_parts:
                # è·å–çˆ¶å…ƒç´ çš„å®Œæ•´æ–‡æœ¬
                full_text = parent.get_text(strip=True)

                # ç§»é™¤æ ‡é¢˜éƒ¨åˆ†ï¼ˆå³<a>æ ‡ç­¾çš„æ–‡æœ¬ï¼‰
                title_text = a_tag.get_text(strip=True)
                if title_text in full_text:
                    content_text = full_text.replace(title_text, '', 1).strip()
                else:
                    content_text = full_text

                # æ¸…ç†å†…å®¹
                if content_text and len(content_text) > 20:
                    content_parts.append(content_text)

            # åˆå¹¶å†…å®¹
            content = ' '.join(content_parts)

            # æ¸…ç†å†…å®¹ï¼ˆç§»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰
            content = self._clean_content(content)

            # ç§»é™¤å†…å®¹å¼€å¤´çš„ "(X minute read)" ç­‰é˜…è¯»æ—¶é—´æ ‡è¯†
            content = re.sub(r'^\(\d+\s*minute\s+read\)\s*', '', content, flags=re.IGNORECASE)

            # å†æ¬¡ç§»é™¤æ ‡é¢˜ï¼ˆé˜²æ­¢æ ‡é¢˜é‡å¤å‡ºç°åœ¨å†…å®¹ä¸­ï¼‰
            title_text = a_tag.get_text(strip=True)
            clean_title = self._clean_html_title(title_text)
            if clean_title.lower() in content.lower():
                # ç§»é™¤æ ‡é¢˜ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                pattern = re.compile(re.escape(clean_title), re.IGNORECASE)
                content = pattern.sub('', content).strip()

            # æ¸…ç†å¼€å¤´çš„æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
            content = content.lstrip('.,;:-)').strip()

            return content

        except Exception as e:
            logger.debug(f"âš ï¸  æå–å†…å®¹å¤±è´¥: {e}")
            return ""

    def _extract_real_url(self, tracking_url: str) -> Optional[str]:
        """
        ä»tldr trackingé“¾æ¥ä¸­æå–çœŸå®URL

        tldrçš„trackingæ ¼å¼ï¼š
        https://tracking.tldrnewsletter.com/CL0/<encoded_url>/...

        Args:
            tracking_url: trackingé“¾æ¥

        Returns:
            çœŸå®URLï¼Œå¦‚æœä¸æ˜¯trackingé“¾æ¥åˆ™è¿”å›åŸURL
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯tldr trackingé“¾æ¥
            if 'tracking.tldrnewsletter.com' in tracking_url:
                # ä½¿ç”¨urllibè§£æ
                from urllib.parse import unquote

                # ä»è·¯å¾„ä¸­æå–ç¼–ç çš„URLéƒ¨åˆ†
                # æ ¼å¼: /CL0/encoded_url/...
                match = re.search(r'/CL0/([^/]+)', tracking_url)
                if match:
                    encoded_url = match.group(1)
                    # è§£ç URLï¼ˆ%2F -> / ç­‰ï¼‰
                    decoded = unquote(encoded_url)
                    # æ·»åŠ https://å‰ç¼€ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if not decoded.startswith('http'):
                        decoded = 'https://' + decoded
                    return decoded

            # å¦‚æœä¸æ˜¯trackingé“¾æ¥ï¼Œç›´æ¥è¿”å›
            if tracking_url.startswith('http'):
                return tracking_url

        except Exception as e:
            logger.debug(f"âš ï¸  URLè§£æå¤±è´¥: {e}")

        return None

    def _is_article_link(self, text: str, url: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªé“¾æ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥

        Args:
            text: é“¾æ¥æ–‡æœ¬
            url: é“¾æ¥URL

        Returns:
            æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
        """
        if not text or not url:
            return False

        # å¿…é¡»æ’é™¤çš„é“¾æ¥
        skip_patterns = [
            'sign up', 'advertise', 'view online', 'unsubscribe',
            'manage your', 'referral', 'jobs@', 'apply here',
            'create your own', 'track your'
        ]

        text_lower = text.lower()
        for pattern in skip_patterns:
            if pattern in text_lower:
                return False

        # æ–‡ç« é“¾æ¥ç‰¹å¾ï¼š
        # 1. åŒ…å« "minute read"
        # 2. æˆ–åŒ…å« "(sponsor)"
        # 3. æˆ–åŒ…å« "(github repo)"
        # 4. ä¸”URLæ˜¯å¤–éƒ¨é“¾æ¥ï¼ˆä¸æ˜¯tldrå†…éƒ¨é“¾æ¥ï¼‰
        if 'minute read' in text_lower:
            return True

        if '(sponsor)' in text_lower or '(github repo)' in text_lower:
            return True

        # æ£€æŸ¥URLæ˜¯å¦æ˜¯å¤–éƒ¨æ–‡ç« é“¾æ¥
        # æ’é™¤tldrå†…éƒ¨é“¾æ¥
        if 'tldr.tech' in url and 'manage' not in url:
            return False

        # å¦‚æœé“¾æ¥æ–‡æœ¬çœ‹èµ·æ¥åƒæ ‡é¢˜ï¼ˆå¤§éƒ¨åˆ†å¤§å†™ï¼Œè¶³å¤Ÿé•¿ï¼‰
        if len(text) > 15:
            upper_count = sum(1 for c in text if c.isupper())
            total_count = sum(1 for c in text if c.isalpha())
            if total_count > 0 and upper_count / total_count > 0.6:
                return True

        return False

    def _clean_html_title(self, title: str) -> str:
        """
        æ¸…ç†HTMLä¸­çš„æ–‡ç« æ ‡é¢˜

        Args:
            title: åŸå§‹æ ‡é¢˜

        Returns:
            æ¸…ç†åçš„æ ‡é¢˜
        """
        # ç§»é™¤ "(X minute read)"
        title = re.sub(r'\s*\(\d+\s*minute\s+read\)', '', title, flags=re.IGNORECASE)

        # ç§»é™¤ "(sponsor)"
        title = re.sub(r'\s*\(sponsor\)', '', title, flags=re.IGNORECASE)

        # ç§»é™¤ "(github repo)"
        title = re.sub(r'\s*\(github\s+repo\)', '', title, flags=re.IGNORECASE)

        return title.strip()

    def _extract_links_plain(self, content: str) -> Dict[str, str]:
        """
        ä»çº¯æ–‡æœ¬ä¸­æå–é“¾æ¥æ˜ å°„

        Links: éƒ¨åˆ†æ ¼å¼ï¼š
        Links:
        ------
        [1] https://example.com/article1
        [2] https://example.com/article2

        Args:
            content: é‚®ä»¶çº¯æ–‡æœ¬å†…å®¹

        Returns:
            {å¼•ç”¨ç¼–å·: URL} çš„æ˜ å°„å­—å…¸ï¼Œå¦‚ {"1": "https://..."}
        """
        link_map = {}

        # æŸ¥æ‰¾ Links: éƒ¨åˆ†
        links_section_pattern = self.rules.get(
            "links_section_pattern",
            r"Links:\n-+\n+(.*?)(?=\n\n\n|\nWant to|\. If you have|\Z)"
        )

        match = re.search(links_section_pattern, content, re.DOTALL)
        if not match:
            logger.warning("âš ï¸  æœªæ‰¾åˆ° Links: éƒ¨åˆ†")
            return link_map

        links_section = match.group(1)

        # æå–æ¯ä¸ªé“¾æ¥
        # æ ¼å¼: [æ•°å­—] URL
        link_pattern = self.rules.get(
            "link_pattern",
            r"\[(\d+)\]\s+(https?://[^\s\]]+)"
        )

        for match in re.finditer(link_pattern, links_section):
            ref_id = match.group(1)
            url = match.group(2)
            link_map[ref_id] = url

        return link_map

    def _split_article_blocks_plain(self, content: str) -> List[str]:
        """
        å°†çº¯æ–‡æœ¬å†…å®¹åˆ†å‰²æˆæ–‡ç« å—

        ç­–ç•¥ï¼š
        1. å…ˆç§»é™¤å¤´éƒ¨å’Œå°¾éƒ¨æ— å…³å†…å®¹
        2. æ ¹æ®æ–‡ç« æ ‡é¢˜æ¨¡å¼åˆ†å‰²

        Args:
            content: é‚®ä»¶çº¯æ–‡æœ¬å†…å®¹

        Returns:
            æ–‡ç« å—çš„åˆ—è¡¨
        """
        # æ­¥éª¤1: ç§»é™¤å¤´éƒ¨æ— å…³å†…å®¹
        header_patterns = self.rules.get("remove_headers", [])
        for pattern in header_patterns:
            content = re.sub(pattern, "", content, flags=re.DOTALL)

        # æ­¥éª¤2: ç§»é™¤å°¾éƒ¨æ— å…³å†…å®¹ï¼ˆå¹¿å‘Šã€æ¨èç­‰ï¼‰
        footer_patterns = self.rules.get("remove_footers", [])
        for pattern in footer_patterns:
            content = re.sub(pattern, "", content, flags=re.DOTALL)

        # æ­¥éª¤3: æ ¹æ®æ–‡ç« æ ‡é¢˜æ¨¡å¼åˆ†å‰²å†…å®¹
        # TLDRçš„æ–‡ç« æ ‡é¢˜æ ¼å¼ï¼šå…¨å¤§å†™ + (X MINUTE READ) + [æ•°å­—]
        title_pattern = self.rules.get(
            "article_title_pattern",
            r"^[A-Z][A-Z\s&\'\-]+(\(\d+\+?\s+MINUTE\s+READ\))\s+\[\d+\]"
        )

        # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« æ ‡é¢˜çš„ä½ç½®
        title_positions = []
        for match in re.finditer(title_pattern, content, re.MULTILINE):
            title_positions.append(match.start())

        if not title_positions:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« æ ‡é¢˜")
            return []

        # æ­¥éª¤4: æ ¹æ®æ ‡é¢˜ä½ç½®åˆ†å‰²æ–‡ç« å—
        article_blocks = []
        for i, pos in enumerate(title_positions):
            # å½“å‰æ–‡ç« çš„èµ·å§‹ä½ç½®
            start = pos

            # ä¸‹ä¸€ç¯‡æ–‡ç« çš„èµ·å§‹ä½ç½®ï¼ˆå¦‚æœæ˜¯æœ€åä¸€ç¯‡ï¼Œåˆ™åˆ°å†…å®¹ç»“å°¾ï¼‰
            end = title_positions[i + 1] if i + 1 < len(title_positions) else len(content)

            # æå–æ–‡ç« å—
            block = content[start:end].strip()
            article_blocks.append(block)

        return article_blocks

    def _extract_article_from_block(
        self,
        block: str,
        link_map: Dict[str, str]
    ) -> Optional[Dict[str, Any]]:
        """
        ä»æ–‡ç« å—ä¸­æå–æ–‡ç« ä¿¡æ¯

        Args:
            block: æ–‡ç« å—æ–‡æœ¬
            link_map: é“¾æ¥æ˜ å°„å­—å…¸

        Returns:
            æ–‡ç« å­—å…¸ï¼ŒåŒ…å« title, url, content ç­‰
        """
        try:
            # æ­¥éª¤1: æå–æ ‡é¢˜
            title_pattern = self.rules.get(
                "article_title_pattern",
                r"^[A-Z][A-Z\s&\'\-]+(\(\d+\+?\s+MINUTE\s+READ\))\s+\[\d+\]"
            )

            title_match = re.search(title_pattern, block, re.MULTILINE)
            if not title_match:
                logger.warning(f"âš ï¸  æ— æ³•æå–æ ‡é¢˜: {block[:50]}...")
                return None

            title_line = title_match.group(0).strip()

            # æå–å¼•ç”¨ç¼–å·
            ref_match = re.search(r"\[(\d+)\]$", title_line)
            ref_id = ref_match.group(1) if ref_match else None

            # è·å–çœŸå®URL
            url = link_map.get(ref_id, "") if ref_id else ""

            # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤ (X MINUTE READ) å’Œ [æ•°å­—]
            clean_title = title_line
            clean_title = re.sub(r"\(\d+\+?\s+MINUTE\s+READ\)", "", clean_title, flags=re.IGNORECASE)
            clean_title = re.sub(r"\s*\[\d+\]$", "", clean_title)
            clean_title = clean_title.strip()

            # æ­¥éª¤2: æå–å†…å®¹ï¼ˆæ ‡é¢˜ä¹‹åçš„æ‰€æœ‰æ–‡æœ¬ï¼‰
            # æ‰¾åˆ°æ ‡é¢˜è¡Œä¹‹åçš„å†…å®¹
            lines = block.split('\n')
            content_lines = []

            # è·³è¿‡æ ‡é¢˜è¡Œï¼Œæ”¶é›†å†…å®¹
            skip_title = True
            for line in lines:
                if skip_title:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
                    if re.match(title_pattern, line.strip()):
                        skip_title = False
                        continue
                else:
                    # æ”¶é›†å†…å®¹è¡Œ
                    stripped = line.strip()
                    if stripped:
                        content_lines.append(stripped)

            # åˆå¹¶å†…å®¹æ®µè½
            content = ' '.join(content_lines)

            # æ­¥éª¤3: æ¸…ç†å†…å®¹
            content = self._clean_content(content)

            if not content:
                logger.warning(f"âš ï¸  æ–‡ç« å†…å®¹ä¸ºç©º: {clean_title}")
                return None

            return {
                "title": clean_title,
                "url": url,
                "content": content,
                "metadata": {
                    "ref_id": ref_id,
                    "original_title_line": title_line
                }
            }

        except Exception as e:
            logger.error(f"âŒ æå–æ–‡ç« å¤±è´¥: {e}")
            return None

    def _clean_content(self, content: str) -> str:
        """
        æ¸…ç†æ–‡ç« å†…å®¹

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content)

        # ç§»é™¤ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚ "" ç­‰ç‰¹æ®Šç©ºç™½å­—ç¬¦ï¼‰
        content = re.sub(r'[\u200c\u200e\u200f\u00a0]+', ' ', content)

        # ç§»é™¤å¹¿å‘Šæ ‡è¯†
        ad_patterns = self.rules.get("ad_patterns", [])
        for pattern in ad_patterns:
            content = re.sub(pattern, "", content, flags=re.IGNORECASE)

        return content.strip()

    def _preprocess_lines(self, lines: List[str]) -> List[str]:
        """
        é¢„å¤„ç†è¡Œï¼Œåˆå¹¶è·¨è¡Œçš„æ ‡é¢˜

        æŸäº›æ ‡é¢˜å¯èƒ½è·¨å¤šè¡Œï¼Œå¦‚ï¼š
        CURSOR CEO BUILT A BROWSER USING AI, BUT DOES IT REALLY WORK? (5
        MINUTE READ) [15]

        åº”è¯¥åˆå¹¶ä¸ºï¼š
        CURSOR CEO BUILT A BROWSER USING AI, BUT DOES IT REALLY WORK? (5 MINUTE READ) [15]
        """
        processed = []
        i = 0

        while i < len(lines):
            current_line = lines[i].strip()

            # å¦‚æœå½“å‰è¡Œä¸åŒ…å« [æ•°å­—] ä½†çœ‹èµ·æ¥åƒæ ‡é¢˜çš„ä¸€éƒ¨åˆ†ï¼ˆå…¨å¤§å†™ï¼‰
            # ä¸”ä¸‹ä¸€è¡ŒåŒ…å« [æ•°å­—]ï¼Œåˆ™åˆå¹¶
            if (not re.search(r'\[\d+\]$', current_line) and
                current_line.isupper() and
                len(current_line) > 5 and
                i + 1 < len(lines)):

                next_line = lines[i + 1].strip()

                # å¦‚æœä¸‹ä¸€è¡ŒåŒ…å« [æ•°å­—] å¼•ç”¨ï¼Œåˆå¹¶ä¸¤è¡Œ
                if re.search(r'\[\d+\]$', next_line):
                    merged = f"{current_line} {next_line}"
                    processed.append(merged)
                    i += 2
                    continue

            # å¦åˆ™ç›´æ¥æ·»åŠ å½“å‰è¡Œ
            processed.append(current_line)
            i += 1

        return processed

    def _is_title_line(self, line: str) -> bool:
        """
        åˆ¤æ–­ä¸€è¡Œæ˜¯å¦æ˜¯æ–‡ç« æ ‡é¢˜

        æ ‡é¢˜ç‰¹å¾ï¼š
        1. åŒ…å« [æ•°å­—] å¼•ç”¨
        2. ä¸å¤ªçŸ­ï¼ˆ>10å­—ç¬¦ï¼‰
        3. å¤§éƒ¨åˆ†æ˜¯å¤§å†™å­—æ¯
        4. ä¸æ˜¯å¯¼èˆªã€åˆ†ç±»ç­‰
        """
        # å¿…é¡»åŒ…å« [æ•°å­—] å¼•ç”¨
        if not re.search(r'\[\d+\]$', line):
            return False

        # ç§»é™¤å¼•ç”¨æ ‡è®°
        title_part = re.sub(r'\s*\[\d+\]$', '', line).strip()

        # é•¿åº¦æ£€æŸ¥
        if len(title_part) < 10:
            return False

        # è¿‡æ»¤å¯¼èˆªå’Œåˆ†ç±»
        skip_patterns = [
            r'^Sign Up',
            r'^Advertise',
            r'^View Online',
            r'^TLDR DEV',
            r'^ARTICLES&TUTORIALS$',
            r'^OPINIONS&ADVICE$',
            r'^LAUNCHES&TOOLS$',
            r'^MISCELLANEOUS$',
            r'^QUICK LINKS$',
        ]

        for pattern in skip_patterns:
            if re.match(pattern, title_part, re.IGNORECASE):
                return False

        # æ£€æŸ¥å¤§å†™å­—æ¯æ¯”ä¾‹ï¼ˆè‡³å°‘50%æ˜¯å¤§å†™ï¼‰
        upper_count = sum(1 for c in title_part if c.isupper())
        total_count = sum(1 for c in title_part if c.isalpha())

        if total_count > 0 and upper_count / total_count < 0.5:
            return False

        return True

    def _count_article_lines(self, lines: List[str], title_index: int) -> int:
        """è®¡ç®—æ–‡ç« å ç”¨çš„è¡Œæ•°ï¼ˆç”¨äºè·³è¿‡å·²å¤„ç†çš„è¡Œï¼‰"""
        count = 1  # æ ‡é¢˜è¡Œ
        i = title_index + 1

        while i < len(lines):
            line = lines[i].strip()

            if not line:
                if count > 1:  # å·²ç»æ”¶é›†äº†å†…å®¹
                    break
                i += 1
                continue

            if self._is_title_line(line):
                break

            count += 1
            i += 1

        return count


class TLDREmailParser(EmailRegexParser):
    """
    TLDRé‚®ä»¶ä¸“ç”¨è§£æå™¨
    é¢„é…ç½®äº†TLDRé‚®ä»¶çš„æ­£åˆ™è§„åˆ™
    """

    def __init__(self):
        # TLDRé‚®ä»¶çš„é¢„é…ç½®è§„åˆ™
        config = {
            "regex_rules": {
                # Links: éƒ¨åˆ†çš„æ­£åˆ™æ¨¡å¼
                "links_section_pattern": r"Links:\n-+\n+(.*?)(?=\n\n\n|\nLove TLDR|\nWant to advertise|\nWant to work|\nIf you have|\Z)",

                # å•ä¸ªé“¾æ¥çš„æ­£åˆ™æ¨¡å¼
                "link_pattern": r"\[(\d+)\]\s+(https?://[^\s\]]+)",

                # æ–‡ç« æ ‡é¢˜çš„æ­£åˆ™æ¨¡å¼ - æ›´å®½æ¾çš„æ¨¡å¼
                "article_title_pattern": r"^\s+[A-Z][A-Z0-9\s&\'\-?:,/()+!?]+\s*\[\d+\]\s*",

                # éœ€è¦ä»å¤´éƒ¨ç§»é™¤çš„å†…å®¹
                "remove_headers": [
                    r"^.*?Sign Up.*?\n",  # å¯¼èˆªè¡Œ
                    r"^.*?Advertise.*?\n",
                    r"^.*?View Online.*?\n",
                    r"^\s*TLDR\s*\n",
                    r"^\s*TLDR DEV \d{4}-\d{2}-\d{2}\s*\n",
                    r"^\s*[ğŸ§‘â€ğŸ’»ğŸ§ ğŸš€ğŸâš¡]+\s*\n",
                    r"^\s*[A-Z][A-Z\s&\'\-]+\s*\n",  # åˆ†ç±»æ ‡é¢˜ï¼ˆå¦‚ "ARTICLES & TUTORIALS"ï¼‰
                ],

                # éœ€è¦ä»å°¾éƒ¨ç§»é™¤çš„å†…å®¹
                "remove_footers": [
                    r"\n\nLove TLDR.*?(?=\Z)",  # æ¨èéƒ¨åˆ†
                    r"\n\nWant to advertise.*?(?=\Z)",
                    r"\n\nWant to work.*?(?=\Z)",
                    r"\n\nIf you have any comments.*?(?=\Z)",
                    r"\n\nThanks for reading.*?(?=\Z)",
                    r"\n\nManage your subscriptions.*?(?=\Z)",
                    r"\n\nLinks:\n-+.*?(?=\Z)",  # Linkséƒ¨åˆ†ï¼ˆç”¨äºåˆ†å‰²ï¼Œä¸ç”¨äºæå–ï¼‰
                ],

                # å¹¿å‘Šæ¨¡å¼
                "ad_patterns": [
                    r"\(SPONSOR\)",
                    r"Sponsored by",
                ]
            }
        }

        super().__init__(config)

    def _parse_plain_text(self, content: str) -> List[Dict[str, Any]]:
        """é‡å†™çº¯æ–‡æœ¬è§£ææ–¹æ³•ï¼Œä½¿ç”¨æ›´å‡†ç¡®çš„ç­–ç•¥"""
        articles = []

        # æ­¥éª¤1: æå–é“¾æ¥æ˜ å°„
        link_map = self._extract_links_plain(content)
        logger.info(f"ğŸ“ æå–åˆ° {len(link_map)} ä¸ªé“¾æ¥")

        # æ­¥éª¤2: ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•æå–æ–‡ç« 
        # å…ˆå¤„ç†è·¨è¡Œæ ‡é¢˜ï¼šå°†è¿ç»­çš„å¤§å†™è¡Œåˆå¹¶
        lines = content.split('\n')
        processed_lines = self._preprocess_lines(lines)

        i = 0
        while i < len(processed_lines):
            line = processed_lines[i].strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
            # æ ‡é¢˜ç‰¹å¾ï¼šåŒ…å« [æ•°å­—] å¼•ç”¨ï¼Œä¸”å¤§å¤šæ•°æ–‡å­—æ˜¯å¤§å†™
            if self._is_title_line(line):
                # æå–æ–‡ç« ï¼ˆä½¿ç”¨åŸºç±»çš„_extract_article_from_blockæ–¹æ³•ï¼‰
                # é‡æ–°æ„å»ºæ–‡ç« å—
                title_match = re.search(r'^\s+[A-Z][A-Z0-9\s&\'\-?:,/()+!?]+\s*\[\d+\]\s*', line)
                if title_match:
                    # æ”¶é›†æ–‡ç« å†…å®¹
                    content_lines = []
                    j = i + 1
                    while j < len(processed_lines):
                        next_line = processed_lines[j].strip()
                        if not next_line or self._is_title_line(next_line):
                            break
                        content_lines.append(next_line)
                        j += 1

                    # æ„å»ºæ–‡ç« å—
                    block = line + '\n' + '\n'.join(content_lines)
                    article = self._extract_article_from_block(block, link_map)
                    if article:
                        articles.append(article)
                        i = j
                        continue

            i += 1

        logger.info(f"âœ… æˆåŠŸè§£æ {len(articles)} ç¯‡æ–‡ç« ")
        return articles


def get_parser(source_type: str, config: Optional[Dict[str, Any]] = None) -> EmailRegexParser:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®æºç±»å‹è¿”å›ç›¸åº”çš„è§£æå™¨

    Args:
        source_type: æºç±»å‹ï¼ˆå¦‚ "tldr", "generic"ï¼‰
        config: è‡ªå®šä¹‰é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        é‚®ä»¶è§£æå™¨å®ä¾‹
    """
    if source_type == "tldr":
        return TLDREmailParser()
    elif config:
        return EmailRegexParser(config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}ï¼Œæˆ–ç¼ºå°‘é…ç½®")
