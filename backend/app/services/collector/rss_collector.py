"""
RSSæ•°æ®é‡‡é›†å™¨
"""
import feedparser
import requests
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Tuple, Optional
import logging
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

from backend.app.services.collector.base_collector import BaseCollector

logger = logging.getLogger(__name__)


def _get_author_from_source(source_name: str = None, url: str = None) -> str:
    """
    æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…åç§°
    
    Args:
        source_name: è®¢é˜…æºåç§°
        url: æ–‡ç« URL
        
    Returns:
        ä½œè€…åç§°ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # æºåç§°åˆ°ä½œè€…çš„æ˜ å°„
    source_to_author = {
        "Paul Graham": "Paul Graham",
        "paulgraham.com": "Paul Graham",
        "Paul Graham's Essays": "Paul Graham",
    }
    
    # URLåˆ°ä½œè€…çš„æ˜ å°„
    url_to_author = {
        "paulgraham.com": "Paul Graham",
    }
    
    # é¦–å…ˆæ£€æŸ¥æºåç§°
    if source_name:
        # ç²¾ç¡®åŒ¹é…
        if source_name in source_to_author:
            return source_to_author[source_name]
        # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        for key, author in source_to_author.items():
            if key.lower() in source_name.lower():
                return author
    
    # ç„¶åæ£€æŸ¥URL
    if url:
        for key, author in url_to_author.items():
            if key in url.lower():
                return author
    
    return ""


class RSSCollector(BaseCollector):
    """RSSé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        # å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œç”¨äºç»•è¿‡ç®€å•çš„åçˆ¬è™«æ£€æµ‹
        self.default_headers = {
            "User-Agent": self.user_agent,
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.google.com/",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»RSSæºè·å–æ–‡ç« ï¼ˆå®ç°BaseCollectoræ¥å£ï¼‰

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - url: RSS feed URL
                - name: æºåç§°
                - max_articles: æœ€å¤§æ–‡ç« æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤20ï¼‰

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        url = config.get("url")
        source_name = config.get("name")
        max_articles = config.get("max_articles", 20)
        
        if not url:
            raise ValueError("RSSé…ç½®ä¸­ç¼ºå°‘urlå­—æ®µ")
        
        return self.fetch_feed(url, max_articles, source_name)
    
    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯RSSé…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é‡‡é›†é…ç½®å­—å…¸

        Returns:
            (is_valid, error_message) å…ƒç»„
        """
        if not config.get("url"):
            return False, "RSSé…ç½®ä¸­ç¼ºå°‘urlå­—æ®µ"
        return True, None
    
    def fetch_feed(self, url: str, max_articles: int = 20, source_name: str = None) -> List[Dict[str, Any]]:
        """
        ä»RSSæºè·å–æ–‡ç« ï¼ˆä¿ç•™åŸæœ‰æ¥å£ä»¥ä¿æŒå‘åå…¼å®¹ï¼‰

        Args:
            url: RSS feed URL
            max_articles: æœ€å¤§æ–‡ç« æ•°
            source_name: è®¢é˜…æºåç§°ï¼ˆå°†ç”¨ä½œæ–‡ç« çš„sourceå­—æ®µï¼‰

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}")

            # å‘é€è¯·æ±‚ï¼ˆä½¿ç”¨å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼‰
            # å¯¹äº RSSHub ç­‰å¯èƒ½éœ€è¦éªŒè¯çš„æœåŠ¡ï¼Œä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´
            headers = self.default_headers.copy()
            # å¦‚æœæ˜¯ RSSHubï¼Œæ·»åŠ ç‰¹å®šçš„ Referer
            if "rsshub.app" in url or "rsshub" in url.lower():
                headers["Referer"] = "https://rsshub.app/"
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # å¤„ç†å“åº”å†…å®¹ï¼ˆç¡®ä¿æ­£ç¡®è§£å‹ï¼‰
            content = response.content
            content_encoding = response.headers.get('Content-Encoding', '').lower()
            
            # å¦‚æœå“åº”æ˜¯Brotliå‹ç¼©ä½†requestsæ²¡æœ‰è‡ªåŠ¨è§£å‹ï¼Œæ‰‹åŠ¨è§£å‹
            if content_encoding == 'br':
                # æ£€æŸ¥å†…å®¹æ˜¯å¦çœŸçš„æ˜¯å‹ç¼©çš„ï¼ˆå‰å‡ ä¸ªå­—èŠ‚æ˜¯Brotlié­”æ•°ï¼‰
                if content[:2] == b'\x81\x16' or content[:2] == b'\xce\xb2' or content[:1] == b'\xce':
                    try:
                        import brotli
                        content = brotli.decompress(content)
                        logger.debug("æ‰‹åŠ¨è§£å‹Brotliå‹ç¼©å†…å®¹")
                    except ImportError:
                        logger.warning("æ£€æµ‹åˆ°Brotliå‹ç¼©ä½†æœªå®‰è£…brotliåº“ï¼Œè¯·è¿è¡Œ: pip install brotli")
                        # å°è¯•ç§»é™¤brä»Accept-Encodingé‡æ–°è¯·æ±‚
                        headers_no_br = headers.copy()
                        if 'br' in headers_no_br.get('Accept-Encoding', ''):
                            accept_encoding = headers_no_br.get('Accept-Encoding', '')
                            accept_encoding = accept_encoding.replace('br', '').replace(',,', ',').strip(', ')
                            headers_no_br['Accept-Encoding'] = accept_encoding
                            logger.info("é‡æ–°è¯·æ±‚ï¼ˆä¸ä½¿ç”¨Brotliå‹ç¼©ï¼‰...")
                            response = requests.get(url, headers=headers_no_br, timeout=self.timeout)
                            response.raise_for_status()
                            content = response.content
                    except Exception as e:
                        logger.warning(f"Brotliè§£å‹å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°è¯·æ±‚ï¼ˆä¸ä½¿ç”¨Brotliï¼‰...")
                        # å°è¯•ç§»é™¤brä»Accept-Encodingé‡æ–°è¯·æ±‚
                        headers_no_br = headers.copy()
                        if 'br' in headers_no_br.get('Accept-Encoding', ''):
                            accept_encoding = headers_no_br.get('Accept-Encoding', '')
                            accept_encoding = accept_encoding.replace('br', '').replace(',,', ',').strip(', ')
                            headers_no_br['Accept-Encoding'] = accept_encoding
                            response = requests.get(url, headers=headers_no_br, timeout=self.timeout)
                            response.raise_for_status()
                            content = response.content

            # è§£æRSS
            feed = feedparser.parse(content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # æå–æ–‡ç« ä¿¡æ¯
            articles = []
            for entry in feed.entries[:max_articles]:
                article = self._parse_entry(entry, feed.feed, source_name=source_name)
                if article:
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return articles

        except requests.RequestException as e:
            # ä¸åœ¨è¿™é‡Œæ‰“å°é”™è¯¯æ—¥å¿—ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…ç»Ÿä¸€å¤„ç†
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…èƒ½å¤Ÿæ•è·å¹¶è®°å½•å¤±è´¥
            raise
        except Exception as e:
            # ä¸åœ¨è¿™é‡Œæ‰“å°é”™è¯¯æ—¥å¿—ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…ç»Ÿä¸€å¤„ç†
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…èƒ½å¤Ÿæ•è·å¹¶è®°å½•å¤±è´¥
            raise

    def _parse_entry(self, entry: Any, feed_info: Any, source_name: str = None) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡æ–‡ç« 

        Args:
            entry: feedparser entry
            feed_info: feedä¿¡æ¯
            source_name: è®¢é˜…æºåç§°ï¼ˆä¼˜å…ˆä½¿ç”¨æ­¤ä½œä¸ºsourceï¼Œè€Œä¸æ˜¯feed titleï¼‰

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            # åŸºæœ¬å­—æ®µ
            title = entry.get("title", "æ— æ ‡é¢˜")
            url = entry.get("link", "")
            author = entry.get("author", "")

            # å‘å¸ƒæ—¶é—´
            # feedparserè¿”å›çš„æ—¶é—´æ˜¯UTCæ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                # åˆ›å»ºUTCæ—¶é—´å¯¹è±¡
                utc_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
                local_tz = timezone(timedelta(hours=8))
                published_at = utc_time.astimezone(local_tz).replace(tzinfo=None)
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                # åˆ›å»ºUTCæ—¶é—´å¯¹è±¡
                utc_time = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ï¼ˆUTC+8ï¼‰
                local_tz = timezone(timedelta(hours=8))
                published_at = utc_time.astimezone(local_tz).replace(tzinfo=None)

            # å†…å®¹æå–
            content = ""
            if hasattr(entry, "content") and entry.content:
                content = entry.content[0].value if isinstance(entry.content, list) else entry.content
            elif hasattr(entry, "summary"):
                content = entry.summary
            elif hasattr(entry, "description"):
                content = entry.description

            # æ¸…ç†HTMLæ ‡ç­¾
            content = self._clean_html(content)

            # æ¥æºï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è®¢é˜…æºåç§°ï¼Œå¦åˆ™ä½¿ç”¨feed title
            source = source_name if source_name else feed_info.get("title", "Unknown")
            
            # æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…ï¼ˆå¦‚æœRSS feedä¸­çš„authorä¸å‡†ç¡®ï¼‰
            correct_author = _get_author_from_source(source_name, url)
            if correct_author:
                author = correct_author
            # å¦‚æœRSS feedä¸­æ²¡æœ‰authorï¼Œä½†å¯ä»¥æ ¹æ®æºåç§°ç¡®å®šï¼Œåˆ™ä½¿ç”¨ç¡®å®šçš„ä½œè€…
            elif not author and correct_author:
                author = correct_author

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å¤±è´¥: {e}")
            return None

    def _clean_html(self, html: str) -> str:
        """
        å°†HTMLè½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            html: HTMLå­—ç¬¦ä¸²

        Returns:
            Markdownæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        if not html:
            return ""
        return self.html_to_markdown(html)

    def _extract_date_from_page(self, soup: BeautifulSoup, url: str) -> Optional[datetime]:
        """
        ä»é¡µé¢HTMLä¸­æå–å‘å¸ƒæ—¥æœŸ

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: é¡µé¢URLï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        try:
            text = soup.get_text()

            month_names = [
                'January', 'February', 'March', 'April', 'May', 'June',
                'July', 'August', 'September', 'October', 'November', 'December'
            ]

            month_to_num = {
                'January': 1, 'February': 2, 'March': 3, 'April': 4,
                'May': 5, 'June': 6, 'July': 7, 'August': 8,
                'September': 9, 'October': 10, 'November': 11, 'December': 12
            }

            # Paul Grahamçš„æ—¥æœŸæ ¼å¼: "Month YYYY" (å¦‚ "October 2023")
            if 'paulgraham.com' in url:
                date_pattern = r'(' + '|'.join(month_names) + r')\s+(\d{4})'
                match = re.search(date_pattern, text, re.IGNORECASE)
                if match:
                    month_str = match.group(1)
                    year = int(match.group(2))
                    month_num = month_to_num.get(month_str.capitalize())
                    if month_num:
                        return datetime(year, month_num, 1)

            return None

        except Exception as e:
            logger.warning(f"âš ï¸  æå–æ—¥æœŸå¤±è´¥: {e}")
            return None

    def fetch_full_content(self, url: str) -> Tuple[str, Optional[datetime]]:
        """
        è·å–æ–‡ç« çš„å®Œæ•´é¡µé¢å†…å®¹å’Œå‘å¸ƒæ—¥æœŸ

        Args:
            url: æ–‡ç« URL

        Returns:
            (å®Œæ•´å†…å®¹æ–‡æœ¬, å‘å¸ƒæ—¶é—´) çš„å…ƒç»„
        """
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨è·å–å®Œæ•´å†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            # å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main article',
                '[role="article"]',
                '.blog-post-content',
            ]

            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ ï¼Œè½¬æ¢ä¸ºMarkdown
                    content = self.html_to_markdown(str(elements[0]))
                    if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        break

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–bodyå†…å®¹ï¼Œä½†ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰
            if not content or len(content) < 500:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    tag.decompose()
                content = self.html_to_markdown(str(soup))

            # å°è¯•ä»é¡µé¢æå–å‘å¸ƒæ—¥æœŸ
            published_at = self._extract_date_from_page(soup, url)

            logger.info(f"âœ… æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦" + (f"ï¼Œæ—¥æœŸ: {published_at}" if published_at else ""))
            return content, published_at

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return "", None
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æå®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return "", None

    def fetch_single_feed(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆå…¬å¼€æ¥å£ï¼‰

        Args:
            config: RSSé…ç½®ï¼ŒåŒ…å« name, url, max_articles ç­‰å­—æ®µ

        Returns:
            {"articles": [articles], "feed_title": "feed title"}
        """
        return self._fetch_single_feed_with_info(config)

    def _fetch_single_feed_with_info(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆåŒ…å«feedä¿¡æ¯ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

        Args:
            config: RSSé…ç½®

        Returns:
            {"articles": [articles], "feed_title": "feed title"}
        """
        name = config.get("name", "Unknown")
        url = config.get("url")
        max_articles = config.get("max_articles", 20)

        if not url:
            logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
            return {"articles": [], "feed_title": None}

        try:
            # è·å–feedä¿¡æ¯ï¼ˆåªè¯·æ±‚ä¸€æ¬¡ï¼‰
            # ä½¿ç”¨å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´
            headers = self.default_headers.copy()
            # å¦‚æœæ˜¯ RSSHubï¼Œæ·»åŠ ç‰¹å®šçš„ Referer
            if "rsshub.app" in url or "rsshub" in url.lower():
                headers["Referer"] = "https://rsshub.app/"
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # å¤„ç†å“åº”å†…å®¹ï¼ˆç¡®ä¿æ­£ç¡®è§£å‹ï¼‰
            content = response.content
            content_encoding = response.headers.get('Content-Encoding', '').lower()
            
            # å¦‚æœå“åº”æ˜¯Brotliå‹ç¼©ä½†requestsæ²¡æœ‰è‡ªåŠ¨è§£å‹ï¼Œæ‰‹åŠ¨è§£å‹
            if content_encoding == 'br':
                # æ£€æŸ¥å†…å®¹æ˜¯å¦çœŸçš„æ˜¯å‹ç¼©çš„ï¼ˆå‰å‡ ä¸ªå­—èŠ‚æ˜¯Brotlié­”æ•°ï¼‰
                if content[:2] == b'\x81\x16' or content[:2] == b'\xce\xb2' or content[:1] == b'\xce':
                    try:
                        import brotli
                        content = brotli.decompress(content)
                        logger.debug("æ‰‹åŠ¨è§£å‹Brotliå‹ç¼©å†…å®¹")
                    except ImportError:
                        logger.warning("æ£€æµ‹åˆ°Brotliå‹ç¼©ä½†æœªå®‰è£…brotliåº“ï¼Œè¯·è¿è¡Œ: pip install brotli")
                        # å°è¯•ç§»é™¤brä»Accept-Encodingé‡æ–°è¯·æ±‚
                        headers_no_br = headers.copy()
                        if 'br' in headers_no_br.get('Accept-Encoding', ''):
                            accept_encoding = headers_no_br.get('Accept-Encoding', '')
                            accept_encoding = accept_encoding.replace('br', '').replace(',,', ',').strip(', ')
                            headers_no_br['Accept-Encoding'] = accept_encoding
                            logger.info("é‡æ–°è¯·æ±‚ï¼ˆä¸ä½¿ç”¨Brotliå‹ç¼©ï¼‰...")
                            response = requests.get(url, headers=headers_no_br, timeout=self.timeout)
                            response.raise_for_status()
                            content = response.content
                    except Exception as e:
                        logger.warning(f"Brotliè§£å‹å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°è¯·æ±‚ï¼ˆä¸ä½¿ç”¨Brotliï¼‰...")
                        # å°è¯•ç§»é™¤brä»Accept-Encodingé‡æ–°è¯·æ±‚
                        headers_no_br = headers.copy()
                        if 'br' in headers_no_br.get('Accept-Encoding', ''):
                            accept_encoding = headers_no_br.get('Accept-Encoding', '')
                            accept_encoding = accept_encoding.replace('br', '').replace(',,', ',').strip(', ')
                            headers_no_br['Accept-Encoding'] = accept_encoding
                            response = requests.get(url, headers=headers_no_br, timeout=self.timeout)
                            response.raise_for_status()
                            content = response.content

            # è§£æRSS
            feed = feedparser.parse(content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # è·å–feed title
            feed_title = feed.feed.get("title", None) if hasattr(feed, 'feed') else None

            # æå–æ–‡ç« ä¿¡æ¯ï¼ˆä½¿ç”¨è®¢é˜…æºåç§°ä½œä¸ºsourceï¼‰
            # æ³¨æ„ï¼šæ¯ä¸ªçº¿ç¨‹éƒ½ä¼šåˆ›å»ºç‹¬ç«‹çš„feedå¯¹è±¡ï¼Œä¸ä¼šå…±äº«
            articles = []
            for entry in feed.entries[:max_articles]:
                # ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„source_nameï¼Œé˜²æ­¢å¹¶å‘æ—¶ä½¿ç”¨é”™è¯¯çš„åç§°
                article = self._parse_entry(entry, feed.feed, source_name=name)
                if article:
                    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿articleçš„sourceå­—æ®µä¸ä¼ å…¥çš„nameä¸€è‡´
                    if article.get("source") != name:
                        logger.warning(f"  âš ï¸  RSSè§£ææ—¶sourceä¸åŒ¹é…: æœŸæœ›={name}, å®é™…={article.get('source')}, URL={article.get('url', '')[:50]}")
                        article["source"] = name  # å¼ºåˆ¶ä¿®æ­£
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return {"articles": articles, "feed_title": feed_title}

        except requests.RequestException as e:
            # ä¸åœ¨è¿™é‡Œæ‰“å°é”™è¯¯æ—¥å¿—ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…ç»Ÿä¸€å¤„ç†
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…èƒ½å¤Ÿæ•è·å¹¶è®°å½•å¤±è´¥
            raise
        except Exception as e:
            # ä¸åœ¨è¿™é‡Œæ‰“å°é”™è¯¯æ—¥å¿—ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…ç»Ÿä¸€å¤„ç†
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚è°ƒç”¨è€…èƒ½å¤Ÿæ•è·å¹¶è®°å½•å¤±è´¥
            raise
