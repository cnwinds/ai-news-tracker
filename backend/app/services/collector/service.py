"""
ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡
"""
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

from backend.app.services.collector.rss_collector import RSSCollector
from backend.app.services.collector.api_collector import ArXivCollector, HuggingFaceCollector, PapersWithCodeCollector
from backend.app.services.collector.web_collector import WebCollector
from backend.app.services.collector.twitter_collector import TwitterCollector
from backend.app.services.collector.email_collector import EmailCollector
from backend.app.db import get_db
from backend.app.db.models import Article, CollectionLog, RSSSource
from backend.app.services.analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class CollectionService:
    """ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self, ai_analyzer: AIAnalyzer = None):
        # æ•°æ®é‡‡é›†åªä»æ•°æ®åº“è¯»å–æº
        # é…ç½®æ–‡ä»¶ä»…ç”¨äºå¯¼å…¥åŠŸèƒ½ï¼Œä¸ç”¨äºé‡‡é›†
        self.ai_analyzer = ai_analyzer

        # åˆå§‹åŒ–å„ä¸ªé‡‡é›†å™¨
        self.rss_collector = RSSCollector()
        self.arxiv_collector = ArXivCollector()
        self.hf_collector = HuggingFaceCollector()
        self.pwc_collector = PapersWithCodeCollector()
        self.web_collector = WebCollector()
        self.twitter_collector = TwitterCollector()
        self.email_collector = EmailCollector()

        # åˆå§‹åŒ–æ€»ç»“ç”Ÿæˆå™¨
        if ai_analyzer:
            from backend.app.services.collector.summary_generator import SummaryGenerator
            self.summary_generator = SummaryGenerator(ai_analyzer)
        else:
            self.summary_generator = None


    def _merge_extra_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶ extra_config åˆ°ä¸»é…ç½®ä¸­
        
        Args:
            config: æºé…ç½®å­—å…¸
            
        Returns:
            åˆå¹¶åçš„é…ç½®å­—å…¸
        """
        merged_config = config.copy()
        extra_config = config.get("extra_config", {})
        
        if isinstance(extra_config, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            try:
                extra_config = json.loads(extra_config)
            except:
                extra_config = {}
        
        if extra_config:
            # å°† extra_config ä¸­çš„å­—æ®µåˆå¹¶åˆ°ä¸»é…ç½®
            merged_config.update(extra_config)
        
        return merged_config

    def _get_collector_by_type(self, source_type: str, sub_type: Optional[str] = None) -> tuple[Optional[Any], Optional[str]]:
        """
        æ ¹æ®source_typeå’Œsub_typeè·å–å¯¹åº”çš„é‡‡é›†å™¨
        
        Args:
            source_type: æºç±»å‹ï¼ˆrss/api/web/emailï¼‰
            sub_type: æºå­ç±»å‹ï¼ˆå¦‚apiä¸‹çš„arxiv/huggingface/paperswithcodeï¼Œsocialä¸‹çš„twitter/reddit/hackernewsï¼‰
        
        Returns:
            (collector_instance, collector_name) å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›(None, None)
        """
        # æ ¹æ®source_typeå’Œsub_typeé€‰æ‹©é‡‡é›†å™¨
        if source_type == "rss":
            return (self.rss_collector, "rss")
        
        elif source_type == "api":
            if sub_type == "arxiv":
                return (self.arxiv_collector, "arxiv")
            elif sub_type == "huggingface":
                return (self.hf_collector, "huggingface")
            elif sub_type == "paperswithcode":
                return (self.pwc_collector, "paperswithcode")
            elif sub_type == "twitter":
                return (self.twitter_collector, "twitter")
            else:
                return (None, None)
        
        elif source_type == "web":
            return (self.web_collector, "web")
        
        elif source_type == "email":
            return (self.email_collector, "email")
        
        return (None, None)

    def collect_all(self, enable_ai_analysis: bool = True, task_id: int = None) -> Dict[str, Any]:
        """
        é‡‡é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº

        Args:
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            task_id: ä»»åŠ¡IDï¼Œç”¨äºå®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ğŸš€ å¼€å§‹é‡‡é›†æ‰€æœ‰æ•°æ®æº")
        
        db = get_db()
        
        # åœ¨å¼€å§‹é‡‡é›†å‰ï¼Œæ£€æŸ¥å¹¶æ¢å¤æŒ‚èµ·çš„ä»»åŠ¡
        self._recover_stuck_tasks(db)
        
        # å¯¼å…¥åœæ­¢æ£€æŸ¥å‡½æ•°
        from backend.app.api.v1.endpoints.collection import is_stop_requested
        
        stats = {
            "total_articles": 0,
            "new_articles": 0,
            "sources_success": 0,
            "sources_error": 0,
            "start_time": datetime.now(),
        }

        # 1. é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæº + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘è·å–å†…å®¹+AIåˆ†æï¼‰
        logger.info("\nğŸ“¡ é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘æ¨¡å¼ï¼‰")
        if task_id and is_stop_requested(task_id):
            logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
            return stats
        rss_stats = self._collect_rss_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats["total_articles"] += rss_stats.get("total_articles", 0)
        stats["new_articles"] += rss_stats.get("new_articles", 0)
        stats["sources_success"] += rss_stats.get("sources_success", 0)
        stats["sources_error"] += rss_stats.get("sources_error", 0)
        stats["ai_analyzed_count"] = rss_stats.get("ai_analyzed_count", 0)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)
            if is_stop_requested(task_id):
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
                return stats

        # 2. é‡‡é›†APIæºï¼ˆarXiv, Hugging Faceç­‰ï¼‰
        logger.info("\nğŸ“š é‡‡é›†è®ºæ–‡APIæº")
        if task_id and is_stop_requested(task_id):
            logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
            return stats
        api_stats = self._collect_api_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats["total_articles"] += api_stats.get("total_articles", 0)
        stats["new_articles"] += api_stats.get("new_articles", 0)
        stats["sources_success"] += api_stats.get("sources_success", 0)
        stats["sources_error"] += api_stats.get("sources_error", 0)
        stats["ai_analyzed_count"] += api_stats.get("ai_analyzed_count", 0)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)
            if is_stop_requested(task_id):
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
                return stats

        # 3. é‡‡é›†ç½‘ç«™æºï¼ˆé€šè¿‡ç½‘é¡µçˆ¬å–ï¼‰
        logger.info("\nğŸŒ é‡‡é›†ç½‘ç«™æº")
        if task_id and is_stop_requested(task_id):
            logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
            return stats
        web_stats = self._collect_web_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats["total_articles"] += web_stats.get("total_articles", 0)
        stats["new_articles"] += web_stats.get("new_articles", 0)
        stats["sources_success"] += web_stats.get("sources_success", 0)
        stats["sources_error"] += web_stats.get("sources_error", 0)
        stats["ai_analyzed_count"] += web_stats.get("ai_analyzed_count", 0)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)
            if is_stop_requested(task_id):
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
                return stats

        # 3. é‡‡é›†é‚®ä»¶æº
        logger.info("\nğŸ“§ é‡‡é›†é‚®ä»¶æº")
        if task_id and is_stop_requested(task_id):
            logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
            return stats
        email_stats = self._collect_email_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats["total_articles"] += email_stats.get("total_articles", 0)
        stats["new_articles"] += email_stats.get("new_articles", 0)
        stats["sources_success"] += email_stats.get("sources_success", 0)
        stats["sources_error"] += email_stats.get("sources_error", 0)
        stats["ai_analyzed_count"] += email_stats.get("ai_analyzed_count", 0)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)
            if is_stop_requested(task_id):
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡‡é›†")
                return stats

        # 6. å¯é€‰ï¼šè‡ªåŠ¨ç´¢å¼•æ–°æ–‡ç« åˆ°RAGåº“
        if enable_ai_analysis and self.ai_analyzer:
            try:
                logger.info("\nğŸ” å¼€å§‹è‡ªåŠ¨ç´¢å¼•æ–°æ–‡ç« åˆ°RAGåº“...")
                from backend.app.services.rag.rag_service import RAGService
                from backend.app.db.models import ArticleEmbedding
                with db.get_session() as session:
                    rag_service = RAGService(ai_analyzer=self.ai_analyzer, db=session)
                    # è·å–æ‰€æœ‰æœªç´¢å¼•çš„æ–‡ç« ï¼ˆä¸é™åˆ¶æ—¶é—´ï¼Œé¿å…é—æ¼ï¼‰
                    # ä½¿ç”¨å­æŸ¥è¯¢æ’é™¤å·²ç´¢å¼•çš„æ–‡ç« 
                    from sqlalchemy import select
                    indexed_ids = select(ArticleEmbedding.article_id)
                    unindexed_articles = session.query(Article).filter(
                        ~Article.id.in_(indexed_ids)
                    ).all()
                    
                    if unindexed_articles:
                        logger.info(f"ğŸ“ æ‰¾åˆ° {len(unindexed_articles)} ç¯‡æœªç´¢å¼•æ–‡ç« ï¼Œå¼€å§‹ç´¢å¼•...")
                        index_result = rag_service.index_articles_batch(unindexed_articles, batch_size=10)
                        stats["rag_indexed"] = index_result.get("success", 0)
                        logger.info(f"âœ… RAGç´¢å¼•å®Œæˆ: æˆåŠŸ {index_result.get('success', 0)} ç¯‡ï¼Œå¤±è´¥ {index_result.get('failed', 0)} ç¯‡")
                    else:
                        logger.info("â„¹ï¸  æ‰€æœ‰æ–‡ç« å·²ç´¢å¼•")
                        stats["rag_indexed"] = 0
            except Exception as e:
                logger.warning(f"âš ï¸  è‡ªåŠ¨ç´¢å¼•å¤±è´¥ï¼ˆä¸å½±å“é‡‡é›†æµç¨‹ï¼‰: {e}")
                stats["rag_indexed"] = 0

        stats["end_time"] = datetime.now()
        stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

        logger.info(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        logger.info(f"   æ–°å¢æ–‡ç« : {stats['new_articles']}")
        logger.info(f"   æˆåŠŸæºæ•°: {stats['sources_success']}")
        logger.info(f"   å¤±è´¥æºæ•°: {stats['sources_error']}")
        logger.info(f"   AIåˆ†ææ•°: {stats.get('ai_analyzed_count', 0)}")
        logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")

        return stats

    def _fetch_articles_full_content(self, articles: List[Dict[str, Any]], source_name: str, max_workers: int = 3) -> List[Dict[str, Any]]:
        """
        å¹¶å‘è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            source_name: æºåç§°
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3ï¼ˆé¿å…å¯¹å•ä¸ªç½‘ç«™å‹åŠ›è¿‡å¤§ï¼‰
        
        Returns:
            æ›´æ–°åçš„æ–‡ç« åˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦è·å–å®Œæ•´å†…å®¹çš„æ–‡ç« ï¼ˆblogæ–‡ç« ï¼‰
        articles_to_fetch = [
            article for article in articles 
            if article.get("category") == "rss" and article.get("url")
        ]
        
        if not articles_to_fetch:
            return articles
        
        logger.info(f"  ğŸ“„ å¼€å§‹å¹¶å‘è·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # å¹¶å‘è·å–å®Œæ•´å†…å®¹
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_article = {
                executor.submit(self.rss_collector.fetch_full_content, article["url"]): article
                for article in articles_to_fetch
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                completed += 1

                try:
                    full_content, published_at = future.result()
                    if full_content:
                        article["content"] = full_content
                        # å¦‚æœä»é¡µé¢æå–åˆ°äº†æ—¥æœŸï¼Œæ›´æ–°æ–‡ç« çš„published_atå­—æ®µ
                        if published_at:
                            article["published_at"] = published_at
                            logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²è·å–å®Œæ•´å†…å®¹å’Œæ—¥æœŸ: {article['title'][:50]}...")
                        else:
                            logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²è·å–å®Œæ•´å†…å®¹: {article['title'][:50]}...")
                    else:
                        logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] æ— æ³•è·å–å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨RSSæ‘˜è¦: {article['title'][:50]}...")
                except Exception as e:
                    logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] è·å–å®Œæ•´å†…å®¹å¤±è´¥: {article['title'][:50]}... - {e}")
        
        logger.info(f"  âœ… å®Œæ•´å†…å®¹è·å–å®Œæˆ: {len(articles_to_fetch)} ç¯‡æ–‡ç« ")
        return articles

    def _update_task_progress(self, db, task_id: int, stats: Dict[str, Any]):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            from backend.app.db.models import CollectionTask
            with db.get_session() as session:
                task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                if task:
                    task.new_articles_count = stats.get('new_articles', 0)
                    task.total_sources = stats.get('sources_success', 0) + stats.get('sources_error', 0)
                    task.success_sources = stats.get('sources_success', 0)
                    task.failed_sources = stats.get('sources_error', 0)
                    task.ai_analyzed_count = stats.get('analyzed_count', 0)
                    session.commit()
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

    def _process_single_rss_source(self, db, source_name: str, feed_result: Dict[str, Any], enable_ai_analysis: bool = False, task_id: int = None) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªRSSæºï¼šè·å–å®Œæ•´å†…å®¹ -> ä¿å­˜æ–‡ç«  -> AIåˆ†æï¼ˆå…¨æµç¨‹å¹¶å‘ï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            source_name: è®¢é˜…æºåç§°
            feed_result: RSSé‡‡é›†ç»“æœ
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            task_id: ä»»åŠ¡ID

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        result_stats = {
            "source_name": source_name,
            "total_articles": 0,
            "new_articles": 0,
            "skipped_articles": 0,  # å·²å­˜åœ¨çš„æ–‡ç« 
            "ai_analyzed": 0,
            "ai_skipped": 0,  # å·²åˆ†æçš„æ–‡ç« 
            "success": False,
            "error": None
        }

        try:
            articles = feed_result.get("articles", [])
            feed_title = feed_result.get("feed_title")

            if not articles:
                # å³ä½¿æ²¡æœ‰æ–‡ç« ï¼Œä¹Ÿè¦è®°å½•æ—¥å¿—ï¼ˆæˆåŠŸä½†æ— æ–‡ç« ï¼‰
                self._log_collection(db, source_name, "rss", "success", 0, task_id=task_id)
                result_stats["success"] = True
                return result_stats

            # ç¡®ä¿æ‰€æœ‰æ–‡ç« çš„sourceå­—æ®µéƒ½æ˜¯è®¢é˜…æºåç§°ï¼Œå¹¶è®¾ç½®æ­£ç¡®çš„author
            # è¿™æ˜¯å…³é”®çš„é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¼ºåˆ¶è¦†ç›–æ‰€æœ‰æ–‡ç« çš„sourceå­—æ®µï¼Œé˜²æ­¢å¹¶å‘å†²çª
            from backend.app.services.collector.rss_collector import _get_author_from_source
            from backend.app.core.settings import settings
            
            # åº”ç”¨æ–‡ç« å¹´é¾„è¿‡æ»¤ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            filtered_articles = []
            skipped_old_count = 0
            max_article_age_days = settings.MAX_ARTICLE_AGE_DAYS
            
            if max_article_age_days > 0:
                age_threshold = datetime.now() - timedelta(days=max_article_age_days)
                for article in articles:
                    published_at = article.get("published_at")
                    if published_at and published_at < age_threshold:
                        skipped_old_count += 1
                        continue
                    filtered_articles.append(article)
                articles = filtered_articles
                if skipped_old_count > 0:
                    logger.info(f"  â­ï¸  {source_name}: è·³è¿‡äº† {skipped_old_count} ç¯‡è¶…è¿‡ {max_article_age_days} å¤©çš„æ—§æ–‡ç« ")
            
            for article in articles:
                # å¼ºåˆ¶è®¾ç½®sourceå­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¢é˜…æºåç§°
                # è¿™å¯ä»¥é˜²æ­¢å¹¶å‘æ—¶feed titleè¢«é”™è¯¯ä½¿ç”¨
                article["source"] = source_name
                
                # æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…ï¼ˆè¦†ç›–RSS feedä¸­å¯èƒ½ä¸å‡†ç¡®çš„authorï¼‰
                correct_author = _get_author_from_source(source_name, article.get("url", ""))
                if correct_author:
                    article["author"] = correct_author
                
                # é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æœæ–‡ç« çš„sourceä¸ä¼ å…¥çš„source_nameä¸ä¸€è‡´ï¼Œè®°å½•è­¦å‘Š
                if article.get("source") != source_name:
                    logger.warning(f"  âš ï¸  æ–‡ç« sourceä¸åŒ¹é…: æœŸæœ›={source_name}, å®é™…={article.get('source')}, URL={article.get('url', '')[:50]}")
                    article["source"] = source_name  # å¼ºåˆ¶ä¿®æ­£

            logger.info(f"  ğŸ“¥ {source_name}: å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")

            # æ³¨æ„ï¼šä¸å†éœ€è¦ä¿®æ­£sourceå­—æ®µï¼Œå› ä¸ºæ·»åŠ äº†source_idå¤–é”®å…³è”
            # ä¿å­˜æ–‡ç« æ—¶ä¼šè‡ªåŠ¨æ ¹æ®source_nameæŸ¥è¯¢RSSSourceè·å–source_id
            # å¦‚æœRSSSource.nameè¢«ä¿®æ”¹ï¼Œå¯ä»¥é€šè¿‡article.rss_source.nameè·å–æœ€æ–°åç§°

            # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æ£€æŸ¥å“ªäº›æ–‡ç« å·²å­˜åœ¨ä¸”æœ‰å†…å®¹ã€å·²åˆ†æ
            existing_articles_data = {}
            with db.get_session() as session:
                # æŸ¥è¯¢å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆåŒ…æ‹¬å†…å®¹å’Œåˆ†æçŠ¶æ€ï¼‰
                url_list = [article.get("url") for article in articles if article.get("url")]
                if url_list:
                    existing = session.query(
                        Article.url,
                        Article.content,
                        Article.is_processed
                    ).filter(Article.url.in_(url_list)).all()

                    # å­˜å‚¨æ¯ä¸ªURLçš„çŠ¶æ€ï¼š{"url": {"has_content": bool, "is_processed": bool}}
                    for row in existing:
                        existing_articles_data[row[0]] = {
                            "has_content": bool(row[1] and row[1].strip()),  # æ£€æŸ¥å†…å®¹æ˜¯å¦éç©º
                            "is_processed": row[2]
                        }

            # ç¬¬äºŒæ­¥ï¼šåˆ†ç±»æ–‡ç« 
            articles_to_fetch = []  # éœ€è¦è·å–å†…å®¹çš„æ–‡ç« 
            articles_to_analyze = []  # éœ€è¦AIåˆ†æçš„æ–‡ç« 
            skipped_count = 0  # å®Œå…¨è·³è¿‡çš„æ–‡ç« 

            for article in articles:
                url = article.get("url")
                if not url:
                    continue

                if url not in existing_articles_data:
                    # æ–°æ–‡ç« ï¼Œéœ€è¦è·å–å†…å®¹å’ŒAIåˆ†æ
                    articles_to_fetch.append(article)
                else:
                    # æ–‡ç« å·²å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹å’Œåˆ†æçŠ¶æ€
                    status = existing_articles_data[url]

                    if not status["has_content"]:
                        # å†…å®¹ä¸ºç©ºï¼Œéœ€è¦é‡æ–°è·å–
                        articles_to_fetch.append(article)

                    if not status["is_processed"]:
                        # æœªåˆ†æï¼Œéœ€è¦é‡æ–°åˆ†æï¼ˆè®°å½•URLä»¥ä¾¿åç»­æŸ¥æ‰¾IDï¼‰
                        articles_to_analyze.append(article)

                    if status["has_content"] and status["is_processed"]:
                        # å†…å®¹å®Œæ•´ä¸”å·²åˆ†æï¼Œå®Œå…¨è·³è¿‡
                        skipped_count += 1

            result_stats["total_articles"] = len(articles)
            result_stats["skipped_articles"] = skipped_count

            fetch_count = len(articles_to_fetch)
            analyze_count = len(articles_to_analyze)

            if skipped_count > 0 or fetch_count > 0 or analyze_count > 0:
                logger.info(f"  ğŸ“Š {source_name}: è·³è¿‡ {skipped_count} ç¯‡å®Œæ•´æ–‡ç« , éœ€è·å–å†…å®¹ {fetch_count} ç¯‡, éœ€AIåˆ†æ {analyze_count} ç¯‡")

            if not articles_to_fetch and not articles_to_analyze:
                logger.info(f"  âœ… {source_name}: æ‰€æœ‰æ–‡ç« éƒ½å·²å®Œæ•´é‡‡é›†å’Œåˆ†æ")
                # å³ä½¿æ‰€æœ‰æ–‡ç« éƒ½å·²å®Œæ•´ï¼Œä¹Ÿè¦æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å’Œè®°å½•æ—¥å¿—
                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_collected_at = datetime.now()
                        source_obj.last_error = None
                        session.commit()
                # è®°å½•é‡‡é›†æ—¥å¿—ï¼ˆæˆåŠŸä½†æ— éœ€å¤„ç†ï¼‰
                self._log_collection(db, source_name, "rss", "success", len(articles), task_id=task_id)
                result_stats["success"] = True
                return result_stats

            logger.info(f"  ğŸ“¥ {source_name}: è·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹...")

            # ç¬¬ä¸‰æ­¥ï¼šå¹¶å‘è·å–å®Œæ•´å†…å®¹ï¼ˆ3ä¸ªå¹¶å‘ï¼‰
            articles_with_full_content = self._fetch_articles_full_content(
                articles_to_fetch, source_name, max_workers=3
            )

            # ç¬¬å››æ­¥ï¼šä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“
            logger.info(f"  ğŸ’¾ {source_name}: å¼€å§‹ä¿å­˜æ–‡ç« ...")
            saved_article_ids = []
            updated_count = 0  # æ›´æ–°çš„æ–‡ç« æ•°ï¼ˆå·²æœ‰URLä½†è¡¥å……äº†å†…å®¹ï¼‰
            new_count = 0  # æ–°å¢æ–‡ç« æ•°

            for article in articles_with_full_content:
                result = self._save_or_update_article_and_get_id(db, article)
                if result:
                    # åªä¿å­˜æ–‡ç« IDï¼ˆæ•´æ•°ï¼‰ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå­—å…¸
                    saved_article_ids.append(result["id"])
                    if result["is_new"]:
                        new_count += 1
                    else:
                        updated_count += 1

            result_stats["new_articles"] = new_count

            # æ›´æ–°RSSæºçš„ç»Ÿè®¡ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_collected_at = datetime.now()
                    source_obj.articles_count += len(articles)
                    source_obj.last_error = None

                    # ä»æ•°æ®åº“ä¸­æŸ¥è¯¢è¯¥æºæœ€æ–°çš„çœŸå®published_atï¼ˆè€Œä¸æ˜¯RSS feedçš„æ›´æ–°æ—¶é—´ï¼‰
                    latest_article = session.query(Article).filter(
                        Article.source == source_name,
                        Article.published_at.isnot(None)
                    ).order_by(Article.published_at.desc()).first()

                    if latest_article:
                        source_obj.latest_article_published_at = latest_article.published_at

                    session.commit()

            # è®°å½•é‡‡é›†æ—¥å¿—
            self._log_collection(db, source_name, "rss", "success", len(articles), task_id=task_id)

            # ç¬¬äº”æ­¥ï¼šå¦‚æœå¯ç”¨AIåˆ†æï¼Œå¤„ç†éœ€è¦åˆ†æçš„æ–‡ç« ï¼ˆåŒ…æ‹¬æ–°æ–‡ç« å’Œæ—§æ–‡ç« ï¼‰
            if enable_ai_analysis and self.ai_analyzer and (saved_article_ids or articles_to_analyze):
                # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ†æçš„æ–‡ç« IDï¼ˆå·²ç»æ˜¯æ•´æ•°åˆ—è¡¨ï¼‰
                all_article_ids = saved_article_ids.copy()

                # å¯¹äºå·²æœ‰URLä½†æœªåˆ†æçš„æ–‡ç« ï¼ŒæŸ¥è¯¢å®ƒä»¬çš„ID
                if articles_to_analyze:
                    with db.get_session() as session:
                        for article in articles_to_analyze:
                            existing = session.query(Article.id).filter(Article.url == article.get("url")).first()
                            if existing:
                                all_article_ids.append(existing.id)

                # æ£€æŸ¥å“ªäº›æ–‡ç« å·²ç»åˆ†æè¿‡äº†
                unanalyzed_ids = self._filter_unanalyzed_articles(db, all_article_ids)
                ai_skipped = len(all_article_ids) - len(unanalyzed_ids)

                if ai_skipped > 0:
                    logger.info(f"  â­ï¸  {source_name}: è·³è¿‡ {ai_skipped} ç¯‡å·²åˆ†æçš„æ–‡ç« ")

                if unanalyzed_ids:
                    logger.info(f"  ğŸ¤– {source_name}: å¼€å§‹AIåˆ†æ {len(unanalyzed_ids)} ç¯‡æ–‡ç« ...")
                    analyzed_count = self._analyze_articles_by_ids(db, unanalyzed_ids, max_workers=3)
                    result_stats["ai_analyzed"] = analyzed_count

                result_stats["ai_skipped"] = ai_skipped

            result_stats["success"] = True
            logger.info(f"  âœ… {source_name}: æ€»å…± {len(articles)} ç¯‡, è·³è¿‡ {skipped_count} ç¯‡, æ–°å¢ {new_count} ç¯‡, æ›´æ–° {updated_count} ç¯‡, AIåˆ†æ {result_stats['ai_analyzed']} ç¯‡")

        except Exception as e:
            logger.error(f"  âŒ {source_name}: {e}")
            result_stats["error"] = str(e)

            # æ›´æ–°é”™è¯¯ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_error = str(e)
                    session.commit()

            self._log_collection(db, source_name, "rss", "error", 0, str(e), task_id=task_id)

        return result_stats

    def _collect_rss_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæºåŒæ—¶é‡‡é›† + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘è·å–å†…å®¹+AIåˆ†æï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦åœ¨é‡‡é›†æ¯ä¸ªæºåç«‹å³è¿›è¡ŒAIåˆ†æ

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        # ä»æ•°æ®åº“è¯»å–RSSæºï¼ˆåªè¯»å–source_typeä¸ºrssçš„æºï¼‰
        rss_configs = []
        from backend.app.core.settings import settings
        # ç¡®ä¿åŠ è½½æœ€æ–°çš„é…ç½®
        settings.load_collector_settings()
        
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(
                RSSSource.enabled == True,
                RSSSource.source_type == "rss"
            ).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                rss_configs.append({
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                    "max_articles": settings.MAX_ARTICLES_PER_SOURCE,  # ä½¿ç”¨é…ç½®å€¼
                    "category": source.category,
                    "tier": source.tier,
                })
                # é¢„å…ˆåŠ è½½å±æ€§
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
                _ = source.last_collected_at
                _ = source.articles_count
            session.expunge_all()

        # åªä»æ•°æ®åº“è¯»å–æºï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æºåˆ™ä¸é‡‡é›†
        if not rss_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„RSSæºï¼Œè·³è¿‡é‡‡é›†")
            return stats

        logger.info(f"  ğŸš€ å¼€å§‹é‡‡é›† {len(rss_configs)} ä¸ªRSSæºï¼ˆç¬¬ä¸€å±‚å¹¶å‘ï¼‰")

        # ç¬¬ä¸€å±‚å¹¶å‘ï¼šåŒæ—¶é‡‡é›†å¤šä¸ªRSSæº
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰RSSé‡‡é›†ä»»åŠ¡
            future_to_source = {}

            for rss_config in rss_configs:
                source_name = rss_config["name"]

                # æ·±æ‹·è´é…ç½®å¯¹è±¡ï¼Œé¿å…å¤šçº¿ç¨‹å…±äº«å¼•ç”¨å¯¼è‡´çš„å¹¶å‘é—®é¢˜
                # è™½ç„¶é»˜è®¤å‚æ•°æ•è·äº†å¼•ç”¨ï¼Œä½†å¦‚æœåœ¨è°ƒç”¨è¿‡ç¨‹ä¸­ä¿®æ”¹äº†å­—å…¸ï¼Œä»æœ‰é£é™©
                import copy
                config_copy = copy.deepcopy(rss_config)

                # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è·å˜é‡çš„å€¼ï¼Œé¿å…é—­åŒ…é™·é˜±
                # è¿™æ˜¯å…³é”®çš„ä¿®å¤ï¼šé€šè¿‡é»˜è®¤å‚æ•°åœ¨å®šä¹‰æ—¶æ•è·å€¼ï¼Œè€Œä¸æ˜¯åœ¨è¿è¡Œæ—¶å¼•ç”¨å˜é‡
                def collect_single_source(config=config_copy, name=source_name, task_id_param=task_id):
                    try:
                        # è·å–RSS feedï¼ˆä½¿ç”¨ä¼ å…¥çš„configï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨æ­£ç¡®çš„é…ç½®ï¼‰
                        feed_data = self.rss_collector.fetch_single_feed(config)
                        
                        # å¦‚æœfetch_single_feedè¿”å›Noneæˆ–æ— æ•ˆæ•°æ®ï¼Œè®°å½•é”™è¯¯æ—¥å¿—
                        if not feed_data:
                            error_msg = f"{name}: RSS feedè·å–å¤±è´¥ï¼Œè¿”å›æ•°æ®ä¸ºç©º"
                            logger.error(f"  âŒ {error_msg}")
                            self._log_collection(db, name, "rss", "error", 0, error_msg, task_id=task_id_param)
                            return {
                                "source_name": name,
                                "success": False,
                                "error": error_msg,
                                "total_articles": 0,
                                "new_articles": 0,
                                "ai_analyzed": 0
                            }

                        # å¤„ç†è¿™ä¸ªæºï¼ˆåŒ…å«è·å–å®Œæ•´å†…å®¹ã€ä¿å­˜ã€AIåˆ†æï¼‰
                        # ä½¿ç”¨ä¼ å…¥çš„nameï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨æ­£ç¡®çš„æºåç§°
                        result = self._process_single_rss_source(
                            db, name, feed_data, enable_ai_analysis, task_id=task_id_param
                        )
                        return result
                    except Exception as e:
                        logger.error(f"  âŒ {name} é‡‡é›†å¤±è´¥: {e}")
                        # è®°å½•å¤±è´¥æ—¥å¿—ï¼ˆå¦‚æœfetch_single_feedå¤±è´¥ï¼Œ_process_single_rss_sourceä¸ä¼šè¢«è°ƒç”¨ï¼Œæ‰€ä»¥ä¸ä¼šé‡å¤ï¼‰
                        # å¦‚æœ_process_single_rss_sourceå†…éƒ¨æŠ›å‡ºå¼‚å¸¸ï¼Œå®ƒè‡ªå·±ä¼šè®°å½•æ—¥å¿—ï¼Œè¿™é‡Œå†è®°å½•ä¸€æ¬¡ä¼šé‡å¤
                        # ä½†ä¸ºäº†ç¡®ä¿æ‰€æœ‰å¼‚å¸¸éƒ½è¢«è®°å½•ï¼Œè¿™é‡Œä¹Ÿè®°å½•ä¸€æ¬¡ï¼ˆå¯èƒ½ä¼šæœ‰é‡å¤ï¼Œä½†æ¯”é—æ¼å¥½ï¼‰
                        self._log_collection(db, name, "rss", "error", 0, str(e), task_id=task_id_param)
                        # æ›´æ–°æ•°æ®åº“ä¸­çš„ last_error å­—æ®µ
                        try:
                            with db.get_session() as session:
                                source_obj = session.query(RSSSource).filter(RSSSource.name == name).first()
                                if source_obj:
                                    source_obj.last_error = str(e)
                                    session.commit()
                        except Exception as e2:
                            logger.error(f"âŒ æ›´æ–°æºé”™è¯¯ä¿¡æ¯å¤±è´¥ {name}: {e2}")
                        return {
                            "source_name": name,
                            "success": False,
                            "error": str(e),
                            "total_articles": 0,
                            "new_articles": 0,
                            "ai_analyzed": 0
                        }

                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(collect_single_source)
                future_to_source[future] = source_name

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                completed += 1

                try:
                    result = future.result()

                    if result["success"]:
                        stats["sources_success"] += 1
                        stats["new_articles"] += result["new_articles"]
                        stats["total_articles"] += result["total_articles"]
                        stats["ai_analyzed_count"] += result.get("ai_analyzed", 0)
                    else:
                        stats["sources_error"] += 1
                        # é”™è¯¯æ—¥å¿—å·²åœ¨ collect_single_source ä¸­æ‰“å°ï¼Œè¿™é‡Œä¸å†é‡å¤æ‰“å°
                        # åªæ›´æ–°æ•°æ®åº“ä¸­çš„ last_error å­—æ®µ
                        try:
                            with db.get_session() as session:
                                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                                if source_obj:
                                    source_obj.last_error = result.get('error', 'æœªçŸ¥é”™è¯¯')
                                    session.commit()
                        except Exception as e:
                            logger.error(f"âŒ æ›´æ–°æºé”™è¯¯ä¿¡æ¯å¤±è´¥ {source_name}: {e}")

                except Exception as e:
                    logger.error(f"  âŒ {source_name} å¤„ç†å¼‚å¸¸: {e}")
                    stats["sources_error"] += 1
                    # æ›´æ–°æ•°æ®åº“ä¸­çš„ last_error å­—æ®µ
                    try:
                        with db.get_session() as session:
                            source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                            if source_obj:
                                source_obj.last_error = str(e)
                                session.commit()
                    except Exception as e2:
                        logger.error(f"âŒ æ›´æ–°æºé”™è¯¯ä¿¡æ¯å¤±è´¥ {source_name}: {e2}")

        logger.info(f"  âœ… RSSé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢: {stats['new_articles']} ç¯‡, AIåˆ†æ: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _process_articles_from_source(self, db, articles: List[Dict[str, Any]], source_name: str, source_type: str, enable_ai_analysis: bool = False, task_id: int = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€å¤„ç†æ–‡ç« ï¼šä¿å­˜ + AIåˆ†æ

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            articles: æ–‡ç« åˆ—è¡¨
            source_name: æºåç§°
            source_type: æºç±»å‹ (rss/api/web/social)
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            task_id: ä»»åŠ¡ID

        Returns:
            {"total": int, "new": int, "ai_analyzed": int}
        """
        if not articles:
            return {"total": 0, "new": 0, "ai_analyzed": 0}

        # ç»Ÿä¸€ä¿®æ­£æ‰€æœ‰æ–‡ç« çš„sourceå­—æ®µä¸ºé…ç½®ä¸­çš„nameï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æºåç§°
        for article in articles:
            article["source"] = source_name

        new_count = 0
        saved_article_ids = []

        for article in articles:
            result = self._save_or_update_article_and_get_id(db, article)
            if result:
                saved_article_ids.append(result["id"])
                if result["is_new"]:
                    new_count += 1

        result = {"total": len(articles), "new": new_count, "ai_analyzed": 0}

        # æ£€æŸ¥AIåˆ†ææ¡ä»¶å¹¶è®°å½•æ—¥å¿—
        if not enable_ai_analysis:
            logger.info(f"  â„¹ï¸  {source_name}: AIåˆ†ææœªå¯ç”¨ï¼ˆenable_ai_analysis=Falseï¼‰")
        elif not self.ai_analyzer:
            logger.warning(f"  âš ï¸  {source_name}: AIåˆ†æå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æã€‚è¯·æ£€æŸ¥LLMæä¾›å•†å’Œå‘é‡æ¨¡å‹é…ç½®ã€‚")
        elif not saved_article_ids:
            logger.info(f"  â„¹ï¸  {source_name}: æ²¡æœ‰ä¿å­˜çš„æ–‡ç« ï¼Œè·³è¿‡AIåˆ†æ")
        else:
            # æ‰€æœ‰æ¡ä»¶æ»¡è¶³ï¼Œè¿›è¡ŒAIåˆ†æ
            unanalyzed_ids = self._filter_unanalyzed_articles(db, saved_article_ids)
            ai_skipped = len(saved_article_ids) - len(unanalyzed_ids)

            if ai_skipped > 0:
                logger.info(f"  â­ï¸  {source_name}: è·³è¿‡ {ai_skipped} ç¯‡å·²åˆ†æçš„æ–‡ç« ")

            if unanalyzed_ids:
                logger.info(f"  ğŸ¤– {source_name}: å¼€å§‹AIåˆ†æ {len(unanalyzed_ids)} ç¯‡æ–‡ç« ...")
                analyzed_count = self._analyze_articles_by_ids(db, unanalyzed_ids, max_workers=3)
                result["ai_analyzed"] = analyzed_count
            else:
                logger.info(f"  â„¹ï¸  {source_name}: æ‰€æœ‰æ–‡ç« éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡æ–°åˆ†æ")

        return result

    def _collect_api_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†APIæº

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        api_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(
                RSSSource.enabled == True,
                RSSSource.source_type == "api"
            ).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                config = {
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                    "category": source.category,
                    "sub_type": source.sub_type,  # è¯»å–sub_typeå­—æ®µ
                }

                if source.extra_config:
                    try:
                        import json
                        extra_config = json.loads(source.extra_config)
                        if isinstance(extra_config, dict):
                            config.update(extra_config)
                    except:
                        pass

                api_configs.append(config)
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
            session.expunge_all()

        # åªä»æ•°æ®åº“è¯»å–æºï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æºåˆ™ä¸é‡‡é›†
        if not api_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„APIæºï¼Œè·³è¿‡é‡‡é›†")
            return stats

        for config in api_configs:
            if not config.get("enabled", True):
                continue

            # åˆå¹¶ extra_config åˆ°ä¸»é…ç½®
            config = self._merge_extra_config(config)
            name = config.get("name")
            sub_type = config.get("sub_type")  # ä»æ•°æ®åº“è¯»å–çš„sub_type

            try:
                # ä½¿ç”¨source_typeå’Œsub_typeè·å–é‡‡é›†å™¨
                collector, collector_name = self._get_collector_by_type("api", sub_type)
                
                if not collector:
                    error_msg = f"{name}: æ— æ³•ç¡®å®šAPIé‡‡é›†å™¨ç±»å‹ã€‚è¯·è®¾ç½®sub_typeå­—æ®µ (arxiv/huggingface/paperswithcode/twitter)"
                    logger.error(f"  âŒ {error_msg}")
                    stats["sources_error"] += 1
                    self._log_collection(db, name, "api", "error", 0, error_msg, task_id=task_id)
                    
                    with db.get_session() as session:
                        source_obj = session.query(RSSSource).filter(RSSSource.name == name).first()
                        if source_obj:
                            source_obj.last_error = error_msg
                            session.commit()
                    continue
                
                articles = []
                collector_used = collector_name
                
                # æ ¹æ®ä¸åŒçš„é‡‡é›†å™¨è°ƒç”¨ç›¸åº”çš„æ–¹æ³•
                from backend.app.core.settings import settings
                settings.load_collector_settings()
                
                if collector_name == "arxiv":
                    query = config.get("query")
                    if not query:
                        raise ValueError(f"{name}: ArXivé‡‡é›†å™¨éœ€è¦é…ç½®queryå‚æ•°")
                    max_results = config.get("max_results", settings.MAX_ARTICLES_PER_SOURCE)
                    articles = self.arxiv_collector.fetch_papers(query, max_results)
                
                elif collector_name == "huggingface":
                    limit = config.get("max_results", settings.MAX_ARTICLES_PER_SOURCE)
                    articles = self.hf_collector.fetch_trending_papers(limit)
                
                elif collector_name == "paperswithcode":
                    limit = config.get("max_results", settings.MAX_ARTICLES_PER_SOURCE)
                    articles = self.pwc_collector.fetch_trending_papers(limit)
                
                elif collector_name == "twitter":
                    # Twitter ä½¿ç”¨ä¸“é—¨çš„ Twitter é‡‡é›†å™¨ï¼ˆæ”¯æŒ Nitter RSSã€TodayRssã€Twitter APIï¼‰
                    # å¦‚æœconfigä¸­æ²¡æœ‰max_tweetsï¼Œä½¿ç”¨max_articlesæˆ–é…ç½®å€¼
                    if "max_tweets" not in config:
                        config["max_tweets"] = config.get("max_articles", settings.MAX_ARTICLES_PER_SOURCE)
                    articles = self.twitter_collector.fetch_tweets(config)

                if not articles:
                    logger.info(f"  âš ï¸  {name}: ä½¿ç”¨{collector_used}é‡‡é›†å™¨æœªè·å–åˆ°æ–‡ç« ")
                    stats["sources_error"] += 1
                    self._log_collection(db, name, "api", "error", 0, f"ä½¿ç”¨{collector_used}é‡‡é›†å™¨æœªè·å–åˆ°æ–‡ç« ", task_id=task_id)
                    continue

                process_result = self._process_articles_from_source(db, articles, name, "api", enable_ai_analysis, task_id=task_id)

                self._log_collection(db, name, "api", "success", process_result["total"], task_id=task_id)
                stats["sources_success"] += 1
                stats["new_articles"] += process_result["new"]
                stats["total_articles"] += process_result["total"]
                stats["ai_analyzed_count"] += process_result["ai_analyzed"]

                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == name).first()
                    if source_obj:
                        source_obj.last_collected_at = datetime.now()
                        source_obj.articles_count += len(articles)
                        source_obj.last_error = None

                        latest_article = session.query(Article).filter(
                            Article.source == name,
                            Article.published_at.isnot(None)
                        ).order_by(Article.published_at.desc()).first()

                        if latest_article:
                            source_obj.latest_article_published_at = latest_article.published_at

                        session.commit()

                logger.info(f"  âœ… {name}: {process_result['total']} ç¯‡, æ–°å¢ {process_result['new']} ç¯‡, AIåˆ†æ {process_result['ai_analyzed']} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {name}: {e}")
                self._log_collection(db, name, "api", "error", 0, str(e), task_id=task_id)
                stats["sources_error"] += 1
                
                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == name).first()
                    if source_obj:
                        source_obj.last_error = str(e)
                        session.commit()

        logger.info(f"  âœ… APIé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢: {stats['new_articles']} ç¯‡, AIåˆ†æ: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _collect_web_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†ç½‘ç«™æºï¼ˆé€šè¿‡ç½‘é¡µçˆ¬å–ï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        # ä¼˜å…ˆä»æ•°æ®åº“è¯»å–Webæº
        web_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(
                RSSSource.enabled == True,
                RSSSource.source_type == "web"
            ).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                config = {
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                }
                
                # ä¼˜å…ˆä½¿ç”¨ extra_config å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä» note å­—æ®µè§£æ
                if source.extra_config:
                    try:
                        import json
                        extra_config = json.loads(source.extra_config)
                        if isinstance(extra_config, dict):
                            config["extra_config"] = extra_config
                    except:
                        pass
                elif source.note:
                    try:
                        import json
                        note_config = json.loads(source.note)
                        # å¦‚æœnoteæ˜¯extra_configæ ¼å¼ï¼Œå°†å…¶æ”¾å…¥extra_configå­—æ®µ
                        if isinstance(note_config, dict):
                            config["extra_config"] = note_config
                        else:
                            config["note"] = source.note
                    except:
                        config["note"] = source.note
                
                web_configs.append(config)
                # é¢„å…ˆåŠ è½½å±æ€§
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
            session.expunge_all()

        # åªä»æ•°æ®åº“è¯»å–æºï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æºåˆ™ä¸é‡‡é›†
        if not web_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„Webæºï¼Œè·³è¿‡é‡‡é›†")
            return stats

        logger.info(f"  ğŸš€ å¼€å§‹é‡‡é›† {len(web_configs)} ä¸ªç½‘ç«™æº")

        for config in web_configs:
            if not config.get("enabled", True):
                continue

            # åˆå¹¶ extra_config åˆ°ä¸»é…ç½®
            config = self._merge_extra_config(config)
            source_name = config.get("name", "Unknown")
            
            # å¦‚æœæ²¡æœ‰é…ç½®max_articlesï¼Œä½¿ç”¨å…¨å±€é…ç½®å€¼
            if "max_articles" not in config:
                from backend.app.core.settings import settings
                settings.load_collector_settings()
                config["max_articles"] = settings.MAX_ARTICLES_PER_SOURCE

            try:
                logger.info(f"  ğŸŒ å¼€å§‹é‡‡é›†ç½‘ç«™: {source_name}")

                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®ï¼ˆarticle_selectorï¼‰
                if not config.get("article_selector"):
                    logger.warning(f"  âš ï¸  {source_name}: ç¼ºå°‘ article_selector é…ç½®ï¼Œè·³è¿‡")
                    stats["sources_error"] += 1
                    self._log_collection(db, source_name, "web", "error", 0, "ç¼ºå°‘ article_selector é…ç½®", task_id=task_id)
                    continue

                articles = self.web_collector.fetch_articles(config)

                if not articles:
                    logger.info(f"  âš ï¸  {source_name}: æœªè·å–åˆ°æ–‡ç« ")
                    stats["sources_error"] += 1
                    self._log_collection(db, source_name, "web", "error", 0, "æœªè·å–åˆ°æ–‡ç« ", task_id=task_id)
                    continue

                process_result = self._process_articles_from_source(db, articles, source_name, "web", enable_ai_analysis, task_id=task_id)

                # æ›´æ–°Webæºçš„ç»Ÿè®¡ä¿¡æ¯
                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_collected_at = datetime.now()
                        source_obj.articles_count += len(articles)
                        source_obj.last_error = None

                        # æ›´æ–°æœ€æ–°æ–‡ç« å‘å¸ƒæ—¶é—´
                        latest_article = session.query(Article).filter(
                            Article.source == source_name,
                            Article.published_at.isnot(None)
                        ).order_by(Article.published_at.desc()).first()

                        if latest_article:
                            source_obj.latest_article_published_at = latest_article.published_at

                        session.commit()

                self._log_collection(db, source_name, "web", "success", process_result["total"], task_id=task_id)
                stats["sources_success"] += 1
                stats["new_articles"] += process_result["new"]
                stats["total_articles"] += process_result["total"]
                stats["ai_analyzed_count"] += process_result["ai_analyzed"]

                logger.info(f"  âœ… {source_name}: {process_result['total']} ç¯‡, æ–°å¢ {process_result['new']} ç¯‡, AIåˆ†æ {process_result['ai_analyzed']} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {source_name}: {e}")
                stats["sources_error"] += 1
                self._log_collection(db, source_name, "web", "error", 0, str(e), task_id=task_id)
                
                # æ›´æ–°é”™è¯¯ä¿¡æ¯
                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_error = str(e)
                        session.commit()

        logger.info(f"  âœ… ç½‘ç«™æºé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢: {stats['new_articles']} ç¯‡, AIåˆ†æ: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _collect_email_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†é‚®ä»¶æº

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        # ä»æ•°æ®åº“è¯»å–é‚®ä»¶æº
        email_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(
                RSSSource.enabled == True,
                RSSSource.source_type == "email"
            ).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                config = {
                    "id": source.id,
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                }

                if source.extra_config:
                    try:
                        import json
                        extra_config = json.loads(source.extra_config)
                        if isinstance(extra_config, dict):
                            config.update(extra_config)
                    except:
                        pass

                email_configs.append(config)
            session.expunge_all()

        # åªä»æ•°æ®åº“è¯»å–æºï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æºåˆ™ä¸é‡‡é›†
        if not email_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„é‚®ä»¶æºï¼Œè·³è¿‡é‡‡é›†")
            return stats

        logger.info(f"  ğŸš€ å¼€å§‹é‡‡é›† {len(email_configs)} ä¸ªé‚®ä»¶æº")

        for config in email_configs:
            if not config.get("enabled", True):
                continue

            # åˆå¹¶ extra_config åˆ°ä¸»é…ç½®
            config = self._merge_extra_config(config)
            source_name = config.get("name", "Unknown")

            try:
                logger.info(f"  ğŸ“§ å¼€å§‹é‡‡é›†é‚®ä»¶: {source_name}")

                # éªŒè¯é…ç½®
                is_valid, error_msg = self.email_collector.validate_config(config)
                if not is_valid:
                    logger.warning(f"  âš ï¸  {source_name}: {error_msg}")
                    stats["sources_error"] += 1
                    self._log_collection(db, source_name, "email", "error", 0, error_msg, task_id=task_id)
                    continue

                # é‡‡é›†æ–‡ç« 
                articles = self.email_collector.fetch_articles(config)

                if not articles:
                    logger.info(f"  âš ï¸  {source_name}: æœªè·å–åˆ°æ–‡ç« ")
                    stats["sources_error"] += 1
                    self._log_collection(db, source_name, "email", "error", 0, "æœªè·å–åˆ°æ–‡ç« ", task_id=task_id)
                    continue

                process_result = self._process_articles_from_source(db, articles, source_name, "email", enable_ai_analysis, task_id=task_id)

                # æ›´æ–°é‚®ä»¶æºçš„ç»Ÿè®¡ä¿¡æ¯
                source_id = config.get("id")
                with db.get_session() as session:
                    if source_id:
                        source_obj = session.query(RSSSource).filter(RSSSource.id == source_id).first()
                    else:
                        source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    
                    if source_obj:
                        source_obj.last_collected_at = datetime.now()
                        source_obj.articles_count = process_result.get("new_articles", 0) + process_result.get("skipped_articles", 0)
                        source_obj.last_error = None

                        # æ›´æ–°æœ€æ–°æ–‡ç« å‘å¸ƒæ—¶é—´
                        latest_article = session.query(Article).filter(
                            Article.source == source_name,
                            Article.published_at.isnot(None)
                        ).order_by(Article.published_at.desc()).first()

                        if latest_article:
                            source_obj.latest_article_published_at = latest_article.published_at

                        session.commit()

                self._log_collection(db, source_name, "email", "success", process_result["total"], task_id=task_id)
                stats["sources_success"] += 1
                stats["new_articles"] += process_result["new"]
                stats["total_articles"] += process_result["total"]
                stats["ai_analyzed_count"] += process_result["ai_analyzed"]

                logger.info(f"  âœ… {source_name}: {process_result['total']} ç¯‡, æ–°å¢ {process_result['new']} ç¯‡, AIåˆ†æ {process_result['ai_analyzed']} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {source_name}: {e}")
                stats["sources_error"] += 1
                self._log_collection(db, source_name, "email", "error", 0, str(e), task_id=task_id)
                
                # æ›´æ–°é”™è¯¯ä¿¡æ¯
                with db.get_session() as session:
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_error = str(e)
                        session.commit()

        logger.info(f"  âœ… é‚®ä»¶æºé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢: {stats['new_articles']} ç¯‡, AIåˆ†æ: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _save_or_update_article_and_get_id(self, db, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“å¹¶è¿”å›æ–‡ç« IDå’Œä¿¡æ¯

        Returns:
            {"id": int, "is_new": bool} - æ–‡ç« IDå’Œæ˜¯å¦ä¸ºæ–°æ–‡ç« 
            å¦‚æœä¿å­˜å¤±è´¥è¿”å›None
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with db.get_session() as session:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = session.query(Article).filter(Article.url == article["url"]).first()

                    if existing:
                        # æ–‡ç« å·²å­˜åœ¨ï¼Œæ›´æ–°å†…å®¹ï¼ˆå¦‚æœæ–°å†…å®¹æ›´å®Œæ•´ï¼‰
                        content = article.get("content", "")
                        if content and content.strip():  # å¦‚æœæœ‰æ–°å†…å®¹
                            # åªåœ¨å†…å®¹ä¸ºç©ºæˆ–æ˜æ˜¾æ›´çŸ­æ—¶æ‰æ›´æ–°
                            if not existing.content or (existing.content and len(content) > len(existing.content)):
                                existing.content = content
                                # æ›´æ–°sourceå­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¢é˜…æºåç§°
                                existing.source = article.get("source", existing.source)

                                session.commit()
                                return {"id": existing.id, "is_new": False}
                        return {"id": existing.id, "is_new": False}

                    # åˆ›å»ºæ–°æ–‡ç« 
                    content = article.get("content", "")
                    new_article = Article(
                        title=article.get("title"),
                        url=article.get("url"),
                        content=content,
                        source=article.get("source"),
                        category=article.get("category"),
                        author=article.get("author"),
                        published_at=article.get("published_at"),
                        extra_data=article.get("metadata"),
                    )

                    session.add(new_article)
                    session.commit()

                    # è¿”å›æ–°æ’å…¥çš„æ–‡ç« ID
                    return {"id": new_article.id, "is_new": True}

            except Exception as e:
                # å¦‚æœæ˜¯å”¯ä¸€æ€§çº¦æŸé”™è¯¯ï¼Œå¯èƒ½æ˜¯ç”±å¹¶å‘å¼•èµ·çš„ï¼Œé‡è¯•
                if "UNIQUE constraint failed" in str(e) and attempt < max_retries - 1:
                    logger.warning(f"âš ï¸  å¹¶å‘å†²çªï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {article.get('url', 'Unknown')}")
                    time.sleep(0.1 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    logger.error(f"âŒ ä¿å­˜æˆ–æ›´æ–°æ–‡ç« å¤±è´¥: {e}")
                    return None

        return None


    def _analyze_articles(self, db, batch_size: int = 50, max_age_days: int = None, max_workers: int = 3) -> Dict[str, Any]:
        """
        AIåˆ†ææœªåˆ†æçš„æ–‡ç« ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            max_age_days: æœ€å¤§æ–‡ç« å¹´é¾„ï¼ˆå¤©æ•°ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ–‡ç« ä¸åˆ†æã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3
        """
        from backend.app.core.settings import settings
        
        # å¦‚æœæœªæŒ‡å®šmax_age_daysï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼
        if max_age_days is None:
            max_age_days = settings.MAX_ANALYSIS_AGE_DAYS
        
        stats = {"analyzed_count": 0, "analysis_error": 0, "skipped_old": 0}

        with db.get_session() as session:
            # è®¡ç®—æ—¶é—´é˜ˆå€¼ï¼ˆåªåˆ†ææœ€è¿‘max_age_dayså¤©çš„æ–‡ç« ï¼‰
            # å¦‚æœmax_age_daysä¸º0ï¼Œè¡¨ç¤ºä¸é™åˆ¶ï¼Œåˆ†ææ‰€æœ‰æ–‡ç« 
            if max_age_days > 0:
                time_threshold = datetime.now() - timedelta(days=max_age_days)
            else:
                time_threshold = None
            
            # è·å–æœªåˆ†æçš„æ–‡ç« ï¼ˆåªåˆ†ææœ€è¿‘çš„æ–‡ç« ï¼‰
            query = session.query(Article).filter(
                Article.is_processed == False,
                Article.published_at.isnot(None)
            )
            
            # å¦‚æœé…ç½®äº†æ—¶é—´é™åˆ¶ï¼Œæ·»åŠ æ—¶é—´è¿‡æ»¤
            if time_threshold:
                query = query.filter(Article.published_at >= time_threshold)
            
            unanalyzed = query.order_by(Article.published_at.desc()).limit(batch_size).all()
            
            # ç»Ÿè®¡è·³è¿‡çš„æ—§æ–‡ç« ï¼ˆä»…åœ¨é…ç½®äº†æ—¶é—´é™åˆ¶æ—¶ï¼‰
            if time_threshold:
                skipped_count = (
                    session.query(Article)
                    .filter(
                        Article.is_processed == False,
                        Article.published_at.isnot(None),
                        Article.published_at < time_threshold
                    )
                    .count()
                )
            else:
                skipped_count = 0
            stats["skipped_old"] = skipped_count

            if not unanalyzed:
                if skipped_count > 0:
                    logger.info(f"  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æçš„æ–‡ç« ï¼ˆè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
                else:
                    logger.info("  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æçš„æ–‡ç« ")
                return stats

            logger.info(f"  ğŸ¤– å¼€å§‹å¹¶å‘åˆ†æ {len(unanalyzed)} ç¯‡æ–‡ç« ï¼ˆæŒ‰æ—¶é—´ä»æ–°åˆ°æ—§æ’åºï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼Œè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
            
            # æ˜¾ç¤ºå°†è¦åˆ†æçš„æ–‡ç« æ—¶é—´èŒƒå›´
            if unanalyzed:
                latest_date = unanalyzed[0].published_at
                oldest_date = unanalyzed[-1].published_at
                if latest_date and oldest_date:
                    logger.info(f"  ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: {oldest_date.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')}")

            # é¢„å…ˆåŠ è½½æ‰€æœ‰å±æ€§ï¼Œé¿å…åœ¨å¹¶å‘æ—¶å‡ºç°DetachedInstanceError
            for article in unanalyzed:
                _ = article.id
                _ = article.title
                _ = article.content
                _ = article.source
                _ = article.published_at
            
            session.expunge_all()

            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIAnalyzerå®ä¾‹ï¼Œé¿å…å¹¶å‘å†²çª
            # OpenAIå®¢æˆ·ç«¯å†…éƒ¨æœ‰è¿æ¥æ± ï¼Œå¤šçº¿ç¨‹å…±äº«ä¸å®‰å…¨
            from backend.app.utils.factories import create_ai_analyzer

            # å¹¶å‘åˆ†ææ–‡ç« 
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è· article.idï¼Œé¿å…é—­åŒ…é™·é˜±å’Œ DetachedInstanceError
            def analyze_single_article(article_obj, article_id=None):
                """åˆ†æå•ç¯‡æ–‡ç« ï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰"""
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIåˆ†æå™¨å®ä¾‹
                thread_ai_analyzer = create_ai_analyzer()

                # å¦‚æœä¼ å…¥çš„æ˜¯ article å¯¹è±¡ï¼Œæå– IDï¼›å¦åˆ™ä½¿ç”¨ä¼ å…¥çš„ article_id
                if article_id is None:
                    article_id = article_obj.id if hasattr(article_obj, 'id') else None

                try:
                    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                    with db.get_session() as article_session:
                        # é‡æ–°æŸ¥è¯¢æ–‡ç« ï¼ˆé¿å…DetachedInstanceErrorï¼‰
                        article_obj = article_session.query(Article).filter(Article.id == article_id).first()
                        if not article_obj:
                            return {"success": False, "reason": "article_not_found"}

                        # å¦‚æœå·²ç»åˆ†æè¿‡ï¼Œè·³è¿‡AIåˆ†æ
                        if article_obj.is_processed:
                            return {"success": False, "reason": "already_processed"}

                        # å‡†å¤‡æ–‡ç« æ•°æ®
                        article_dict = {
                            "title": article_obj.title,
                            "content": article_obj.content,
                            "source": article_obj.source,
                            "published_at": article_obj.published_at,
                        }

                        # è·å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæºé…ç½®äº†ï¼‰
                        custom_prompt = None
                        if article_obj.source:
                            source_obj = session.query(RSSSource).filter(
                                RSSSource.name == article_obj.source
                            ).first()
                            if source_obj and source_obj.analysis_prompt:
                                custom_prompt = source_obj.analysis_prompt

                        # AIåˆ†æï¼ˆä½¿ç”¨çº¿ç¨‹ç‹¬ç«‹çš„AIåˆ†æå™¨ï¼‰
                        result = thread_ai_analyzer.analyze_article(
                            article_dict, 
                            custom_prompt=custom_prompt
                        )

                        # æ›´æ–°æ–‡ç« 
                        # ç¡®ä¿ summary æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆAIå¯èƒ½è¿”å›dictï¼‰
                        summary_value = result.get("summary", "")
                        if isinstance(summary_value, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–æ–‡æœ¬å†…å®¹æˆ–è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                            if "text" in summary_value:
                                summary_value = summary_value["text"]
                            elif "content" in summary_value:
                                summary_value = summary_value["content"]
                            else:
                                import json
                                summary_value = json.dumps(summary_value, ensure_ascii=False)
                        elif not isinstance(summary_value, str):
                            summary_value = str(summary_value) if summary_value else ""

                        article_obj.summary = summary_value
                        article_obj.tags = result.get("tags")
                        article_obj.importance = result.get("importance")
                        article_obj.target_audience = result.get("target_audience")
                        # ä¿å­˜ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœAIåˆ†æè¿”å›äº†title_zhï¼‰
                        if result.get("title_zh"):
                            article_obj.title_zh = result.get("title_zh")
                        article_obj.is_processed = True

                        article_session.commit()
                        return {"success": True, "article_id": article_obj.id}
                        
                except Exception as e:
                    logger.error(f"  âŒ åˆ†ææ–‡ç« å¤±è´¥ (ID={article_id}): {e}")
                    return {"success": False, "error": str(e)}

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è· article.idï¼Œé¿å…é—­åŒ…é™·é˜±
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_article = {
                    executor.submit(analyze_single_article, article, article.id): article
                    for article in unanalyzed
                }
                
                completed = 0
                for future in as_completed(future_to_article):
                    article = future_to_article[future]
                    article_id = article.id  # æå‰ä¿å­˜ IDï¼Œé¿å… DetachedInstanceError
                    completed += 1
                    
                    try:
                        result = future.result()
                        if result.get("success"):
                            stats["analyzed_count"] += 1
                            if completed % 5 == 0 or completed == len(unanalyzed):
                                logger.info(f"  âœ… [{completed}/{len(unanalyzed)}] AIåˆ†æè¿›åº¦")
                        else:
                            stats["analysis_error"] += 1
                    except Exception as e:
                        logger.error(f"  âŒ åˆ†ææ–‡ç« å¼‚å¸¸ (ID={article_id}): {e}")
                        stats["analysis_error"] += 1

        logger.info(f"  âœ… AIåˆ†æå®Œæˆ: {stats['analyzed_count']} ç¯‡æˆåŠŸ, {stats['analysis_error']} ç¯‡å¤±è´¥")
        return stats

    def _analyze_articles_by_ids(self, db, article_ids: List[int], max_workers: int = 3) -> int:
        """
        æ ¹æ®æ–‡ç« IDåˆ—è¡¨è¿›è¡Œå¹¶å‘AIåˆ†æ

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æˆåŠŸåˆ†æçš„æ–‡ç« æ•°é‡
        """
        if not article_ids or not self.ai_analyzer:
            return 0

        analyzed_count = 0

        # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIAnalyzerå®ä¾‹ï¼Œé¿å…å¹¶å‘å†²çª
        # OpenAIå®¢æˆ·ç«¯å†…éƒ¨æœ‰è¿æ¥æ± ï¼Œå¤šçº¿ç¨‹å…±äº«ä¸å®‰å…¨
        from backend.app.utils.factories import create_ai_analyzer

        def analyze_single_article_id(article_id):
            """æ ¹æ®IDåˆ†æå•ç¯‡æ–‡ç« """
            try:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIåˆ†æå™¨å®ä¾‹
                thread_ai_analyzer = create_ai_analyzer()

                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                with db.get_session() as session:
                    # é‡æ–°æŸ¥è¯¢æ–‡ç« 
                    article_obj = session.query(Article).filter(Article.id == article_id).first()
                    if not article_obj or article_obj.is_processed:
                        return {"success": False, "reason": "already_processed"}

                    # å‡†å¤‡æ–‡ç« æ•°æ®
                    article_dict = {
                        "title": article_obj.title,
                        "content": article_obj.content,
                        "source": article_obj.source,
                        "published_at": article_obj.published_at,
                    }

                    # è·å–è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¦‚æœæºé…ç½®äº†ï¼‰
                    custom_prompt = None
                    if article_obj.source:
                        source_obj = session.query(RSSSource).filter(
                            RSSSource.name == article_obj.source
                        ).first()
                        if source_obj and source_obj.analysis_prompt:
                            custom_prompt = source_obj.analysis_prompt

                    # AIåˆ†æï¼ˆä½¿ç”¨çº¿ç¨‹ç‹¬ç«‹çš„AIåˆ†æå™¨ï¼‰
                    result = thread_ai_analyzer.analyze_article(
                        article_dict,
                        custom_prompt=custom_prompt
                    )

                    # æ›´æ–°æ–‡ç« 
                    # ç¡®ä¿ summary æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆAIå¯èƒ½è¿”å›dictï¼‰
                    summary_value = result.get("summary", "")
                    if isinstance(summary_value, dict):
                        # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–æ–‡æœ¬å†…å®¹æˆ–è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                        if "text" in summary_value:
                            summary_value = summary_value["text"]
                        elif "content" in summary_value:
                            summary_value = summary_value["content"]
                        else:
                            import json
                            summary_value = json.dumps(summary_value, ensure_ascii=False)
                    elif not isinstance(summary_value, str):
                        summary_value = str(summary_value) if summary_value else ""

                    article_obj.summary = summary_value
                    article_obj.tags = result.get("tags")
                    article_obj.importance = result.get("importance")
                    article_obj.target_audience = result.get("target_audience")
                    # ä¿å­˜ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœAIåˆ†æè¿”å›äº†title_zhï¼‰
                    if result.get("title_zh"):
                        article_obj.title_zh = result.get("title_zh")
                    article_obj.is_processed = True

                    session.commit()
                    return {"success": True, "article_id": article_obj.id}

            except Exception as e:
                logger.error(f"  âŒ åˆ†ææ–‡ç« å¤±è´¥ (ID={article_id}): {e}")
                return {"success": False, "error": str(e)}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {
                executor.submit(analyze_single_article_id, article_id): article_id
                for article_id in article_ids
            }

            completed = 0
            for future in as_completed(future_to_id):
                article_id = future_to_id[future]
                completed += 1

                try:
                    result = future.result()
                    if result.get("success"):
                        analyzed_count += 1
                        if completed % 5 == 0 or completed == len(article_ids):
                            logger.info(f"  âœ… [{completed}/{len(article_ids)}] AIåˆ†æè¿›åº¦")
                except Exception as e:
                    logger.error(f"  âŒ åˆ†ææ–‡ç« å¼‚å¸¸ (ID={article_id}): {e}")

        logger.info(f"  âœ… AIåˆ†æå®Œæˆ: {analyzed_count} ç¯‡")
        return analyzed_count

    def _filter_unanalyzed_articles(self, db, article_ids: List[int]) -> List[int]:
        """
        è¿‡æ»¤å‡ºæœªåˆ†æçš„æ–‡ç« IDåˆ—è¡¨

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨

        Returns:
            æœªåˆ†æçš„æ–‡ç« IDåˆ—è¡¨
        """
        if not article_ids:
            return []

        try:
            with db.get_session() as session:
                # æŸ¥è¯¢æœªåˆ†æçš„æ–‡ç« 
                unanalyzed = session.query(Article.id).filter(
                    Article.id.in_(article_ids),
                    Article.is_processed == False
                ).all()

                return [row[0] for row in unanalyzed]
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æœªåˆ†ææ–‡ç« å¤±è´¥: {e}")
            return article_ids  # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›æ‰€æœ‰IDç»§ç»­å¤„ç†

    def _recover_stuck_tasks(self, db):
        """
        æ£€æµ‹å¹¶æ¢å¤æŒ‚èµ·çš„é‡‡é›†ä»»åŠ¡
        
        å¦‚æœå‘ç°çŠ¶æ€ä¸ºrunningä½†è¶…è¿‡ä¸€å®šæ—¶é—´ï¼ˆé»˜è®¤1å°æ—¶ï¼‰çš„ä»»åŠ¡ï¼Œå°†å…¶æ ‡è®°ä¸ºerror
        """
        try:
            from backend.app.db.models import CollectionTask
            with db.get_session() as session:
                # æŸ¥æ‰¾æ‰€æœ‰runningçŠ¶æ€çš„ä»»åŠ¡
                running_tasks = session.query(CollectionTask).filter(
                    CollectionTask.status == "running"
                ).all()
                
                if not running_tasks:
                    return
                
                # è¶…æ—¶æ—¶é—´ï¼š1å°æ—¶
                timeout_threshold = timedelta(hours=1)
                current_time = datetime.now()
                
                for task in running_tasks:
                    # è®¡ç®—ä»»åŠ¡è¿è¡Œæ—¶é—´
                    running_time = current_time - task.started_at
                    
                    if running_time > timeout_threshold:
                        # ä»»åŠ¡è¶…æ—¶ï¼Œæ ‡è®°ä¸ºerror
                        logger.warning(
                            f"âš ï¸  æ£€æµ‹åˆ°æŒ‚èµ·çš„é‡‡é›†ä»»åŠ¡ (ID: {task.id})ï¼Œ"
                            f"è¿è¡Œæ—¶é—´: {running_time.total_seconds()/3600:.1f}å°æ—¶ï¼Œ"
                            f"å°†å…¶æ ‡è®°ä¸ºerrorçŠ¶æ€"
                        )
                        task.status = "error"
                        task.error_message = f"ä»»åŠ¡è¶…æ—¶ä¸­æ–­ï¼ˆè¿è¡Œæ—¶é—´è¶…è¿‡{timeout_threshold.total_seconds()/3600:.1f}å°æ—¶ï¼‰"
                        task.completed_at = current_time
                        session.commit()
                        logger.info(f"âœ… å·²æ¢å¤æŒ‚èµ·çš„ä»»åŠ¡ (ID: {task.id})")
                    else:
                        # ä»»åŠ¡è¿˜åœ¨è¿è¡Œä¸­ï¼Œä½†æ—¶é—´è¾ƒçŸ­ï¼Œå¯èƒ½æ˜¯æ­£å¸¸çš„
                        logger.debug(
                            f"â„¹ï¸  å‘ç°è¿è¡Œä¸­çš„ä»»åŠ¡ (ID: {task.id})ï¼Œ"
                            f"è¿è¡Œæ—¶é—´: {running_time.total_seconds()/60:.1f}åˆ†é’Ÿ"
                        )
        except Exception as e:
            logger.error(f"âŒ æ¢å¤æŒ‚èµ·ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    def _log_collection(self, db, source_name: str, source_type: str, status: str, count: int, error: str = None, task_id: int = None):
        """è®°å½•é‡‡é›†æ—¥å¿—"""
        try:
            with db.get_session() as session:
                log = CollectionLog(
                    source_name=source_name,
                    source_type=source_type,
                    status=status,
                    articles_count=count,
                    error_message=error,
                    task_id=task_id,
                )
                session.add(log)
                session.commit()
        except Exception as e:
            logger.error(f"âŒ è®°å½•æ—¥å¿—å¤±è´¥: {e}")

    def generate_daily_summary(self, db, date: datetime = None):
        """
        ç”Ÿæˆæ¯æ—¥æ€»ç»“

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            date: æ€»ç»“æ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰
        """
        if not self.summary_generator:
            logger.warning("âš ï¸  æœªåˆå§‹åŒ–AIåˆ†æå™¨ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“")
            return None

        return self.summary_generator.generate_daily_summary(db, date)

    def generate_weekly_summary(self, db, date: datetime = None):
        """
        ç”Ÿæˆæ¯å‘¨æ€»ç»“

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            date: æ€»ç»“æ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰
        """
        if not self.summary_generator:
            logger.warning("âš ï¸  æœªåˆå§‹åŒ–AIåˆ†æå™¨ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“")
            return None

        return self.summary_generator.generate_weekly_summary(db, date)
