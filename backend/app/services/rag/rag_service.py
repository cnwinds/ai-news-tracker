"""
RAGÊúçÂä° - ÂÆûÁé∞ÊñáÁ´†ÂêëÈáèÁ¥¢Âºï„ÄÅÊêúÁ¥¢ÂíåÈóÆÁ≠îÂäüËÉΩ
"""
import json
import logging
import numpy as np
import struct
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, text
from sqlalchemy.engine import Connection

from backend.app.db.models import Article, ArticleEmbedding
from backend.app.services.analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class RAGService:
    """RAGÊúçÂä°Á±ª"""

    def __init__(self, ai_analyzer: AIAnalyzer, db: Session):
        """
        ÂàùÂßãÂåñRAGÊúçÂä°

        Args:
            ai_analyzer: AIÂàÜÊûêÂô®ÂÆû‰æãÔºàÁî®‰∫éÁîüÊàêÂµåÂÖ•ÂêëÈáèÔºâ
            db: Êï∞ÊçÆÂ∫ì‰ºöËØù
        """
        self.ai_analyzer = ai_analyzer
        self.db = db
        self._use_sqlite_vec = self._check_sqlite_vec_available()

    def _check_sqlite_vec_available(self) -> bool:
        """Ê£ÄÊü•sqlite-vecÊâ©Â±ïÊòØÂê¶ÂèØÁî®"""
        try:
            # Â∞ùËØïÊü•ËØ¢vec0ËôöÊãüË°®
            result = self.db.execute(text("SELECT 1 FROM vec_embeddings LIMIT 1"))
            result.fetchone()
            logger.debug("‚úÖ sqlite-vecÊâ©Â±ïÂèØÁî®ÔºåÂ∞Ü‰ΩøÁî®SQLÂêëÈáèÊêúÁ¥¢")
            return True
        except Exception as e:
            logger.debug(f"‚ÑπÔ∏è  sqlite-vecÊâ©Â±ï‰∏çÂèØÁî®ÔºåÂ∞Ü‰ΩøÁî®PythonÂêëÈáèËÆ°ÁÆó: {e}")
            return False

    def _vector_to_blob(self, vector: List[float]) -> bytes:
        """Â∞ÜÂêëÈáèËΩ¨Êç¢‰∏∫BLOBÊ†ºÂºèÔºàsqlite-vecÈúÄË¶ÅÔºâ"""
        # sqlite-vecÊúüÊúõÁöÑÊ†ºÂºèÔºöÊµÆÁÇπÊï∞Êï∞ÁªÑÔºàÂ∞èÁ´ØÂ∫èÔºâ
        return struct.pack(f'{len(vector)}f', *vector)

    def _vector_to_match_string(self, vector: List[float]) -> str:
        """Â∞ÜÂêëÈáèËΩ¨Êç¢‰∏∫MATCHÊìç‰ΩúÁ¨¶ÈúÄË¶ÅÁöÑÂ≠óÁ¨¶‰∏≤Ê†ºÂºè"""
        # sqlite-vecÁöÑMATCHÊìç‰ΩúÁ¨¶ÈúÄË¶ÅJSONÊï∞ÁªÑÊ†ºÂºèÁöÑÂ≠óÁ¨¶‰∏≤
        return json.dumps(vector)

    def _combine_article_text(self, article: Article) -> str:
        """
        ÁªÑÂêàÊñáÁ´†ÁöÑÊâÄÊúâÂ≠óÊÆµ‰∏∫Á¥¢ÂºïÊñáÊú¨

        Args:
            article: ÊñáÁ´†ÂØπË±°

        Returns:
            ÁªÑÂêàÂêéÁöÑÊñáÊú¨
        """
        parts = []
        
        # Ê†áÈ¢ò
        if article.title:
            parts.append(f"Ê†áÈ¢ò: {article.title}")
        
        # ‰∏≠ÊñáÊ†áÈ¢ò
        if article.title_zh:
            parts.append(f"‰∏≠ÊñáÊ†áÈ¢ò: {article.title_zh}")
        
        # ÊëòË¶Å
        if article.summary:
            parts.append(f"ÊëòË¶Å: {article.summary}")
        
        # ÂÜÖÂÆπÔºàÊà™ÂèñÂâç2000Â≠óÁ¨¶ÔºåÁ∫¶2000 tokensÔºåÁ¨¶ÂêàÊúÄ‰Ω≥ÂÆûË∑µ256-512 tokensÁöÑ4ÂÄçËåÉÂõ¥Ôºâ
        # Â¶ÇÊûúÂ∑≤ÊúâÊëòË¶ÅÔºåÂÜÖÂÆπ‰Ωú‰∏∫Ë°•ÂÖÖ‰ø°ÊÅØÔºå‰∏çÈúÄË¶ÅÂ§™Èïø
        if article.content:
            # ‰ºòÂÖà‰ΩøÁî®ÊëòË¶ÅÔºåÂ¶ÇÊûúÊëòË¶ÅÂ≠òÂú®ÔºåÂÜÖÂÆπÂè™ÂèñÂâç2000Â≠óÁ¨¶‰Ωú‰∏∫Ë°•ÂÖÖ
            # Â¶ÇÊûúÊëòË¶Å‰∏çÂ≠òÂú®ÔºåÂàôÂèñÂâç3000Â≠óÁ¨¶
            max_content_length = 2000 if article.summary else 3000
            content_preview = article.content[:max_content_length]
            parts.append(f"ÂÜÖÂÆπ: {content_preview}")
        
        # ÂÖ≥ÈîÆÁÇπ
        if article.key_points:
            if isinstance(article.key_points, list):
                key_points_str = "„ÄÅ".join(article.key_points)
                parts.append(f"ÂÖ≥ÈîÆÁÇπ: {key_points_str}")
        
        # ‰∏ªÈ¢ò
        if article.topics:
            if isinstance(article.topics, list):
                topics_str = "„ÄÅ".join(article.topics)
                parts.append(f"‰∏ªÈ¢ò: {topics_str}")
        
        # Ê†áÁ≠æ
        if article.tags:
            if isinstance(article.tags, list):
                tags_str = "„ÄÅ".join(article.tags)
                parts.append(f"Ê†áÁ≠æ: {tags_str}")
        
        # Êù•Ê∫ê
        if article.source:
            parts.append(f"Êù•Ê∫ê: {article.source}")
        
        combined_text = "\n\n".join(parts)
        return combined_text if combined_text.strip() else ""

    def generate_embedding(self, text: str) -> List[float]:
        """
        ÁîüÊàêÊñáÊú¨ÁöÑÂµåÂÖ•ÂêëÈáè

        Args:
            text: Ë¶ÅÁîüÊàêÂµåÂÖ•ÂêëÈáèÁöÑÊñáÊú¨

        Returns:
            ÂµåÂÖ•ÂêëÈáèÂàóË°®
        """
        if not text or not text.strip():
            logger.warning("‚ö†Ô∏è  ÁîüÊàêÂµåÂÖ•ÂêëÈáèÊó∂ÊñáÊú¨‰∏∫Á©∫")
            return []
        
        return self.ai_analyzer.generate_embedding(text)

    def index_article(self, article: Article) -> bool:
        """
        Á¥¢ÂºïÂçïÁØáÊñáÁ´†

        Args:
            article: ÊñáÁ´†ÂØπË±°

        Returns:
            ÊòØÂê¶ÊàêÂäü
        """
        try:
            # Ê£ÄÊü•ÊòØÂê¶Â∑≤Á¥¢Âºï
            existing = self.db.query(ArticleEmbedding).filter(
                ArticleEmbedding.article_id == article.id
            ).first()
            
            if existing:
                logger.debug(f"ÊñáÁ´† {article.id} Â∑≤Â≠òÂú®Á¥¢ÂºïÔºåÂ∞ÜÊõ¥Êñ∞")
            
            # ÁîüÊàêÁ¥¢ÂºïÊñáÊú¨
            text_content = self._combine_article_text(article)
            if not text_content.strip():
                logger.warning(f"‚ö†Ô∏è  ÊñáÁ´† {article.id} Ê≤°ÊúâÂèØÁ¥¢ÂºïÁöÑÂÜÖÂÆπ")
                return False
            
            # ÁîüÊàêÂµåÂÖ•ÂêëÈáè
            logger.info(f"üìù Ê≠£Âú®‰∏∫ÊñáÁ´† {article.id} ÁîüÊàêÂµåÂÖ•ÂêëÈáè...")
            embedding = self.generate_embedding(text_content)
            
            if not embedding:
                logger.error(f"‚ùå ÊñáÁ´† {article.id} ÂµåÂÖ•ÂêëÈáèÁîüÊàêÂ§±Ë¥•")
                return False
            
            # ‰øùÂ≠òÊàñÊõ¥Êñ∞
            if existing:
                existing.embedding = embedding
                existing.text_content = text_content
                existing.embedding_model = self.ai_analyzer.embedding_model
                existing.updated_at = datetime.now()
            else:
                embedding_obj = ArticleEmbedding(
                    article_id=article.id,
                    embedding=embedding,
                    text_content=text_content,
                    embedding_model=self.ai_analyzer.embedding_model
                )
                self.db.add(embedding_obj)
            
            self.db.commit()
            
            # Â¶ÇÊûúsqlite-vecÂèØÁî®ÔºåÂêåÊ≠•Âà∞vec0ËôöÊãüË°®
            if self._use_sqlite_vec:
                try:
                    # sqlite-vecÁöÑvec0Ë°®ÈúÄË¶ÅÂ≠òÂÇ®ÊµÆÁÇπÊï∞Êï∞ÁªÑÔºàBLOBÊ†ºÂºèÔºâ
                    vector_blob = self._vector_to_blob(embedding)
                    self.db.execute(
                        text("""
                            INSERT OR REPLACE INTO vec_embeddings (article_id, embedding)
                            VALUES (:article_id, :embedding)
                        """),
                        {"article_id": article.id, "embedding": vector_blob}
                    )
                    self.db.commit()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  ÂêåÊ≠•ÂêëÈáèÂà∞vec0Ë°®Â§±Ë¥•: {e}")
            
            logger.info(f"‚úÖ ÊñáÁ´† {article.id} Á¥¢ÂºïÊàêÂäü")
            return True
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"‚ùå ÊñáÁ´† {article.id} Á¥¢ÂºïÂ§±Ë¥•: {e}")
            return False

    def index_articles_batch(self, articles: List[Article], batch_size: int = 10) -> Dict[str, Any]:
        """
        ÊâπÈáèÁ¥¢ÂºïÊñáÁ´†

        Args:
            articles: ÊñáÁ´†ÂàóË°®
            batch_size: ÊâπÂ§ÑÁêÜÂ§ßÂ∞è

        Returns:
            ÁªüËÆ°‰ø°ÊÅØ
        """
        total = len(articles)
        success_count = 0
        fail_count = 0
        
        logger.info(f"üöÄ ÂºÄÂßãÊâπÈáèÁ¥¢Âºï {total} ÁØáÊñáÁ´†...")
        
        for i, article in enumerate(articles, 1):
            try:
                if self.index_article(article):
                    success_count += 1
                else:
                    fail_count += 1
                
                if i % batch_size == 0:
                    logger.info(f"üìä ËøõÂ∫¶: {i}/{total} (ÊàêÂäü: {success_count}, Â§±Ë¥•: {fail_count})")
                    
            except Exception as e:
                logger.error(f"‚ùå ÊâπÈáèÁ¥¢ÂºïÊñáÁ´† {article.id} Êó∂Âá∫Èîô: {e}")
                fail_count += 1
        
        logger.info(f"‚úÖ ÊâπÈáèÁ¥¢ÂºïÂÆåÊàê: ÊÄªËÆ° {total}, ÊàêÂäü {success_count}, Â§±Ë¥• {fail_count}")
        
        return {
            "total": total,
            "success": success_count,
            "failed": fail_count
        }

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        ËÆ°ÁÆó‰∏§‰∏™ÂêëÈáèÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶

        Args:
            vec1: ÂêëÈáè1
            vec2: ÂêëÈáè2

        Returns:
            Áõ∏‰ººÂ∫¶ÂàÜÊï∞ (0-1)
        """
        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            logger.error(f"‚ùå ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶Â§±Ë¥•: {e}")
            return 0.0

    def search_articles(
        self,
        query: str,
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        ËØ≠‰πâÊêúÁ¥¢ÊñáÁ´†

        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            top_k: ËøîÂõûÂâçk‰∏™ÁªìÊûú
            filters: ËøáÊª§Êù°‰ª∂Ôºàsources, importance, time_rangeÁ≠âÔºâ

        Returns:
            ÊêúÁ¥¢ÁªìÊûúÂàóË°®ÔºåÊØè‰∏™ÁªìÊûúÂåÖÂê´ÊñáÁ´†‰ø°ÊÅØÂíåÁõ∏‰ººÂ∫¶ÂàÜÊï∞
        """
        try:
            # ÁîüÊàêÊü•ËØ¢ÂêëÈáè
            logger.info(f"üîç Ê≠£Âú®ÊêúÁ¥¢: {query[:50]}...")
            query_embedding = self.generate_embedding(query)
            
            if not query_embedding:
                logger.error("‚ùå Êü•ËØ¢ÂêëÈáèÁîüÊàêÂ§±Ë¥•")
                return []
            
            # Â¶ÇÊûúsqlite-vecÂèØÁî®Ôºå‰ΩøÁî®SQLÂêëÈáèÊêúÁ¥¢
            if self._use_sqlite_vec:
                return self._search_with_sqlite_vec(query_embedding, top_k, filters)
            else:
                # ÂõûÈÄÄÂà∞PythonÂêëÈáèËÆ°ÁÆó
                return self._search_with_python(query_embedding, top_k, filters)
            
        except Exception as e:
            logger.error(f"‚ùå ÊêúÁ¥¢Â§±Ë¥•: {e}")
            return []

    def _search_with_sqlite_vec(
        self,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """‰ΩøÁî®sqlite-vecËøõË°åÂêëÈáèÊêúÁ¥¢"""
        try:
            # Ê£ÄÊü•vec_embeddingsË°®ÊòØÂê¶ÊúâÊï∞ÊçÆ
            vec_count = self.db.execute(text("SELECT COUNT(*) FROM vec_embeddings")).scalar()
            logger.debug(f"vec_embeddingsË°®‰∏≠Êúâ {vec_count} Êù°ËÆ∞ÂΩï")
            if vec_count == 0:
                logger.warning("‚ö†Ô∏è  vec_embeddingsË°®‰∏∫Á©∫ÔºåÂõûÈÄÄÂà∞PythonËÆ°ÁÆó")
                return self._search_with_python(query_embedding, top_k, filters)
            
            # Ê£ÄÊü•Êü•ËØ¢ÂêëÈáèÁª¥Â∫¶ÊòØÂê¶‰∏éÊï∞ÊçÆÂ∫ì‰∏≠Â≠òÂÇ®ÁöÑÂêëÈáèÁª¥Â∫¶ÂåπÈÖç
            # ‰ªéarticle_embeddingsË°®Ëé∑Âèñ‰∏Ä‰∏™Ê†∑Êú¨ÂêëÈáèÊù•Ê£ÄÊü•Áª¥Â∫¶
            query_dim = len(query_embedding)
            sample_embedding = self.db.query(ArticleEmbedding).first()
            if sample_embedding and sample_embedding.embedding:
                stored_dim = len(sample_embedding.embedding)
                logger.debug(f"Êü•ËØ¢ÂêëÈáèÁª¥Â∫¶: {query_dim}, Â≠òÂÇ®ÂêëÈáèÁª¥Â∫¶: {stored_dim}")
                if query_dim != stored_dim:
                    logger.warning(
                        f"‚ö†Ô∏è  ÂêëÈáèÁª¥Â∫¶‰∏çÂåπÈÖçÔºöÊü•ËØ¢ÂêëÈáèÁª¥Â∫¶ {query_dim}Ôºå"
                        f"Â≠òÂÇ®ÂêëÈáèÁª¥Â∫¶ {stored_dim}ÔºåÂõûÈÄÄÂà∞PythonËÆ°ÁÆó"
                    )
                    return self._search_with_python(query_embedding, top_k, filters)
            else:
                logger.warning("‚ö†Ô∏è  Êú™ÊâæÂà∞Â∑≤Á¥¢ÂºïÁöÑÊñáÁ´†ÂêëÈáèÔºåÂõûÈÄÄÂà∞PythonËÆ°ÁÆó")
                return self._search_with_python(query_embedding, top_k, filters)
            
            # sqlite-vec‰ΩøÁî®MATCHÊìç‰ΩúÁ¨¶ÔºåÈúÄË¶ÅJSONÊï∞ÁªÑÊ†ºÂºèÁöÑÂ≠óÁ¨¶‰∏≤
            # ÊàñËÄÖÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®BLOBÊ†ºÂºè
            query_vector_str = self._vector_to_match_string(query_embedding)
            
            # ÊûÑÂª∫Âü∫Á°ÄÊü•ËØ¢ - ‰ΩøÁî®MATCHÊìç‰ΩúÁ¨¶
            # vec0 ÁöÑ MATCH ÈúÄË¶ÅÊòéÁ°ÆÊåáÂÆö k ÂèÇÊï∞ÔºöMATCH ? AND k = 10
            # Ê≥®ÊÑèÔºök ÂèÇÊï∞ÂøÖÈ°ªÂ§ß‰∫éÁ≠â‰∫é top_kÔºåÊàë‰ª¨‰ΩøÁî® top_k * 2 ‰ª•Á°Æ‰øùÊúâË∂≥Â§üÁöÑÁªìÊûúÁî®‰∫éËøáÊª§
            # k ÂèÇÊï∞ÂøÖÈ°ªÁõ¥Êé•ÂÜôÂú® SQL ‰∏≠Ôºå‰∏çËÉΩ‰Ωú‰∏∫ÂèÇÊï∞ÁªëÂÆö
            k_value = max(top_k * 2, 10)  # Ëá≥Â∞ëËøîÂõû 10 ‰∏™ÁªìÊûú
            
            # ÊûÑÂª∫Âü∫Á°ÄÊü•ËØ¢
            sql = f"""
                SELECT 
                    v.article_id,
                    distance,
                    a.id, a.title, a.title_zh, a.url, a.summary, a.source,
                    a.published_at, a.importance, a.topics, a.tags
                FROM vec_embeddings v
                JOIN articles a ON v.article_id = a.id
                WHERE v.embedding MATCH :query_vector AND k = {k_value}
            """
            
            params = {
                "query_vector": query_vector_str
            }
            
            # Ê∑ªÂä†ËøáÊª§Êù°‰ª∂
            if filters:
                conditions = []
                if filters.get("sources"):
                    placeholders = ",".join([f":source_{i}" for i in range(len(filters["sources"]))])
                    conditions.append(f"a.source IN ({placeholders})")
                    for i, source in enumerate(filters["sources"]):
                        params[f"source_{i}"] = source
                
                if filters.get("importance"):
                    placeholders = ",".join([f":importance_{i}" for i in range(len(filters["importance"]))])
                    conditions.append(f"a.importance IN ({placeholders})")
                    for i, imp in enumerate(filters["importance"]):
                        params[f"importance_{i}"] = imp
                
                if filters.get("time_from"):
                    conditions.append("a.published_at >= :time_from")
                    params["time_from"] = filters["time_from"]
                
                if filters.get("time_to"):
                    conditions.append("a.published_at <= :time_to")
                    params["time_to"] = filters["time_to"]
                
                if conditions:
                    sql += " AND " + " AND ".join(conditions)
            
            # ÊúÄÂêéÈôêÂà∂ËøîÂõûÁöÑÁªìÊûúÊï∞ÈáèÔºàk Â∑≤ÁªèÈôêÂà∂‰∫Ü KNN ÁªìÊûúÔºåËøôÈáåÂÜçÈôêÂà∂ÊúÄÁªàËøîÂõûÊï∞ÈáèÔºâ
            sql += f" ORDER BY distance LIMIT {top_k}"
            
            # ÊâßË°åÊü•ËØ¢
            result = self.db.execute(text(sql), params)
            rows = result.fetchall()
            
            # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè
            search_results = []
            for row in rows:
                # sqlite-vecËøîÂõûÁöÑdistanceÊòØÊ¨ßÊ∞èË∑ùÁ¶ªÔºåÈúÄË¶ÅËΩ¨Êç¢‰∏∫Áõ∏‰ººÂ∫¶
                # Áõ∏‰ººÂ∫¶ = 1 / (1 + distance)
                distance = float(row[1]) if row[1] is not None else float('inf')
                similarity = 1.0 / (1.0 + distance) if distance < float('inf') else 0.0
                
                # Â§ÑÁêÜ published_atÔºöÂèØËÉΩÊòØ datetime ÂØπË±°ÊàñÂ≠óÁ¨¶‰∏≤
                published_at = row[8]
                if published_at:
                    if isinstance(published_at, datetime):
                        published_at_str = published_at.isoformat()
                    elif isinstance(published_at, str):
                        published_at_str = published_at
                    else:
                        published_at_str = str(published_at)
                else:
                    published_at_str = None
                
                # Â§ÑÁêÜ topicsÔºöÂèØËÉΩÊòØÂàóË°®Êàñ JSON Â≠óÁ¨¶‰∏≤
                topics = row[10]
                if topics:
                    if isinstance(topics, str):
                        try:
                            topics = json.loads(topics)
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"Êó†Ê≥ïËß£Êûê topics JSON: {topics}")
                            topics = []
                    elif not isinstance(topics, list):
                        topics = []
                else:
                    topics = []
                
                # Â§ÑÁêÜ tagsÔºöÂèØËÉΩÊòØÂàóË°®Êàñ JSON Â≠óÁ¨¶‰∏≤
                tags = row[11]
                if tags:
                    if isinstance(tags, str):
                        try:
                            tags = json.loads(tags)
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"Êó†Ê≥ïËß£Êûê tags JSON: {tags}")
                            tags = []
                    elif not isinstance(tags, list):
                        tags = []
                else:
                    tags = []
                
                search_results.append({
                    "id": row[2],
                    "title": row[3],
                    "title_zh": row[4],
                    "url": row[5],
                    "summary": row[6],
                    "source": row[7],
                    "published_at": published_at_str,
                    "importance": row[9],
                    "topics": topics,
                    "tags": tags,
                    "similarity": similarity
                })
            
            # ÂéªÈáçÔºöÊåâÊñáÁ´†IDÂéªÈáçÔºå‰øùÁïôÁõ∏‰ººÂ∫¶ÊúÄÈ´òÁöÑËÆ∞ÂΩï
            seen_article_ids = {}
            deduplicated_results = []
            for result in search_results:
                article_id = result["id"]
                if article_id not in seen_article_ids:
                    seen_article_ids[article_id] = result
                    deduplicated_results.append(result)
                else:
                    # Â¶ÇÊûúÂ∑≤Â≠òÂú®ÔºåÊØîËæÉÁõ∏‰ººÂ∫¶Ôºå‰øùÁïôÊõ¥È´òÁöÑ
                    existing = seen_article_ids[article_id]
                    if result["similarity"] > existing["similarity"]:
                        # ÊõøÊç¢‰∏∫Áõ∏‰ººÂ∫¶Êõ¥È´òÁöÑËÆ∞ÂΩï
                        index = deduplicated_results.index(existing)
                        deduplicated_results[index] = result
                        seen_article_ids[article_id] = result
            
            # ÊåâÁõ∏‰ººÂ∫¶ÈáçÊñ∞ÊéíÂ∫èÔºàÂéªÈáçÂêéÂèØËÉΩÈ°∫Â∫èÊîπÂèòÔºâ
            deduplicated_results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # ÈôêÂà∂ËøîÂõûÊï∞Èáè
            final_results = deduplicated_results[:top_k]
            
            logger.info(f"‚úÖ ÊêúÁ¥¢ÂÆåÊàêÔºà‰ΩøÁî®sqlite-vecÔºâÔºåÊâæÂà∞ {len(search_results)} ‰∏™ÁªìÊûúÔºåÂéªÈáçÂêé {len(final_results)} ‰∏™")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå sqlite-vecÊêúÁ¥¢Â§±Ë¥•: {e}ÔºåÂõûÈÄÄÂà∞PythonËÆ°ÁÆó")
            return self._search_with_python(query_embedding, top_k, filters)

    def _search_with_python(
        self,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """‰ΩøÁî®PythonËøõË°åÂêëÈáèÊêúÁ¥¢ÔºàÂõûÈÄÄÊñπÊ°àÔºâ"""
        # Ëé∑ÂèñÊâÄÊúâÂ∑≤Á¥¢ÂºïÁöÑÊñáÁ´†ÂµåÂÖ•
        query_obj = self.db.query(ArticleEmbedding, Article).join(
            Article, ArticleEmbedding.article_id == Article.id
        )
        
        # Â∫îÁî®ËøáÊª§Êù°‰ª∂
        if filters:
            if filters.get("sources"):
                query_obj = query_obj.filter(Article.source.in_(filters["sources"]))
            
            if filters.get("importance"):
                query_obj = query_obj.filter(Article.importance.in_(filters["importance"]))
            
            if filters.get("time_from"):
                query_obj = query_obj.filter(Article.published_at >= filters["time_from"])
            
            if filters.get("time_to"):
                query_obj = query_obj.filter(Article.published_at <= filters["time_to"])
        
        # Ëé∑ÂèñÊâÄÊúâÂåπÈÖçÁöÑÊñáÁ´†ÂµåÂÖ•
        embeddings = query_obj.all()
        
        if not embeddings:
            logger.warning("‚ö†Ô∏è  Ê≤°ÊúâÊâæÂà∞Â∑≤Á¥¢ÂºïÁöÑÊñáÁ´†")
            return []
        
        # Ê£ÄÊü•Êü•ËØ¢ÂêëÈáèÁª¥Â∫¶
        query_dim = len(query_embedding)
        
        # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶
        results = []
        for embedding_obj, article in embeddings:
            if not embedding_obj.embedding:
                continue
            
            stored_dim = len(embedding_obj.embedding)
            if query_dim != stored_dim:
                # Ë∑≥ËøáÁª¥Â∫¶‰∏çÂåπÈÖçÁöÑÂêëÈáè
                logger.debug(
                    f"‚ö†Ô∏è  Ë∑≥ËøáÁª¥Â∫¶‰∏çÂåπÈÖçÁöÑÊñáÁ´† {article.id}Ôºö"
                    f"Êü•ËØ¢ÂêëÈáèÁª¥Â∫¶ {query_dim}ÔºåÂ≠òÂÇ®ÂêëÈáèÁª¥Â∫¶ {stored_dim}"
                )
                continue
            
            similarity = self._cosine_similarity(query_embedding, embedding_obj.embedding)
            results.append({
                "article": article,
                "similarity": similarity,
                "embedding_id": embedding_obj.id
            })
        
        # ÊåâÁõ∏‰ººÂ∫¶ÊéíÂ∫è
        results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # ËøîÂõûtop_k
        top_results = results[:top_k]
        
        # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè
        search_results = []
        for result in top_results:
            article = result["article"]
            
            # Â§ÑÁêÜ topicsÔºöÁ°Æ‰øùÊòØÂàóË°®
            topics = article.topics
            if topics and isinstance(topics, str):
                try:
                    topics = json.loads(topics)
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"Êó†Ê≥ïËß£Êûê topics JSON: {topics}")
                    topics = []
            elif not isinstance(topics, list):
                topics = topics if topics else []
            
            # Â§ÑÁêÜ tagsÔºöÁ°Æ‰øùÊòØÂàóË°®
            tags = article.tags
            if tags and isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"Êó†Ê≥ïËß£Êûê tags JSON: {tags}")
                    tags = []
            elif not isinstance(tags, list):
                tags = tags if tags else []
            
            search_results.append({
                "id": article.id,
                "title": article.title,
                "title_zh": article.title_zh,
                "url": article.url,
                "summary": article.summary,
                "source": article.source,
                "published_at": article.published_at.isoformat() if article.published_at else None,
                "importance": article.importance,
                "topics": topics,
                "tags": tags,
                "similarity": result["similarity"]
            })
        
        # ÂéªÈáçÔºöÊåâÊñáÁ´†IDÂéªÈáçÔºå‰øùÁïôÁõ∏‰ººÂ∫¶ÊúÄÈ´òÁöÑËÆ∞ÂΩï
        seen_article_ids = {}
        deduplicated_results = []
        for result in search_results:
            article_id = result["id"]
            if article_id not in seen_article_ids:
                seen_article_ids[article_id] = result
                deduplicated_results.append(result)
            else:
                # Â¶ÇÊûúÂ∑≤Â≠òÂú®ÔºåÊØîËæÉÁõ∏‰ººÂ∫¶Ôºå‰øùÁïôÊõ¥È´òÁöÑ
                existing = seen_article_ids[article_id]
                if result["similarity"] > existing["similarity"]:
                    # ÊõøÊç¢‰∏∫Áõ∏‰ººÂ∫¶Êõ¥È´òÁöÑËÆ∞ÂΩï
                    index = deduplicated_results.index(existing)
                    deduplicated_results[index] = result
                    seen_article_ids[article_id] = result
        
        # ÊåâÁõ∏‰ººÂ∫¶ÈáçÊñ∞ÊéíÂ∫èÔºàÂéªÈáçÂêéÂèØËÉΩÈ°∫Â∫èÊîπÂèòÔºâ
        deduplicated_results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # ÈôêÂà∂ËøîÂõûÊï∞Èáè
        final_results = deduplicated_results[:top_k]
        
        logger.info(f"‚úÖ ÊêúÁ¥¢ÂÆåÊàêÔºà‰ΩøÁî®PythonËÆ°ÁÆóÔºâÔºåÊâæÂà∞ {len(search_results)} ‰∏™ÁªìÊûúÔºåÂéªÈáçÂêé {len(final_results)} ‰∏™")
        return final_results

    def query_articles(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        RAGÈóÆÁ≠îÔºöÂü∫‰∫éÊ£ÄÁ¥¢Âà∞ÁöÑÊñáÁ´†ÂõûÁ≠îÈóÆÈ¢ò

        Args:
            question: ÈóÆÈ¢òÊñáÊú¨
            top_k: Ê£ÄÁ¥¢ÁöÑÊñáÁ´†Êï∞Èáè

        Returns:
            ÂåÖÂê´Á≠îÊ°àÂíåÂºïÁî®ÊñáÁ´†ÁöÑÂ≠óÂÖ∏
        """
        try:
            logger.info(f"üîç ÂºÄÂßãÈóÆÁ≠îÊµÅÁ®ã: question={question[:100]}, top_k={top_k}")
            
            # Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†
            try:
                relevant_articles = self.search_articles(question, top_k=top_k)
                logger.info(f"‚úÖ Ê£ÄÁ¥¢Âà∞ {len(relevant_articles)} ÁØáÁõ∏ÂÖ≥ÊñáÁ´†")
            except Exception as e:
                logger.error(f"‚ùå Ê£ÄÁ¥¢ÊñáÁ´†Â§±Ë¥•: {e}", exc_info=True)
                import traceback
                logger.error(f"Ê£ÄÁ¥¢ÊñáÁ´†ÂÆåÊï¥Â†ÜÊ†à:\n{traceback.format_exc()}")
                raise
            
            if not relevant_articles:
                logger.warning("‚ö†Ô∏è  Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÊñáÁ´†")
                return {
                    "answer": "Êä±Ê≠âÔºåÊ≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÁöÑÊñáÁ´†Êù•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÇ",
                    "sources": [],
                    "articles": []
                }
            
            # ÊûÑÂª∫‰∏ä‰∏ãÊñá
            try:
                context_parts = []
                for i, article_info in enumerate(relevant_articles, 1):
                    try:
                        article_text = f"""
ÊñáÁ´† {i}:
Ê†áÈ¢ò: {article_info.get('title', 'N/A')}
"""
                        if article_info.get('title_zh'):
                            article_text += f"‰∏≠ÊñáÊ†áÈ¢ò: {article_info['title_zh']}\n"
                        if article_info.get('summary'):
                            article_text += f"ÊëòË¶Å: {article_info['summary']}\n"
                        if article_info.get('topics'):
                            topics = article_info['topics']
                            if isinstance(topics, list):
                                article_text += f"‰∏ªÈ¢ò: {', '.join(topics)}\n"
                            else:
                                article_text += f"‰∏ªÈ¢ò: {topics}\n"
                        article_text += f"Êù•Ê∫ê: {article_info.get('source', 'N/A')}\n"
                        article_text += f"Áõ∏‰ººÂ∫¶: {article_info.get('similarity', 0):.3f}\n"
                        
                        context_parts.append(article_text)
                    except Exception as e:
                        logger.error(f"‚ùå ÊûÑÂª∫ÊñáÁ´† {i} ‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}", exc_info=True)
                        logger.error(f"ÊñáÁ´†‰ø°ÊÅØ: {article_info}")
                        continue
                
                context = "\n---\n".join(context_parts)
                logger.info(f"‚úÖ ÊûÑÂª∫‰∏ä‰∏ãÊñáÂÆåÊàêÔºåÈïøÂ∫¶: {len(context)} Â≠óÁ¨¶")
            except Exception as e:
                logger.error(f"‚ùå ÊûÑÂª∫‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}", exc_info=True)
                import traceback
                logger.error(f"ÊûÑÂª∫‰∏ä‰∏ãÊñáÂÆåÊï¥Â†ÜÊ†à:\n{traceback.format_exc()}")
                raise
            
            # ÊûÑÂª∫ÊèêÁ§∫ËØç
            try:
                prompt = f"""Âü∫‰∫é‰ª•‰∏ãÊñáÁ´†ÂÜÖÂÆπÔºåÂõûÁ≠îÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇËØ∑‰ΩøÁî®‰∏≠ÊñáÂõûÁ≠îÔºåÂπ∂ÂºïÁî®ÂÖ∑‰ΩìÁöÑÊñáÁ´†„ÄÇ

Áõ∏ÂÖ≥ÊñáÁ´†Ôºö
{context}

Áî®Êà∑ÈóÆÈ¢òÔºö{question}

ËØ∑Êèê‰æõËØ¶ÁªÜ„ÄÅÂáÜÁ°ÆÁöÑÁ≠îÊ°àÔºåÂπ∂Âú®ÂõûÁ≠î‰∏≠ÂºïÁî®Áõ∏ÂÖ≥ÁöÑÊñáÁ´†„ÄÇÂ¶ÇÊûúÊñáÁ´†‰∏≠Ê≤°ÊúâË∂≥Â§üÁöÑ‰ø°ÊÅØÊù•ÂõûÁ≠îÈóÆÈ¢òÔºåËØ∑ËØ¥Êòé„ÄÇ"""
                logger.info(f"‚úÖ ÊèêÁ§∫ËØçÊûÑÂª∫ÂÆåÊàêÔºåÈïøÂ∫¶: {len(prompt)} Â≠óÁ¨¶")
            except Exception as e:
                logger.error(f"‚ùå ÊûÑÂª∫ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}", exc_info=True)
                raise
            
            # Ë∞ÉÁî®LLMÁîüÊàêÁ≠îÊ°à
            try:
                logger.info(f"ü§ñ Ê≠£Âú®Ë∞ÉÁî®LLMÁîüÊàêÁ≠îÊ°à...")
                logger.debug(f"‰ΩøÁî®Ê®°Âûã: {self.ai_analyzer.model}")
                logger.debug(f"ÊèêÁ§∫ËØçÂâç100Â≠óÁ¨¶: {prompt[:100]}")
                
                response = self.ai_analyzer.client.chat.completions.create(
                    model=self.ai_analyzer.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑAIÊñ∞ÈóªÂä©ÊâãÔºåÊìÖÈïøÂü∫‰∫éÊèê‰æõÁöÑÊñáÁ´†ÂÜÖÂÆπÂõûÁ≠îÈóÆÈ¢ò„ÄÇËØ∑‰ΩøÁî®‰∏≠ÊñáÂõûÁ≠îÔºåÂπ∂ÂáÜÁ°ÆÂºïÁî®ÊñáÁ´†Êù•Ê∫ê„ÄÇ"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000,
                )
                
                logger.info(f"‚úÖ LLMÂìçÂ∫îÊé•Êî∂ÊàêÂäü")
                logger.debug(f"ÂìçÂ∫îÂØπË±°Á±ªÂûã: {type(response)}")
                logger.debug(f"ÂìçÂ∫îchoicesÊï∞Èáè: {len(response.choices) if hasattr(response, 'choices') else 0}")
                
                if not response.choices:
                    raise ValueError("LLMÂìçÂ∫î‰∏≠Ê≤°Êúâchoices")
                
                answer = response.choices[0].message.content.strip()
                logger.info(f"‚úÖ Á≠îÊ°àÁîüÊàêÊàêÂäüÔºåÈïøÂ∫¶: {len(answer)} Â≠óÁ¨¶")
                
            except Exception as e:
                logger.error(f"‚ùå Ë∞ÉÁî®LLMÂ§±Ë¥•: {e}", exc_info=True)
                logger.error(f"LLMÂÆ¢Êà∑Á´ØÁ±ªÂûã: {type(self.ai_analyzer.client)}")
                logger.error(f"Ê®°ÂûãÂêçÁß∞: {self.ai_analyzer.model}")
                import traceback
                logger.error(f"LLMË∞ÉÁî®ÂÆåÊï¥Â†ÜÊ†à:\n{traceback.format_exc()}")
                raise
            
            # ÊûÑÂª∫ËøîÂõûÁªìÊûú
            try:
                sources = [article.get("source", "N/A") for article in relevant_articles]
                result = {
                    "answer": answer,
                    "sources": sources,
                    "articles": relevant_articles
                }
                logger.info(f"‚úÖ ÈóÆÁ≠îÊµÅÁ®ãÂÆåÊàê: answerÈïøÂ∫¶={len(answer)}, sourcesÊï∞Èáè={len(sources)}, articlesÊï∞Èáè={len(relevant_articles)}")
                return result
            except Exception as e:
                logger.error(f"‚ùå ÊûÑÂª∫ËøîÂõûÁªìÊûúÂ§±Ë¥•: {e}", exc_info=True)
                import traceback
                logger.error(f"ÊûÑÂª∫ËøîÂõûÁªìÊûúÂÆåÊï¥Â†ÜÊ†à:\n{traceback.format_exc()}")
                raise
            
        except Exception as e:
            logger.error(f"‚ùå ÈóÆÁ≠îÂ§±Ë¥•: {e}", exc_info=True)
            import traceback
            logger.error(f"ÈóÆÁ≠îÂÆåÊï¥Â†ÜÊ†àË∑üË∏™:\n{traceback.format_exc()}")
            return {
                "answer": f"Êä±Ê≠âÔºåÁîüÊàêÁ≠îÊ°àÊó∂Âá∫Áé∞ÈîôËØØ: {str(e)}",
                "sources": [],
                "articles": []
            }

    def get_index_stats(self) -> Dict[str, Any]:
        """
        Ëé∑ÂèñÁ¥¢ÂºïÁªüËÆ°‰ø°ÊÅØ

        Returns:
            ÁªüËÆ°‰ø°ÊÅØÂ≠óÂÖ∏
        """
        try:
            total_articles = self.db.query(Article).count()
            indexed_articles = self.db.query(ArticleEmbedding).count()
            unindexed_articles = total_articles - indexed_articles
            
            # ÊåâÊù•Ê∫êÁªüËÆ°
            source_stats = {}
            embeddings = self.db.query(ArticleEmbedding, Article).join(
                Article, ArticleEmbedding.article_id == Article.id
            ).all()
            
            for embedding_obj, article in embeddings:
                source = article.source
                if source not in source_stats:
                    source_stats[source] = 0
                source_stats[source] += 1
            
            return {
                "total_articles": total_articles,
                "indexed_articles": indexed_articles,
                "unindexed_articles": unindexed_articles,
                "index_coverage": indexed_articles / total_articles if total_articles > 0 else 0.0,
                "source_stats": source_stats
            }
        except Exception as e:
            logger.error(f"‚ùå Ëé∑ÂèñÁ¥¢ÂºïÁªüËÆ°Â§±Ë¥•: {e}")
            return {
                "total_articles": 0,
                "indexed_articles": 0,
                "unindexed_articles": 0,
                "index_coverage": 0.0,
                "source_stats": {}
            }

