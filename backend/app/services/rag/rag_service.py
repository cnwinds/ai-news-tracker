"""
RAGæœåŠ¡ - å®ç°æ–‡ç« å‘é‡ç´¢å¼•ã€æœç´¢å’Œé—®ç­”åŠŸèƒ½
"""
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_

from backend.app.db.models import Article, ArticleEmbedding
from backend.app.services.analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class RAGService:
    """RAGæœåŠ¡ç±»"""

    def __init__(self, ai_analyzer: AIAnalyzer, db: Session):
        """
        åˆå§‹åŒ–RAGæœåŠ¡

        Args:
            ai_analyzer: AIåˆ†æå™¨å®ä¾‹ï¼ˆç”¨äºç”ŸæˆåµŒå…¥å‘é‡ï¼‰
            db: æ•°æ®åº“ä¼šè¯
        """
        self.ai_analyzer = ai_analyzer
        self.db = db
        self._init_vector_extension()

    def _init_vector_extension(self):
        """åˆå§‹åŒ–sqlite-vssæ‰©å±•ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        try:
            # å°è¯•åŠ è½½sqlite-vssæ‰©å±•
            # æ³¨æ„ï¼šè¿™éœ€è¦åœ¨SQLiteè¿æ¥ä¸Šæ‰§è¡Œï¼Œè€Œä¸æ˜¯åœ¨SQLAlchemyä¼šè¯ä¸Š
            # æˆ‘ä»¬å°†åœ¨å®é™…ä½¿ç”¨æ—¶å¤„ç†
            logger.info("âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨Pythonå‘é‡è®¡ç®—ï¼‰")
        except Exception as e:
            logger.warning(f"âš ï¸  sqlite-vssæ‰©å±•ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨Pythonå‘é‡è®¡ç®—: {e}")

    def _combine_article_text(self, article: Article) -> str:
        """
        ç»„åˆæ–‡ç« çš„æ‰€æœ‰å­—æ®µä¸ºç´¢å¼•æ–‡æœ¬

        Args:
            article: æ–‡ç« å¯¹è±¡

        Returns:
            ç»„åˆåçš„æ–‡æœ¬
        """
        parts = []
        
        # æ ‡é¢˜
        if article.title:
            parts.append(f"æ ‡é¢˜: {article.title}")
        
        # ä¸­æ–‡æ ‡é¢˜
        if article.title_zh:
            parts.append(f"ä¸­æ–‡æ ‡é¢˜: {article.title_zh}")
        
        # æ‘˜è¦
        if article.summary:
            parts.append(f"æ‘˜è¦: {article.summary}")
        
        # å†…å®¹ï¼ˆæˆªå–å‰5000å­—ç¬¦ä»¥é¿å…è¿‡é•¿ï¼‰
        if article.content:
            content_preview = article.content[:5000]
            parts.append(f"å†…å®¹: {content_preview}")
        
        # å…³é”®ç‚¹
        if article.key_points:
            if isinstance(article.key_points, list):
                key_points_str = "ã€".join(article.key_points)
                parts.append(f"å…³é”®ç‚¹: {key_points_str}")
        
        # ä¸»é¢˜
        if article.topics:
            if isinstance(article.topics, list):
                topics_str = "ã€".join(article.topics)
                parts.append(f"ä¸»é¢˜: {topics_str}")
        
        # æ ‡ç­¾
        if article.tags:
            if isinstance(article.tags, list):
                tags_str = "ã€".join(article.tags)
                parts.append(f"æ ‡ç­¾: {tags_str}")
        
        # æ¥æº
        if article.source:
            parts.append(f"æ¥æº: {article.source}")
        
        combined_text = "\n\n".join(parts)
        return combined_text if combined_text.strip() else ""

    def generate_embedding(self, text: str) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡

        Args:
            text: è¦ç”ŸæˆåµŒå…¥å‘é‡çš„æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not text or not text.strip():
            logger.warning("âš ï¸  ç”ŸæˆåµŒå…¥å‘é‡æ—¶æ–‡æœ¬ä¸ºç©º")
            return []
        
        return self.ai_analyzer.generate_embedding(text)

    def index_article(self, article: Article) -> bool:
        """
        ç´¢å¼•å•ç¯‡æ–‡ç« 

        Args:
            article: æ–‡ç« å¯¹è±¡

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•
            existing = self.db.query(ArticleEmbedding).filter(
                ArticleEmbedding.article_id == article.id
            ).first()
            
            if existing:
                logger.debug(f"æ–‡ç«  {article.id} å·²å­˜åœ¨ç´¢å¼•ï¼Œå°†æ›´æ–°")
            
            # ç”Ÿæˆç´¢å¼•æ–‡æœ¬
            text_content = self._combine_article_text(article)
            if not text_content.strip():
                logger.warning(f"âš ï¸  æ–‡ç«  {article.id} æ²¡æœ‰å¯ç´¢å¼•çš„å†…å®¹")
                return False
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"ğŸ“ æ­£åœ¨ä¸ºæ–‡ç«  {article.id} ç”ŸæˆåµŒå…¥å‘é‡...")
            embedding = self.generate_embedding(text_content)
            
            if not embedding:
                logger.error(f"âŒ æ–‡ç«  {article.id} åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
                return False
            
            # ä¿å­˜æˆ–æ›´æ–°
            if existing:
                existing.embedding = embedding
                existing.text_content = text_content
                existing.embedding_model = self.ai_analyzer.embedding_model
                existing.updated_at = datetime.now()
            else:
                embedding_obj = ArticleEmbedding(
                    article_id=article.id,
                    embedding=embedding,
                    text_content=text_content,
                    embedding_model=self.ai_analyzer.embedding_model
                )
                self.db.add(embedding_obj)
            
            self.db.commit()
            logger.info(f"âœ… æ–‡ç«  {article.id} ç´¢å¼•æˆåŠŸ")
            return True
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"âŒ æ–‡ç«  {article.id} ç´¢å¼•å¤±è´¥: {e}")
            return False

    def index_articles_batch(self, articles: List[Article], batch_size: int = 10) -> Dict[str, Any]:
        """
        æ‰¹é‡ç´¢å¼•æ–‡ç« 

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        total = len(articles)
        success_count = 0
        fail_count = 0
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ç´¢å¼• {total} ç¯‡æ–‡ç« ...")
        
        for i, article in enumerate(articles, 1):
            try:
                if self.index_article(article):
                    success_count += 1
                else:
                    fail_count += 1
                
                if i % batch_size == 0:
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total} (æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count})")
                    
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡ç´¢å¼•æ–‡ç«  {article.id} æ—¶å‡ºé”™: {e}")
                fail_count += 1
        
        logger.info(f"âœ… æ‰¹é‡ç´¢å¼•å®Œæˆ: æ€»è®¡ {total}, æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        
        return {
            "total": total,
            "success": success_count,
            "failed": fail_count
        }

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    def search_articles(
        self,
        query: str,
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢æ–‡ç« 

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶ï¼ˆsources, importance, time_rangeç­‰ï¼‰

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«æ–‡ç« ä¿¡æ¯å’Œç›¸ä¼¼åº¦åˆ†æ•°
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            logger.info(f"ğŸ” æ­£åœ¨æœç´¢: {query[:50]}...")
            query_embedding = self.generate_embedding(query)
            
            if not query_embedding:
                logger.error("âŒ æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥")
                return []
            
            # è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ç« åµŒå…¥
            query_obj = self.db.query(ArticleEmbedding, Article).join(
                Article, ArticleEmbedding.article_id == Article.id
            )
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if filters:
                if filters.get("sources"):
                    query_obj = query_obj.filter(Article.source.in_(filters["sources"]))
                
                if filters.get("importance"):
                    query_obj = query_obj.filter(Article.importance.in_(filters["importance"]))
                
                if filters.get("time_from"):
                    query_obj = query_obj.filter(Article.published_at >= filters["time_from"])
                
                if filters.get("time_to"):
                    query_obj = query_obj.filter(Article.published_at <= filters["time_to"])
            
            # è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡ç« åµŒå…¥
            embeddings = query_obj.all()
            
            if not embeddings:
                logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å·²ç´¢å¼•çš„æ–‡ç« ")
                return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            results = []
            for embedding_obj, article in embeddings:
                similarity = self._cosine_similarity(query_embedding, embedding_obj.embedding)
                results.append({
                    "article": article,
                    "similarity": similarity,
                    "embedding_id": embedding_obj.id
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # è¿”å›top_k
            top_results = results[:top_k]
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            search_results = []
            for result in top_results:
                article = result["article"]
                search_results.append({
                    "id": article.id,
                    "title": article.title,
                    "title_zh": article.title_zh,
                    "url": article.url,
                    "summary": article.summary,
                    "source": article.source,
                    "published_at": article.published_at.isoformat() if article.published_at else None,
                    "importance": article.importance,
                    "topics": article.topics,
                    "tags": article.tags,
                    "similarity": result["similarity"]
                })
            
            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def query_articles(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        RAGé—®ç­”ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡ç« å›ç­”é—®é¢˜

        Args:
            question: é—®é¢˜æ–‡æœ¬
            top_k: æ£€ç´¢çš„æ–‡ç« æ•°é‡

        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå¼•ç”¨æ–‡ç« çš„å­—å…¸
        """
        try:
            # æ£€ç´¢ç›¸å…³æ–‡ç« 
            relevant_articles = self.search_articles(question, top_k=top_k)
            
            if not relevant_articles:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡ç« æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "articles": []
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, article_info in enumerate(relevant_articles, 1):
                article_text = f"""
æ–‡ç«  {i}:
æ ‡é¢˜: {article_info['title']}
"""
                if article_info.get('title_zh'):
                    article_text += f"ä¸­æ–‡æ ‡é¢˜: {article_info['title_zh']}\n"
                if article_info.get('summary'):
                    article_text += f"æ‘˜è¦: {article_info['summary']}\n"
                if article_info.get('topics'):
                    article_text += f"ä¸»é¢˜: {', '.join(article_info['topics'])}\n"
                article_text += f"æ¥æº: {article_info['source']}\n"
                article_text += f"ç›¸ä¼¼åº¦: {article_info['similarity']:.3f}\n"
                
                context_parts.append(article_text)
            
            context = "\n---\n".join(context_parts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ–‡ç« ã€‚

ç›¸å…³æ–‡ç« ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³çš„æ–‡ç« ã€‚å¦‚æœæ–‡ç« ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜ã€‚"""

            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            logger.info(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
            response = self.ai_analyzer.client.chat.completions.create(
                model=self.ai_analyzer.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæä¾›çš„æ–‡ç« å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å‡†ç¡®å¼•ç”¨æ–‡ç« æ¥æºã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2000,
            )
            
            answer = response.choices[0].message.content.strip()
            
            return {
                "answer": answer,
                "sources": [article["source"] for article in relevant_articles],
                "articles": relevant_articles
            }
            
        except Exception as e:
            logger.error(f"âŒ é—®ç­”å¤±è´¥: {e}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
                "articles": []
            }

    def get_index_stats(self) -> Dict[str, Any]:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_articles = self.db.query(Article).count()
            indexed_articles = self.db.query(ArticleEmbedding).count()
            unindexed_articles = total_articles - indexed_articles
            
            # æŒ‰æ¥æºç»Ÿè®¡
            source_stats = {}
            embeddings = self.db.query(ArticleEmbedding, Article).join(
                Article, ArticleEmbedding.article_id == Article.id
            ).all()
            
            for embedding_obj, article in embeddings:
                source = article.source
                if source not in source_stats:
                    source_stats[source] = 0
                source_stats[source] += 1
            
            return {
                "total_articles": total_articles,
                "indexed_articles": indexed_articles,
                "unindexed_articles": unindexed_articles,
                "index_coverage": indexed_articles / total_articles if total_articles > 0 else 0.0,
                "source_stats": source_stats
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "total_articles": 0,
                "indexed_articles": 0,
                "unindexed_articles": 0,
                "index_coverage": 0.0,
                "source_stats": {}
            }

