"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - ä½¿ç”¨APScheduler BackgroundSchedulerï¼ˆé€‚é…FastAPIï¼‰
"""
import logging
import os
from datetime import datetime, timedelta

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from dotenv import load_dotenv

from backend.app.core.settings import settings
from backend.app.db import get_db
from backend.app.db.models import Article
from backend.app.services.collector import CollectionService
from backend.app.utils import create_ai_analyzer, setup_logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = setup_logger(__name__)


class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆä½¿ç”¨BackgroundSchedulerï¼Œé€‚é…FastAPIï¼‰"""

    def __init__(self):
        # ä½¿ç”¨ BackgroundScheduler è€Œä¸æ˜¯ BlockingScheduler
        # BackgroundScheduler åœ¨åå°çº¿ç¨‹è¿è¡Œï¼Œä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹
        self.scheduler = BackgroundScheduler()
        
        # åˆå§‹åŒ–æœåŠ¡
        self._init_services()

    def _init_services(self):
        """åˆå§‹åŒ–å„ä¸ªæœåŠ¡"""
        self.ai_analyzer = create_ai_analyzer()
        if self.ai_analyzer:
            logger.info("âœ… AIåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("âš ï¸  æœªé…ç½®OPENAI_API_KEYï¼ŒAIåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

        # é‡‡é›†æœåŠ¡
        self.collector = CollectionService(ai_analyzer=self.ai_analyzer)
        logger.info("âœ… é‡‡é›†æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        # é€šçŸ¥æœåŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            from backend.app.services.notification import NotificationService
            # ä»æ•°æ®åº“åŠ è½½é€šçŸ¥é…ç½®
            settings.load_settings_from_db()
            
            webhook_url = settings.NOTIFICATION_WEBHOOK_URL
            platform = settings.NOTIFICATION_PLATFORM
            secret = settings.NOTIFICATION_SECRET
            
            if webhook_url:
                self.notifier = NotificationService(
                    platform=platform,
                    webhook_url=webhook_url,
                    secret=secret
                )
                logger.info(f"âœ… é€šçŸ¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼ˆå¹³å°: {platform}ï¼‰")
            else:
                self.notifier = None
                logger.warning("âš ï¸  æœªé…ç½®é€šçŸ¥Webhook URLï¼Œæ¨é€åŠŸèƒ½å°†ä¸å¯ç”¨")
        except ImportError as e:
            self.notifier = None
            logger.warning(f"âš ï¸  é€šçŸ¥æœåŠ¡æ¨¡å—æœªæ‰¾åˆ°ï¼Œæ¨é€åŠŸèƒ½å°†ä¸å¯ç”¨: {e}")
        except Exception as e:
            self.notifier = None
            logger.warning(f"âš ï¸  é€šçŸ¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

        # æ•°æ®åº“
        self.db = get_db()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")

    def add_collection_job(self, interval_hours: int = None):
        """
        æ·»åŠ å®šæ—¶é‡‡é›†ä»»åŠ¡ï¼ˆä½¿ç”¨é—´éš”æ—¶é—´ï¼‰

        Args:
            interval_hours: é‡‡é›†é—´éš”ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        if interval_hours is None:
            interval_hours = settings.get_auto_collection_interval_hours()
            if interval_hours is None:
                # å¦‚æœè‡ªåŠ¨é‡‡é›†æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤çš„ COLLECTION_INTERVAL_HOURS
                interval_hours = settings.COLLECTION_INTERVAL_HOURS
        
        try:
            if interval_hours <= 0:
                raise ValueError(f"æ— æ•ˆçš„é‡‡é›†é—´éš”: {interval_hours} å°æ—¶")

            self.scheduler.add_job(
                func=self._run_collection,
                trigger=IntervalTrigger(hours=interval_hours),
                id="collection_job",
                name="å®šæ—¶æ•°æ®é‡‡é›†",
                replace_existing=True,
            )

            logger.info(f"âœ… å®šæ—¶é‡‡é›†ä»»åŠ¡å·²æ·»åŠ : æ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®šæ—¶é‡‡é›†ä»»åŠ¡å¤±è´¥: {e}")

    def add_daily_summary_job(self, cron_expression: str = None):
        """
        æ·»åŠ æ¯æ—¥æ‘˜è¦ä»»åŠ¡

        Args:
            cron_expression: cronè¡¨è¾¾å¼ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        if cron_expression is None:
            cron_expression = settings.get_daily_summary_cron()
            if not cron_expression:
                logger.warning("âš ï¸  æ¯æ—¥æ€»ç»“æœªå¯ç”¨æˆ–é…ç½®æ— æ•ˆ")
                return
        
        try:
            self.scheduler.add_job(
                func=self._run_daily_summary,
                trigger=CronTrigger.from_crontab(cron_expression),
                id="daily_summary_job",
                name="æ¯æ—¥æ‘˜è¦ç”Ÿæˆ",
                replace_existing=True,
            )

            logger.info(f"âœ… æ¯æ—¥æ‘˜è¦ä»»åŠ¡å·²æ·»åŠ : {cron_expression}")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ¯æ—¥æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")

    def add_weekly_summary_job(self, cron_expression: str = None):
        """
        æ·»åŠ æ¯å‘¨æ‘˜è¦ä»»åŠ¡

        Args:
            cron_expression: cronè¡¨è¾¾å¼ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        if cron_expression is None:
            cron_expression = settings.get_weekly_summary_cron()
            if not cron_expression:
                logger.warning("âš ï¸  æ¯å‘¨æ€»ç»“æœªå¯ç”¨æˆ–é…ç½®æ— æ•ˆ")
                return
        
        try:
            self.scheduler.add_job(
                func=self._run_weekly_summary,
                trigger=CronTrigger.from_crontab(cron_expression),
                id="weekly_summary_job",
                name="æ¯å‘¨æ‘˜è¦æ¨é€",
                replace_existing=True,
            )

            logger.info(f"âœ… æ¯å‘¨æ‘˜è¦ä»»åŠ¡å·²æ·»åŠ : {cron_expression}")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ¯å‘¨æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")

    def add_social_media_report_job(self, cron_expression: str = None):
        """
        æ·»åŠ ç¤¾äº¤å¹³å°AIå°æŠ¥å®šæ—¶ç”Ÿæˆä»»åŠ¡

        Args:
            cron_expression: cronè¡¨è¾¾å¼ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        if cron_expression is None:
            cron_expression = settings.get_social_media_auto_report_cron()
            if not cron_expression:
                logger.warning("âš ï¸  ç¤¾äº¤å¹³å°å®šæ—¶ç”ŸæˆAIå°æŠ¥æœªå¯ç”¨æˆ–é…ç½®æ— æ•ˆ")
                return
        
        try:
            self.scheduler.add_job(
                func=self._run_social_media_report,
                trigger=CronTrigger.from_crontab(cron_expression),
                id="social_media_report_job",
                name="ç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆ",
                replace_existing=True,
            )

            logger.info(f"âœ… ç¤¾äº¤å¹³å°AIå°æŠ¥å®šæ—¶ç”Ÿæˆä»»åŠ¡å·²æ·»åŠ : {cron_expression}")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç¤¾äº¤å¹³å°AIå°æŠ¥å®šæ—¶ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")

    def _run_collection(self):
        """æ‰§è¡Œé‡‡é›†ä»»åŠ¡ï¼ˆè‡ªåŠ¨å®šæ—¶é‡‡é›†ï¼‰"""
        task_id = None
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®šæ—¶é‡‡é›†ä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")
            logger.info(f"ğŸ“‹ ä»»åŠ¡ID: collection_job")
            logger.info(f"ğŸ”„ é‡‡é›†é—´éš”: æ¯ {settings.get_auto_collection_interval_hours() or settings.COLLECTION_INTERVAL_HOURS} å°æ—¶")

            # åˆ›å»ºé‡‡é›†ä»»åŠ¡è®°å½•ï¼ˆä¸æ‰‹åŠ¨é‡‡é›†ä¿æŒä¸€è‡´ï¼‰
            from backend.app.db.models import CollectionTask
            with self.db.get_session() as session:
                task = CollectionTask(
                    status="running",
                    ai_enabled=True,  # å®šæ—¶é‡‡é›†é»˜è®¤å¯ç”¨AIåˆ†æ
                    started_at=datetime.now(),
                )
                session.add(task)
                session.commit()
                session.refresh(task)
                task_id = task.id
                logger.info(f"ğŸ“ å·²åˆ›å»ºé‡‡é›†ä»»åŠ¡è®°å½• (ID: {task_id})")

            # æ‰§è¡Œé‡‡é›†ï¼ˆä¼ é€’ task_id ä»¥ä¾¿æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼‰
            stats = self.collector.collect_all(enable_ai_analysis=True, task_id=task_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            with self.db.get_session() as session:
                task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                if task:
                    task.status = "completed"
                    task.new_articles_count = stats.get('new_articles', 0)
                    task.total_sources = stats.get('sources_success', 0) + stats.get('sources_error', 0)
                    task.success_sources = stats.get('sources_success', 0)
                    task.failed_sources = stats.get('sources_error', 0)
                    task.duration = stats.get('duration', 0)
                    task.completed_at = datetime.now()
                    task.ai_analyzed_count = stats.get('ai_analyzed_count', 0)
                    session.commit()

            logger.info(f"âœ… é‡‡é›†å®Œæˆ:")
            logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
            logger.info(f"   æ–°å¢æ–‡ç« : {stats['new_articles']}")
            logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")
            
            # æ˜¾ç¤ºä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
            job = self.scheduler.get_job("collection_job")
            if job and job.next_run_time:
                logger.info(f"â° ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {job.next_run_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 60)

            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜é‡è¦æ€§æ–‡ç« éœ€è¦å³æ—¶æ¨é€
            if self.notifier:
                self._send_instant_alerts()

        except Exception as e:
            logger.error(f"âŒ é‡‡é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé”™è¯¯
            if task_id:
                try:
                    from backend.app.db.models import CollectionTask
                    with self.db.get_session() as session:
                        task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                        if task:
                            task.status = "error"
                            task.error_message = str(e)
                            task.completed_at = datetime.now()
                            session.commit()
                except Exception as update_error:
                    logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {update_error}", exc_info=True)

    def _run_daily_summary(self):
        """æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡ï¼ˆç”Ÿæˆæ€»ç»“å¹¶è‡ªåŠ¨æ¨é€ï¼‰"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")

            if not self.ai_analyzer:
                logger.warning("âš ï¸  AIåˆ†æå™¨æœªé…ç½®ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
                return

            # ä½¿ç”¨æ€»ç»“ç”Ÿæˆå™¨ç”Ÿæˆæ¯æ—¥æ€»ç»“
            # è‡ªåŠ¨æ‰§è¡Œæ—¶ç»Ÿè®¡æ˜¨å¤©çš„å†…å®¹
            from backend.app.services.collector.summary_generator import SummaryGenerator
            summary_generator = SummaryGenerator(self.ai_analyzer)
            yesterday = datetime.now() - timedelta(days=1)
            summary_obj = summary_generator.generate_daily_summary(self.db, yesterday)

            if not summary_obj:
                logger.warning("âš ï¸  æ˜¨æ—¥æš‚æ— ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè·³è¿‡æ¨é€")
                logger.info("=" * 60)
                return

            logger.info("ğŸ“ æ¯æ—¥æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            logger.info(f"   æ–‡ç« æ€»æ•°: {summary_obj.total_articles}")
            logger.info(f"   é«˜é‡è¦æ€§: {summary_obj.high_importance_count}")
            logger.info(f"   ä¸­é‡è¦æ€§: {summary_obj.medium_importance_count}")

            # æ€»ç»“ç”Ÿæˆå®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘æ¨é€
            if self.notifier and hasattr(self.notifier, 'send_daily_summary'):
                logger.info("ğŸ“¤ å¼€å§‹æ¨é€æ¯æ—¥æ‘˜è¦...")
                summary_content = summary_obj.summary_content
                success = self.notifier.send_daily_summary(
                    summary_content,
                    self.db,
                    articles_count=summary_obj.total_articles
                )
                if success:
                    logger.info("âœ… æ¯æ—¥æ‘˜è¦æ¨é€æˆåŠŸ")
                else:
                    logger.error("âŒ æ¯æ—¥æ‘˜è¦æ¨é€å¤±è´¥")
            else:
                if not self.notifier:
                    logger.warning("âš ï¸  é€šçŸ¥æœåŠ¡æœªé…ç½®ï¼Œè·³è¿‡æ¨é€")
                else:
                    logger.warning("âš ï¸  é€šçŸ¥æœåŠ¡ä¸æ”¯æŒæ¯æ—¥æ‘˜è¦æ¨é€")

            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥æ‘˜è¦ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

    def _run_weekly_summary(self):
        """æ‰§è¡Œæ¯å‘¨æ‘˜è¦ä»»åŠ¡ï¼ˆç”Ÿæˆæ€»ç»“å¹¶è‡ªåŠ¨æ¨é€ï¼‰"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“ å¼€å§‹æ‰§è¡Œæ¯å‘¨æ‘˜è¦ä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")

            if not self.ai_analyzer:
                logger.warning("âš ï¸  AIåˆ†æå™¨æœªé…ç½®ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
                return

            # ä½¿ç”¨æ€»ç»“ç”Ÿæˆå™¨ç”Ÿæˆæ¯å‘¨æ€»ç»“
            # è‡ªåŠ¨æ‰§è¡Œæ—¶ç»Ÿè®¡ä¸Šå‘¨çš„å†…å®¹ï¼ˆä¸Šå‘¨å…­åˆ°ä¸Šå‘¨äº”ï¼‰
            # ç”±äºæ¯å‘¨æ€»ç»“åœ¨å‘¨å…­æ‰§è¡Œï¼Œéœ€è¦ä¼ é€’ä¸Šå‘¨å…­çš„æ—¥æœŸ
            from backend.app.services.collector.summary_generator import SummaryGenerator
            summary_generator = SummaryGenerator(self.ai_analyzer)
            # è®¡ç®—ä¸Šå‘¨å…­çš„æ—¥æœŸ
            # å¦‚æœä»Šå¤©æ˜¯å‘¨å…­ï¼Œä¸Šå‘¨å…­æ˜¯7å¤©å‰ï¼›å¦‚æœä»Šå¤©æ˜¯å…¶ä»–æ—¥æœŸï¼Œè®¡ç®—è·ç¦»ä¸Šå‘¨å…­çš„å¤©æ•°
            now = datetime.now()
            weekday = now.weekday()  # Monday=0, Tuesday=1, ..., Sunday=6
            if weekday == 5:  # å‘¨å…­
                days_to_last_saturday = 7
            elif weekday == 6:  # å‘¨æ—¥
                days_to_last_saturday = 1
            else:  # å‘¨ä¸€åˆ°å‘¨äº”
                days_to_last_saturday = weekday + 2
            last_saturday = now - timedelta(days=days_to_last_saturday)
            summary_obj = summary_generator.generate_weekly_summary(self.db, last_saturday)

            if not summary_obj:
                logger.warning("âš ï¸  ä¸Šå‘¨æš‚æ— ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè·³è¿‡æ¨é€")
                logger.info("=" * 60)
                return

            logger.info("ğŸ“ æ¯å‘¨æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            logger.info(f"   æ–‡ç« æ€»æ•°: {summary_obj.total_articles}")
            logger.info(f"   é«˜é‡è¦æ€§: {summary_obj.high_importance_count}")
            logger.info(f"   ä¸­é‡è¦æ€§: {summary_obj.medium_importance_count}")

            # æ€»ç»“ç”Ÿæˆå®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘æ¨é€
            if self.notifier and hasattr(self.notifier, 'send_weekly_summary'):
                logger.info("ğŸ“¤ å¼€å§‹æ¨é€æ¯å‘¨æ‘˜è¦...")
                summary_content = summary_obj.summary_content
                success = self.notifier.send_weekly_summary(
                    summary_content,
                    self.db,
                    articles_count=summary_obj.total_articles
                )
                if success:
                    logger.info("âœ… æ¯å‘¨æ‘˜è¦æ¨é€æˆåŠŸ")
                else:
                    logger.error("âŒ æ¯å‘¨æ‘˜è¦æ¨é€å¤±è´¥")
            else:
                if not self.notifier:
                    logger.warning("âš ï¸  é€šçŸ¥æœåŠ¡æœªé…ç½®ï¼Œè·³è¿‡æ¨é€")
                else:
                    logger.warning("âš ï¸  é€šçŸ¥æœåŠ¡ä¸æ”¯æŒæ¯å‘¨æ‘˜è¦æ¨é€")

            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ æ¯å‘¨æ‘˜è¦ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

    def _run_social_media_report(self):
        """æ‰§è¡Œç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆä»»åŠ¡"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“° å¼€å§‹æ‰§è¡Œç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")

            # å¯¼å…¥ç¤¾äº¤å¹³å°é‡‡é›†å™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨
            from backend.app.services.social_media import SocialMediaCollector
            from backend.app.core.settings import settings
            
            # é‡æ–°åŠ è½½é…ç½®
            settings.load_social_media_settings()
            
            # åˆå§‹åŒ–é‡‡é›†å™¨
            collector = SocialMediaCollector()
            youtube_key = settings.YOUTUBE_API_KEY
            twitter_key = settings.TWITTER_API_KEY
            tiktok_key = settings.TIKTOK_API_KEY
            reddit_client_id = settings.REDDIT_CLIENT_ID
            reddit_client_secret = settings.REDDIT_CLIENT_SECRET
            reddit_user_agent = settings.REDDIT_USER_AGENT

            collector.initialize(
                youtube_api_key=youtube_key,
                twitter_api_key=twitter_key,
                tiktok_api_key=tiktok_key,
                reddit_client_id=reddit_client_id,
                reddit_client_secret=reddit_client_secret,
                reddit_user_agent=reddit_user_agent
            )

            if not collector.report_generator:
                logger.warning("âš ï¸  æŠ¥å‘Šç”Ÿæˆå™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç”Ÿæˆ")
                return

            # æ£€æŸ¥å“ªäº›å¹³å°å·²é…ç½®
            youtube_enabled = collector.youtube_collector is not None
            tiktok_enabled = collector.tiktok_collector is not None
            twitter_enabled = collector.twitter_collector is not None
            reddit_enabled = collector.reddit_collector is not None

            if not any([youtube_enabled, tiktok_enabled, twitter_enabled, reddit_enabled]):
                logger.warning("âš ï¸  æ²¡æœ‰å·²é…ç½®çš„ç¤¾äº¤å¹³å°ï¼Œè·³è¿‡ç”Ÿæˆ")
                return

            # é‡‡é›†æ•°æ®
            from datetime import timedelta
            published_after = datetime.now() - timedelta(days=1)
            
            results = {
                "youtube": [],
                "tiktok": [],
                "twitter": [],
                "reddit": []
            }

            # YouTubeé‡‡é›†
            if youtube_enabled:
                try:
                    youtube_videos = collector.youtube_collector.search_videos(
                        query="AI",
                        published_after=published_after,
                        max_results=50,
                    )
                    results["youtube"] = youtube_videos
                except Exception as e:
                    logger.error(f"YouTubeé‡‡é›†å¤±è´¥: {e}")

            # TikToké‡‡é›†
            if tiktok_enabled:
                try:
                    tiktok_videos = collector.tiktok_collector.search_videos(
                        keyword="AI",
                        min_viral_score=8.0,
                        max_days=1,
                        max_results=50,
                    )
                    results["tiktok"] = tiktok_videos
                except Exception as e:
                    logger.error(f"TikToké‡‡é›†å¤±è´¥: {e}")

            # Twitteré‡‡é›†
            if twitter_enabled:
                try:
                    twitter_tweets = collector.twitter_collector.search_tweets(
                        query="AI",
                        query_type="Top",
                        min_view_count=10000,
                        min_engagement_score=1000,
                        max_results=50,
                    )
                    results["twitter"] = twitter_tweets
                except Exception as e:
                    logger.error(f"Twitteré‡‡é›†å¤±è´¥: {e}")

            # Reddité‡‡é›†
            if reddit_enabled:
                try:
                    reddit_posts = collector.reddit_collector.search_posts(
                        subreddits=["ArtificialInteligence", "artificial"],
                        category="hot",
                        time_range="day",
                        min_upvotes=50,
                        max_results=50,
                    )
                    results["reddit"] = reddit_posts
                except Exception as e:
                    logger.error(f"Reddité‡‡é›†å¤±è´¥: {e}")

            # æ±‡æ€»é‡‡é›†æ•°æ®
            all_posts = []
            for platform, posts in results.items():
                all_posts.extend(posts)

            if not all_posts:
                logger.warning("âš ï¸  æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œè·³è¿‡ç”Ÿæˆ")
                return

            # å°†å­—å…¸è½¬æ¢ä¸ºSocialMediaPostå¯¹è±¡ï¼ˆä¸´æ—¶å¯¹è±¡ï¼‰
            temp_posts = []
            for post_data in all_posts:
                try:
                    from backend.app.db.models import SocialMediaPost
                    temp_post = SocialMediaPost(**post_data)
                    temp_posts.append(temp_post)
                except Exception as e:
                    logger.warning(f"è½¬æ¢å¸–å­æ•°æ®å¤±è´¥: {e}")
                    continue

            if not temp_posts:
                logger.warning("âš ï¸  è½¬æ¢å¸–å­æ•°æ®å¤±è´¥ï¼Œè·³è¿‡ç”Ÿæˆ")
                return

            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä½œä¸ºç¼“å­˜ï¼‰
            saved_post_ids = []
            with self.db.get_session() as session:
                saved_posts = collector.save_posts(session, all_posts)
                saved_post_ids = [post.id for post in saved_posts]

            # ä»æ•°æ®åº“åŠ è½½å·²æœ‰çš„ç¿»è¯‘å’Œä»·å€¼åˆ¤æ–­ç»“æœï¼Œå¡«å……åˆ°ä¸´æ—¶å¯¹è±¡ä¸­
            post_ids_by_platform = {}
            for temp_post in temp_posts:
                if temp_post.post_id:
                    platform = temp_post.platform
                    if platform not in post_ids_by_platform:
                        post_ids_by_platform[platform] = []
                    post_ids_by_platform[platform].append(temp_post.post_id)

            # æ‰¹é‡æŸ¥è¯¢å·²æœ‰çš„ç¿»è¯‘å’Œä»·å€¼åˆ¤æ–­ç»“æœ
            if post_ids_by_platform:
                with self.db.get_session() as session:
                    for platform, post_ids in post_ids_by_platform.items():
                        existing_posts = session.query(SocialMediaPost).filter(
                            SocialMediaPost.platform == platform,
                            SocialMediaPost.post_id.in_(post_ids)
                        ).all()
                        
                        existing_posts_map = {p.post_id: p for p in existing_posts}
                        
                        for temp_post in temp_posts:
                            if temp_post.platform == platform and temp_post.post_id in existing_posts_map:
                                existing_post = existing_posts_map[temp_post.post_id]
                                if existing_post.title_zh:
                                    temp_post.title_zh = existing_post.title_zh
                                if existing_post.has_value is not None:
                                    temp_post.has_value = existing_post.has_value

            # AIåˆ†æ(åå°æ‰§è¡Œ) - åªå¯¹æ–°ä¿å­˜çš„å¸–å­è¿›è¡Œåˆ†æ
            if saved_post_ids:
                import threading
                threading.Thread(
                    target=self._analyze_posts,
                    args=(collector, saved_post_ids),
                    name="social-media-post-analyzer",
                    daemon=True,
                ).start()

            # ç”ŸæˆæŠ¥å‘Š
            report_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            with self.db.get_session() as session:
                report = collector.report_generator.generate_daily_report(
                    db=session,
                    posts=temp_posts,
                    report_date=report_date,
                    youtube_enabled=youtube_enabled,
                    tiktok_enabled=tiktok_enabled,
                    twitter_enabled=twitter_enabled,
                    reddit_enabled=reddit_enabled,
                )

            if report:
                logger.info("âœ… ç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆå®Œæˆ")
                logger.info(f"   YouTube: {report.youtube_count}æ¡")
                logger.info(f"   TikTok: {report.tiktok_count}æ¡")
                logger.info(f"   Twitter: {report.twitter_count}æ¡")
                logger.info(f"   Reddit: {report.reddit_count}æ¡")
                logger.info(f"   æ€»è®¡: {report.total_count}æ¡")
            else:
                logger.warning("âš ï¸  ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼Œæ•°æ®å¯èƒ½ä¸ºç©º")

            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

    def _analyze_posts(self, collector, post_ids):
        """åå°åˆ†æå¸–å­"""
        try:
            from backend.app.db import get_db
            db = get_db()
            with db.get_session() as session:
                from backend.app.db.models import SocialMediaPost
                posts = session.query(SocialMediaPost).filter(SocialMediaPost.id.in_(post_ids)).all()
                collector.analyze_posts(session, posts)
        except Exception as e:
            logger.error(f"å¼‚æ­¥åˆ†æå¤±è´¥: {e}")

    def _send_instant_alerts(self):
        """å‘é€å³æ—¶æé†’ï¼ˆé«˜é‡è¦æ€§æ–‡ç« ï¼‰"""
        try:
            with self.db.get_session() as session:
                # è·å–æœ€è¿‘1å°æ—¶çš„é«˜é‡è¦æ€§æ–‡ç« ä¸”æœªæ¨é€çš„
                time_threshold = datetime.now() - timedelta(hours=1)

                articles = (
                    session.query(Article)
                    .filter(
                        Article.published_at >= time_threshold,
                        Article.importance == "high",
                        Article.is_sent == False
                    )
                    .all()
                )

                if not articles:
                    return

                logger.info(f"ğŸš¨ å‘ç° {len(articles)} ç¯‡é«˜é‡è¦æ€§æ–‡ç« ï¼Œå‡†å¤‡æ¨é€")

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†å³æ—¶é€šçŸ¥
                if not settings.INSTANT_NOTIFICATION_ENABLED:
                    logger.info("âš ï¸  å³æ—¶é€šçŸ¥æœªå¯ç”¨ï¼Œè·³è¿‡æ¨é€")
                    return

                for article in articles:
                    if self.notifier and hasattr(self.notifier, 'send_instant_alert'):
                        success = self.notifier.send_instant_alert(article, db=session)
                        if success:
                            article.is_sent = True
                            logger.info(f"âœ… å·²æ¨é€: {article.title[:50]}...")
                        else:
                            logger.error(f"âŒ æ¨é€å¤±è´¥: {article.title[:50]}...")

                session.commit()

        except Exception as e:
            logger.error(f"âŒ å‘é€å³æ—¶æé†’å¤±è´¥: {e}", exc_info=True)

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        try:
            logger.info("ğŸš€ ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨ä¸­...")
            logger.info(f"ğŸ“… å½“å‰æ—¶é—´: {datetime.now()}")
            logger.info(f"ğŸ“Š è‡ªåŠ¨é‡‡é›†çŠ¶æ€: {'å·²å¯ç”¨' if settings.AUTO_COLLECTION_ENABLED else 'æœªå¯ç”¨'}")

            # æ·»åŠ ä»»åŠ¡
            # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨é‡‡é›†ï¼Œä½¿ç”¨è‡ªåŠ¨é‡‡é›†é—´éš”ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤çš„COLLECTION_INTERVAL_HOURS
            if settings.AUTO_COLLECTION_ENABLED:
                interval_hours = settings.get_auto_collection_interval_hours()
                if interval_hours:
                    logger.info(f"â° ä½¿ç”¨è‡ªåŠ¨é‡‡é›†é—´éš”: æ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡")
                    self.add_collection_job(interval_hours)
                else:
                    logger.warning("âš ï¸  è‡ªåŠ¨é‡‡é›†é—´éš”é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                    self.add_collection_job()
            else:
                # å³ä½¿æœªå¯ç”¨è‡ªåŠ¨é‡‡é›†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤é—´éš”ï¼ˆå¦‚æœéœ€è¦ï¼‰
                logger.info(f"â° è‡ªåŠ¨é‡‡é›†æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é—´éš”: æ¯ {settings.COLLECTION_INTERVAL_HOURS} å°æ—¶")
                # æ³¨æ„ï¼šå¦‚æœä¸éœ€è¦ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
                # self.add_collection_job()
            
            # æ·»åŠ æ€»ç»“ä»»åŠ¡
            if settings.DAILY_SUMMARY_ENABLED:
                self.add_daily_summary_job()
            
            if settings.WEEKLY_SUMMARY_ENABLED:
                self.add_weekly_summary_job()
            
            # æ·»åŠ ç¤¾äº¤å¹³å°AIå°æŠ¥ç”Ÿæˆä»»åŠ¡
            # å¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®ï¼Œç¡®ä¿è·å–æœ€æ–°å€¼
            settings.load_social_media_settings()
            logger.info(f"ğŸ“Š ç¤¾äº¤å¹³å°AIå°æŠ¥å®šæ—¶ç”ŸæˆçŠ¶æ€: {'å·²å¯ç”¨' if settings.SOCIAL_MEDIA_AUTO_REPORT_ENABLED else 'æœªå¯ç”¨'}")
            if settings.SOCIAL_MEDIA_AUTO_REPORT_ENABLED:
                logger.info(f"â° å®šæ—¶ç”Ÿæˆæ—¶é—´: {settings.SOCIAL_MEDIA_AUTO_REPORT_TIME}")
                self.add_social_media_report_job()
            else:
                logger.info("â„¹ï¸  ç¤¾äº¤å¹³å°AIå°æŠ¥å®šæ—¶ç”Ÿæˆæœªå¯ç”¨ï¼Œè·³è¿‡æ·»åŠ ä»»åŠ¡")

            # å¯åŠ¨è°ƒåº¦å™¨ï¼ˆBackgroundScheduler åœ¨åå°è¿è¡Œï¼‰
            self.scheduler.start()

            # æ˜¾ç¤ºå³å°†æ‰§è¡Œçš„ä»»åŠ¡
            jobs = self.scheduler.get_jobs()
            if jobs:
                logger.info(f"ğŸ“‹ å·²æ³¨å†Œ {len(jobs)} ä¸ªå®šæ—¶ä»»åŠ¡:")
                for job in jobs:
                    next_run = job.next_run_time.strftime("%Y-%m-%d %H:%M:%S") if job.next_run_time else "æœªè®¡åˆ’"
                    logger.info(f"   - {job.name} (ID: {job.id})")
                    logger.info(f"     ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")
            else:
                logger.warning("âš ï¸  è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•å®šæ—¶ä»»åŠ¡")

            logger.info("âœ… ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ˆåå°è¿è¡Œï¼‰")

        except Exception as e:
            logger.error(f"âŒ è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)

    def shutdown(self):
        """å…³é—­è°ƒåº¦å™¨"""
        try:
            logger.info("â¹ï¸  æ­£åœ¨å…³é—­è°ƒåº¦å™¨...")
            self.scheduler.shutdown(wait=True)
            logger.info("âœ… è°ƒåº¦å™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­è°ƒåº¦å™¨å¤±è´¥: {e}", exc_info=True)


def create_scheduler() -> TaskScheduler:
    """åˆ›å»ºå¹¶é…ç½®è°ƒåº¦å™¨å®ä¾‹"""
    scheduler = TaskScheduler()
    scheduler.start()
    return scheduler



