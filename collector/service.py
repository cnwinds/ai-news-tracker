"""
ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡
"""
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path
import logging

from sqlalchemy import desc
from sqlalchemy.orm import Session

from collector.rss_collector import RSSCollector
from collector.api_collector import ArXivCollector, HuggingFaceCollector, PapersWithCodeCollector
from database import get_db
from database.models import Article, CollectionLog
from analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class CollectionService:
    """ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self, config_path: str = "config/sources.json", ai_analyzer: AIAnalyzer = None):
        self.ai_analyzer = ai_analyzer
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–å„ä¸ªé‡‡é›†å™¨
        self.rss_collector = RSSCollector()
        self.arxiv_collector = ArXivCollector()
        self.hf_collector = HuggingFaceCollector()
        self.pwc_collector = PapersWithCodeCollector()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {"rss_sources": [], "api_sources": [], "web_sources": [], "social_sources": []}

    def collect_all(self, enable_ai_analysis: bool = True) -> Dict[str, Any]:
        """
        é‡‡é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº

        Args:
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æž

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ðŸš€ å¼€å§‹é‡‡é›†æ‰€æœ‰æ•°æ®æº")
        stats = {
            "total_articles": 0,
            "new_articles": 0,
            "sources_success": 0,
            "sources_error": 0,
            "start_time": datetime.now(),
        }

        db = get_db()

        # 1. é‡‡é›†RSSæº
        logger.info("\nðŸ“¡ é‡‡é›†RSSæº")
        rss_stats = self._collect_rss_sources(db)
        stats.update(rss_stats)

        # 2. é‡‡é›†APIæºï¼ˆarXiv, Hugging Faceç­‰ï¼‰
        logger.info("\nðŸ“š é‡‡é›†è®ºæ–‡APIæº")
        api_stats = self._collect_api_sources(db)
        stats.update(api_stats)

        # 3. AIåˆ†æž
        if enable_ai_analysis and self.ai_analyzer:
            logger.info("\nðŸ¤– å¼€å§‹AIåˆ†æž")
            ai_stats = self._analyze_articles(db)
            stats.update(ai_stats)

        stats["end_time"] = datetime.now()
        stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

        logger.info(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        logger.info(f"   æ–°å¢žæ–‡ç« : {stats['new_articles']}")
        logger.info(f"   æˆåŠŸæºæ•°: {stats['sources_success']}")
        logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")

        return stats

    def _collect_rss_sources(self, db) -> Dict[str, Any]:
        """é‡‡é›†RSSæº"""
        stats = {"sources_success": 0, "sources_error": 0}

        rss_configs = self.config.get("rss_sources", [])
        results = self.rss_collector.fetch_multiple_feeds(rss_configs)

        for source_name, articles in results.items():
            try:
                new_count = 0
                for article in articles:
                    if self._save_article(db, article):
                        new_count += 1

                # è®°å½•æ—¥å¿—
                self._log_collection(db, source_name, "rss", "success", len(articles))
                stats["sources_success"] += 1

                logger.info(f"  âœ… {source_name}: {len(articles)} ç¯‡, æ–°å¢ž {new_count} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {source_name}: {e}")
                self._log_collection(db, source_name, "rss", "error", 0, str(e))
                stats["sources_error"] += 1

        return stats

    def _collect_api_sources(self, db) -> Dict[str, Any]:
        """é‡‡é›†APIæº"""
        stats = {"sources_success": 0, "sources_error": 0}

        api_configs = self.config.get("api_sources", [])

        for config in api_configs:
            if not config.get("enabled", True):
                continue

            name = config.get("name")
            source_type = config.get("category")

            try:
                articles = []
                if "arxiv" in name.lower():
                    query = config.get("query")
                    max_results = config.get("max_results", 20)
                    articles = self.arxiv_collector.fetch_papers(query, max_results)

                elif "hugging Face" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.hf_collector.fetch_trending_papers(limit)

                elif "papers with code" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.pwc_collector.fetch_trending_papers(limit)

                # ä¿å­˜æ–‡ç« 
                new_count = 0
                for article in articles:
                    if self._save_article(db, article):
                        new_count += 1

                # è®°å½•æ—¥å¿—
                self._log_collection(db, name, "api", "success", len(articles))
                stats["sources_success"] += 1

                logger.info(f"  âœ… {name}: {len(articles)} ç¯‡, æ–°å¢ž {new_count} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {name}: {e}")
                self._log_collection(db, name, "api", "error", 0, str(e))
                stats["sources_error"] += 1

        return stats

    def _save_article(self, db, article: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“

        Returns:
            True if new article, False if already exists
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return False

                # åˆ›å»ºæ–°æ–‡ç« 
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=article.get("content", "")[:10000],  # é™åˆ¶é•¿åº¦
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    metadata=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    def _analyze_articles(self, db, batch_size: int = 50) -> Dict[str, Any]:
        """AIåˆ†æžæœªåˆ†æžçš„æ–‡ç« """
        stats = {"analyzed_count": 0, "analysis_error": 0}

        with db.get_session() as session:
            # èŽ·å–æœªåˆ†æžçš„æ–‡ç« 
            unanalyzed = (
                session.query(Article).filter(Article.is_processed == False).order_by(Article.published_at.desc()).limit(batch_size).all()
            )

            if not unanalyzed:
                logger.info("  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æžçš„æ–‡ç« ")
                return stats

            logger.info(f"  ðŸ¤– å¼€å§‹åˆ†æž {len(unanalyzed)} ç¯‡æ–‡ç« ")

            for article in unanalyzed:
                try:
                    # å‡†å¤‡æ–‡ç« æ•°æ®
                    article_dict = {
                        "title": article.title,
                        "content": article.content,
                        "source": article.source,
                        "published_at": article.published_at,
                    }

                    # AIåˆ†æž
                    result = self.ai_analyzer.analyze_article(article_dict)

                    # æ›´æ–°æ–‡ç« 
                    article.summary = result.get("summary")
                    article.topics = result.get("topics")
                    article.tags = result.get("tags")
                    article.importance = result.get("importance")
                    article.target_audience = result.get("target_audience")
                    article.key_points = result.get("key_points")
                    article.is_processed = True

                    stats["analyzed_count"] += 1

                except Exception as e:
                    logger.error(f"  âŒ åˆ†æžæ–‡ç« å¤±è´¥ (ID={article.id}): {e}")
                    stats["analysis_error"] += 1

            session.commit()

        logger.info(f"  âœ… AIåˆ†æžå®Œæˆ: {stats['analyzed_count']} ç¯‡")
        return stats

    def _log_collection(self, db, source_name: str, source_type: str, status: str, count: int, error: str = None):
        """è®°å½•é‡‡é›†æ—¥å¿—"""
        try:
            with db.get_session() as session:
                log = CollectionLog(
                    source_name=source_name,
                    source_type=source_type,
                    status=status,
                    articles_count=count,
                    error_message=error,
                )
                session.add(log)
                session.commit()
        except Exception as e:
            logger.error(f"âŒ è®°å½•æ—¥å¿—å¤±è´¥: {e}")

    def get_recent_articles(self, db, limit: int = 100, hours: int = 24) -> List[Article]:
        """èŽ·å–æœ€è¿‘çš„æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=hours)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold)
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles

    def get_daily_summary(self, db, limit: int = 10) -> List[Article]:
        """èŽ·å–æ¯æ—¥é‡è¦æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=24)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold, Article.importance.in_(["high", "medium"]))
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles
