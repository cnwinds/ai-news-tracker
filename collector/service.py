"""
ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡
"""
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path
import logging
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed

from sqlalchemy import desc
from sqlalchemy.orm import Session

from collector.rss_collector import RSSCollector
from collector.api_collector import ArXivCollector, HuggingFaceCollector, PapersWithCodeCollector
from database import get_db
from database.models import Article, CollectionLog, RSSSource
from analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class CollectionService:
    """ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self, config_path: str = "config/sources.json", ai_analyzer: AIAnalyzer = None):
        self.ai_analyzer = ai_analyzer
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–å„ä¸ªé‡‡é›†å™¨
        self.rss_collector = RSSCollector()
        self.arxiv_collector = ArXivCollector()
        self.hf_collector = HuggingFaceCollector()
        self.pwc_collector = PapersWithCodeCollector()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {"rss_sources": [], "api_sources": [], "web_sources": [], "social_sources": []}

    def collect_all(self, enable_ai_analysis: bool = True, task_id: int = None) -> Dict[str, Any]:
        """
        é‡‡é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº

        Args:
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            task_id: ä»»åŠ¡IDï¼Œç”¨äºå®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ğŸš€ å¼€å§‹é‡‡é›†æ‰€æœ‰æ•°æ®æº")
        stats = {
            "total_articles": 0,
            "new_articles": 0,
            "sources_success": 0,
            "sources_error": 0,
            "start_time": datetime.now(),
        }

        db = get_db()

        # 1. é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæº + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘è·å–å†…å®¹+AIåˆ†æï¼‰
        logger.info("\nğŸ“¡ é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘æ¨¡å¼ï¼‰")
        rss_stats = self._collect_rss_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats.update(rss_stats)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        # 2. é‡‡é›†APIæºï¼ˆarXiv, Hugging Faceç­‰ï¼‰
        logger.info("\nğŸ“š é‡‡é›†è®ºæ–‡APIæº")
        api_stats = self._collect_api_sources(db, task_id=task_id)
        stats.update(api_stats)

        # å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        stats["end_time"] = datetime.now()
        stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

        logger.info(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        logger.info(f"   æ–°å¢æ–‡ç« : {stats['new_articles']}")
        logger.info(f"   æˆåŠŸæºæ•°: {stats['sources_success']}")
        logger.info(f"   AIåˆ†ææ•°: {stats.get('ai_analyzed_count', 0)}")
        logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")

        return stats

    def _fetch_articles_full_content(self, articles: List[Dict[str, Any]], source_name: str, max_workers: int = 3) -> List[Dict[str, Any]]:
        """
        å¹¶å‘è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            source_name: æºåç§°
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3ï¼ˆé¿å…å¯¹å•ä¸ªç½‘ç«™å‹åŠ›è¿‡å¤§ï¼‰
        
        Returns:
            æ›´æ–°åçš„æ–‡ç« åˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦è·å–å®Œæ•´å†…å®¹çš„æ–‡ç« ï¼ˆblogæ–‡ç« ï¼‰
        articles_to_fetch = [
            article for article in articles 
            if article.get("category") == "rss" and article.get("url")
        ]
        
        if not articles_to_fetch:
            return articles
        
        logger.info(f"  ğŸ“„ å¼€å§‹å¹¶å‘è·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # å¹¶å‘è·å–å®Œæ•´å†…å®¹
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_article = {
                executor.submit(self.rss_collector.fetch_full_content, article["url"]): article
                for article in articles_to_fetch
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                completed += 1

                try:
                    full_content, published_at = future.result()
                    if full_content:
                        article["content"] = full_content
                        # å¦‚æœä»é¡µé¢æå–åˆ°äº†æ—¥æœŸï¼Œæ›´æ–°æ–‡ç« çš„published_atå­—æ®µ
                        if published_at:
                            article["published_at"] = published_at
                            logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²è·å–å®Œæ•´å†…å®¹å’Œæ—¥æœŸ: {article['title'][:50]}...")
                        else:
                            logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²è·å–å®Œæ•´å†…å®¹: {article['title'][:50]}...")
                    else:
                        logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] æ— æ³•è·å–å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨RSSæ‘˜è¦: {article['title'][:50]}...")
                except Exception as e:
                    logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] è·å–å®Œæ•´å†…å®¹å¤±è´¥: {article['title'][:50]}... - {e}")
        
        logger.info(f"  âœ… å®Œæ•´å†…å®¹è·å–å®Œæˆ: {len(articles_to_fetch)} ç¯‡æ–‡ç« ")
        return articles

    def _update_task_progress(self, db, task_id: int, stats: Dict[str, Any]):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            from database.models import CollectionTask
            with db.get_session() as session:
                task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                if task:
                    task.new_articles_count = stats.get('new_articles', 0)
                    task.total_sources = stats.get('sources_success', 0) + stats.get('sources_error', 0)
                    task.success_sources = stats.get('sources_success', 0)
                    task.failed_sources = stats.get('sources_error', 0)
                    task.ai_analyzed_count = stats.get('analyzed_count', 0)
                    session.commit()
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

    def _process_single_rss_source(self, db, source_name: str, feed_result: Dict[str, Any], enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªRSSæºï¼šè·å–å®Œæ•´å†…å®¹ -> ä¿å­˜æ–‡ç«  -> AIåˆ†æï¼ˆå…¨æµç¨‹å¹¶å‘ï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            source_name: è®¢é˜…æºåç§°
            feed_result: RSSé‡‡é›†ç»“æœ
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        result_stats = {
            "source_name": source_name,
            "total_articles": 0,
            "new_articles": 0,
            "skipped_articles": 0,  # å·²å­˜åœ¨çš„æ–‡ç« 
            "ai_analyzed": 0,
            "ai_skipped": 0,  # å·²åˆ†æçš„æ–‡ç« 
            "success": False,
            "error": None
        }

        try:
            articles = feed_result.get("articles", [])
            feed_title = feed_result.get("feed_title")

            if not articles:
                result_stats["success"] = True
                return result_stats

            # ç¡®ä¿æ‰€æœ‰æ–‡ç« çš„sourceå­—æ®µéƒ½æ˜¯è®¢é˜…æºåç§°ï¼Œå¹¶è®¾ç½®æ­£ç¡®çš„author
            # è¿™æ˜¯å…³é”®çš„é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¼ºåˆ¶è¦†ç›–æ‰€æœ‰æ–‡ç« çš„sourceå­—æ®µï¼Œé˜²æ­¢å¹¶å‘å†²çª
            from collector.rss_collector import _get_author_from_source
            from config.settings import settings
            
            # åº”ç”¨æ–‡ç« å¹´é¾„è¿‡æ»¤ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            filtered_articles = []
            skipped_old_count = 0
            max_article_age_days = settings.MAX_ARTICLE_AGE_DAYS
            
            if max_article_age_days > 0:
                age_threshold = datetime.now() - timedelta(days=max_article_age_days)
                for article in articles:
                    published_at = article.get("published_at")
                    if published_at and published_at < age_threshold:
                        skipped_old_count += 1
                        continue
                    filtered_articles.append(article)
                articles = filtered_articles
                if skipped_old_count > 0:
                    logger.info(f"  â­ï¸  {source_name}: è·³è¿‡äº† {skipped_old_count} ç¯‡è¶…è¿‡ {max_article_age_days} å¤©çš„æ—§æ–‡ç« ")
            
            for article in articles:
                # å¼ºåˆ¶è®¾ç½®sourceå­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¢é˜…æºåç§°
                # è¿™å¯ä»¥é˜²æ­¢å¹¶å‘æ—¶feed titleè¢«é”™è¯¯ä½¿ç”¨
                article["source"] = source_name
                
                # æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…ï¼ˆè¦†ç›–RSS feedä¸­å¯èƒ½ä¸å‡†ç¡®çš„authorï¼‰
                correct_author = _get_author_from_source(source_name, article.get("url", ""))
                if correct_author:
                    article["author"] = correct_author
                
                # é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æœæ–‡ç« çš„sourceä¸ä¼ å…¥çš„source_nameä¸ä¸€è‡´ï¼Œè®°å½•è­¦å‘Š
                if article.get("source") != source_name:
                    logger.warning(f"  âš ï¸  æ–‡ç« sourceä¸åŒ¹é…: æœŸæœ›={source_name}, å®é™…={article.get('source')}, URL={article.get('url', '')[:50]}")
                    article["source"] = source_name  # å¼ºåˆ¶ä¿®æ­£

            logger.info(f"  ğŸ“¥ {source_name}: å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")

            # æ³¨æ„ï¼šä¸å†éœ€è¦ä¿®æ­£sourceå­—æ®µï¼Œå› ä¸ºæ·»åŠ äº†source_idå¤–é”®å…³è”
            # ä¿å­˜æ–‡ç« æ—¶ä¼šè‡ªåŠ¨æ ¹æ®source_nameæŸ¥è¯¢RSSSourceè·å–source_id
            # å¦‚æœRSSSource.nameè¢«ä¿®æ”¹ï¼Œå¯ä»¥é€šè¿‡article.rss_source.nameè·å–æœ€æ–°åç§°

            # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æ£€æŸ¥å“ªäº›æ–‡ç« å·²å­˜åœ¨ä¸”æœ‰å†…å®¹ã€å·²åˆ†æ
            existing_articles_data = {}
            with db.get_session() as session:
                # æŸ¥è¯¢å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆåŒ…æ‹¬å†…å®¹å’Œåˆ†æçŠ¶æ€ï¼‰
                url_list = [article.get("url") for article in articles if article.get("url")]
                if url_list:
                    existing = session.query(
                        Article.url,
                        Article.content,
                        Article.is_processed
                    ).filter(Article.url.in_(url_list)).all()

                    # å­˜å‚¨æ¯ä¸ªURLçš„çŠ¶æ€ï¼š{"url": {"has_content": bool, "is_processed": bool}}
                    for row in existing:
                        existing_articles_data[row[0]] = {
                            "has_content": bool(row[1] and row[1].strip()),  # æ£€æŸ¥å†…å®¹æ˜¯å¦éç©º
                            "is_processed": row[2]
                        }

            # ç¬¬äºŒæ­¥ï¼šåˆ†ç±»æ–‡ç« 
            articles_to_fetch = []  # éœ€è¦è·å–å†…å®¹çš„æ–‡ç« 
            articles_to_analyze = []  # éœ€è¦AIåˆ†æçš„æ–‡ç« 
            skipped_count = 0  # å®Œå…¨è·³è¿‡çš„æ–‡ç« 

            for article in articles:
                url = article.get("url")
                if not url:
                    continue

                if url not in existing_articles_data:
                    # æ–°æ–‡ç« ï¼Œéœ€è¦è·å–å†…å®¹å’ŒAIåˆ†æ
                    articles_to_fetch.append(article)
                else:
                    # æ–‡ç« å·²å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹å’Œåˆ†æçŠ¶æ€
                    status = existing_articles_data[url]

                    if not status["has_content"]:
                        # å†…å®¹ä¸ºç©ºï¼Œéœ€è¦é‡æ–°è·å–
                        articles_to_fetch.append(article)

                    if not status["is_processed"]:
                        # æœªåˆ†æï¼Œéœ€è¦é‡æ–°åˆ†æï¼ˆè®°å½•URLä»¥ä¾¿åç»­æŸ¥æ‰¾IDï¼‰
                        articles_to_analyze.append(article)

                    if status["has_content"] and status["is_processed"]:
                        # å†…å®¹å®Œæ•´ä¸”å·²åˆ†æï¼Œå®Œå…¨è·³è¿‡
                        skipped_count += 1

            result_stats["total_articles"] = len(articles)
            result_stats["skipped_articles"] = skipped_count

            fetch_count = len(articles_to_fetch)
            analyze_count = len(articles_to_analyze)

            if skipped_count > 0 or fetch_count > 0 or analyze_count > 0:
                logger.info(f"  ğŸ“Š {source_name}: è·³è¿‡ {skipped_count} ç¯‡å®Œæ•´æ–‡ç« , éœ€è·å–å†…å®¹ {fetch_count} ç¯‡, éœ€AIåˆ†æ {analyze_count} ç¯‡")

            if not articles_to_fetch and not articles_to_analyze:
                logger.info(f"  âœ… {source_name}: æ‰€æœ‰æ–‡ç« éƒ½å·²å®Œæ•´é‡‡é›†å’Œåˆ†æ")
                result_stats["success"] = True
                return result_stats

            logger.info(f"  ğŸ“¥ {source_name}: è·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹...")

            # ç¬¬ä¸‰æ­¥ï¼šå¹¶å‘è·å–å®Œæ•´å†…å®¹ï¼ˆ3ä¸ªå¹¶å‘ï¼‰
            articles_with_full_content = self._fetch_articles_full_content(
                articles_to_fetch, source_name, max_workers=3
            )

            # ç¬¬å››æ­¥ï¼šä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“
            logger.info(f"  ğŸ’¾ {source_name}: å¼€å§‹ä¿å­˜æ–‡ç« ...")
            saved_article_ids = []
            updated_count = 0  # æ›´æ–°çš„æ–‡ç« æ•°ï¼ˆå·²æœ‰URLä½†è¡¥å……äº†å†…å®¹ï¼‰
            new_count = 0  # æ–°å¢æ–‡ç« æ•°

            for article in articles_with_full_content:
                result = self._save_or_update_article_and_get_id(db, article)
                if result:
                    # åªä¿å­˜æ–‡ç« IDï¼ˆæ•´æ•°ï¼‰ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå­—å…¸
                    saved_article_ids.append(result["id"])
                    if result["is_new"]:
                        new_count += 1
                    else:
                        updated_count += 1

            result_stats["new_articles"] = new_count

            # æ›´æ–°RSSæºçš„ç»Ÿè®¡ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_collected_at = datetime.now()
                    source_obj.articles_count += len(articles)
                    source_obj.last_error = None

                    # ä»æ•°æ®åº“ä¸­æŸ¥è¯¢è¯¥æºæœ€æ–°çš„çœŸå®published_atï¼ˆè€Œä¸æ˜¯RSS feedçš„æ›´æ–°æ—¶é—´ï¼‰
                    latest_article = session.query(Article).filter(
                        Article.source == source_name,
                        Article.published_at.isnot(None)
                    ).order_by(Article.published_at.desc()).first()

                    if latest_article:
                        source_obj.latest_article_published_at = latest_article.published_at

                    session.commit()

            # è®°å½•é‡‡é›†æ—¥å¿—
            self._log_collection(db, source_name, "rss", "success", len(articles))

            # ç¬¬äº”æ­¥ï¼šå¦‚æœå¯ç”¨AIåˆ†æï¼Œå¤„ç†éœ€è¦åˆ†æçš„æ–‡ç« ï¼ˆåŒ…æ‹¬æ–°æ–‡ç« å’Œæ—§æ–‡ç« ï¼‰
            if enable_ai_analysis and self.ai_analyzer and (saved_article_ids or articles_to_analyze):
                # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ†æçš„æ–‡ç« IDï¼ˆå·²ç»æ˜¯æ•´æ•°åˆ—è¡¨ï¼‰
                all_article_ids = saved_article_ids.copy()

                # å¯¹äºå·²æœ‰URLä½†æœªåˆ†æçš„æ–‡ç« ï¼ŒæŸ¥è¯¢å®ƒä»¬çš„ID
                if articles_to_analyze:
                    with db.get_session() as session:
                        for article in articles_to_analyze:
                            existing = session.query(Article.id).filter(Article.url == article.get("url")).first()
                            if existing:
                                all_article_ids.append(existing.id)

                # æ£€æŸ¥å“ªäº›æ–‡ç« å·²ç»åˆ†æè¿‡äº†
                unanalyzed_ids = self._filter_unanalyzed_articles(db, all_article_ids)
                ai_skipped = len(all_article_ids) - len(unanalyzed_ids)

                if ai_skipped > 0:
                    logger.info(f"  â­ï¸  {source_name}: è·³è¿‡ {ai_skipped} ç¯‡å·²åˆ†æçš„æ–‡ç« ")

                if unanalyzed_ids:
                    logger.info(f"  ğŸ¤– {source_name}: å¼€å§‹AIåˆ†æ {len(unanalyzed_ids)} ç¯‡æ–‡ç« ...")
                    analyzed_count = self._analyze_articles_by_ids(db, unanalyzed_ids, max_workers=3)
                    result_stats["ai_analyzed"] = analyzed_count

                result_stats["ai_skipped"] = ai_skipped

            result_stats["success"] = True
            logger.info(f"  âœ… {source_name}: æ€»å…± {len(articles)} ç¯‡, è·³è¿‡ {skipped_count} ç¯‡, æ–°å¢ {new_count} ç¯‡, æ›´æ–° {updated_count} ç¯‡, AIåˆ†æ {result_stats['ai_analyzed']} ç¯‡")

        except Exception as e:
            logger.error(f"  âŒ {source_name}: {e}")
            result_stats["error"] = str(e)

            # æ›´æ–°é”™è¯¯ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_error = str(e)
                    session.commit()

            self._log_collection(db, source_name, "rss", "error", 0, str(e))

        return result_stats

    def _collect_rss_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæºåŒæ—¶é‡‡é›† + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘è·å–å†…å®¹+AIåˆ†æï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦åœ¨é‡‡é›†æ¯ä¸ªæºåç«‹å³è¿›è¡ŒAIåˆ†æ

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        # ä¼˜å…ˆä»æ•°æ®åº“è¯»å–RSSæº
        rss_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(RSSSource.enabled == True).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                rss_configs.append({
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                    "max_articles": 20,  # é»˜è®¤å€¼
                    "category": source.category,
                    "tier": source.tier,
                })
                # é¢„å…ˆåŠ è½½å±æ€§
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
                _ = source.last_collected_at
                _ = source.articles_count
            session.expunge_all()

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æºï¼Œåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–ï¼ˆå‘åå…¼å®¹ï¼‰
        if not rss_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰RSSæºï¼Œä»é…ç½®æ–‡ä»¶è¯»å–")
            rss_configs = self.config.get("rss_sources", [])

        if not rss_configs:
            logger.warning("  âš ï¸  æ²¡æœ‰é…ç½®RSSæº")
            return stats

        logger.info(f"  ğŸš€ å¼€å§‹é‡‡é›† {len(rss_configs)} ä¸ªRSSæºï¼ˆç¬¬ä¸€å±‚å¹¶å‘ï¼‰")

        # ç¬¬ä¸€å±‚å¹¶å‘ï¼šåŒæ—¶é‡‡é›†å¤šä¸ªRSSæº
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰RSSé‡‡é›†ä»»åŠ¡
            future_to_source = {}

            for rss_config in rss_configs:
                source_name = rss_config["name"]

                # æ·±æ‹·è´é…ç½®å¯¹è±¡ï¼Œé¿å…å¤šçº¿ç¨‹å…±äº«å¼•ç”¨å¯¼è‡´çš„å¹¶å‘é—®é¢˜
                # è™½ç„¶é»˜è®¤å‚æ•°æ•è·äº†å¼•ç”¨ï¼Œä½†å¦‚æœåœ¨è°ƒç”¨è¿‡ç¨‹ä¸­ä¿®æ”¹äº†å­—å…¸ï¼Œä»æœ‰é£é™©
                import copy
                config_copy = copy.deepcopy(rss_config)

                # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è·å˜é‡çš„å€¼ï¼Œé¿å…é—­åŒ…é™·é˜±
                # è¿™æ˜¯å…³é”®çš„ä¿®å¤ï¼šé€šè¿‡é»˜è®¤å‚æ•°åœ¨å®šä¹‰æ—¶æ•è·å€¼ï¼Œè€Œä¸æ˜¯åœ¨è¿è¡Œæ—¶å¼•ç”¨å˜é‡
                def collect_single_source(config=config_copy, name=source_name):
                    try:
                        # è·å–RSS feedï¼ˆä½¿ç”¨ä¼ å…¥çš„configï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨æ­£ç¡®çš„é…ç½®ï¼‰
                        feed_data = self.rss_collector.fetch_single_feed(config)

                        # å¤„ç†è¿™ä¸ªæºï¼ˆåŒ…å«è·å–å®Œæ•´å†…å®¹ã€ä¿å­˜ã€AIåˆ†æï¼‰
                        # ä½¿ç”¨ä¼ å…¥çš„nameï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨æ­£ç¡®çš„æºåç§°
                        result = self._process_single_rss_source(
                            db, name, feed_data, enable_ai_analysis
                        )
                        return result
                    except Exception as e:
                        logger.error(f"  âŒ {name} é‡‡é›†å¤±è´¥: {e}")
                        return {
                            "source_name": name,
                            "success": False,
                            "error": str(e),
                            "total_articles": 0,
                            "new_articles": 0,
                            "ai_analyzed": 0
                        }

                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(collect_single_source)
                future_to_source[future] = source_name

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                completed += 1

                try:
                    result = future.result()

                    if result["success"]:
                        stats["sources_success"] += 1
                        stats["new_articles"] += result["new_articles"]
                        stats["total_articles"] += result["total_articles"]
                        stats["ai_analyzed_count"] += result.get("ai_analyzed", 0)
                    else:
                        stats["sources_error"] += 1
                        logger.error(f"  âŒ {source_name}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                except Exception as e:
                    logger.error(f"  âŒ {source_name} å¤„ç†å¼‚å¸¸: {e}")
                    stats["sources_error"] += 1

        logger.info(f"  âœ… RSSé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢: {stats['new_articles']} ç¯‡, AIåˆ†æ: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _collect_api_sources(self, db, task_id: int = None) -> Dict[str, Any]:
        """é‡‡é›†APIæº"""
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0}

        api_configs = self.config.get("api_sources", [])

        for config in api_configs:
            if not config.get("enabled", True):
                continue

            name = config.get("name")
            source_type = config.get("category")

            try:
                articles = []
                if "arxiv" in name.lower():
                    query = config.get("query")
                    max_results = config.get("max_results", 20)
                    articles = self.arxiv_collector.fetch_papers(query, max_results)

                elif "hugging Face" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.hf_collector.fetch_trending_papers(limit)

                elif "papers with code" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.pwc_collector.fetch_trending_papers(limit)

                # ä¿å­˜æ–‡ç« 
                new_count = 0
                for article in articles:
                    if self._save_article(db, article):
                        new_count += 1

                # è®°å½•æ—¥å¿—
                self._log_collection(db, name, "api", "success", len(articles))
                stats["sources_success"] += 1
                stats["new_articles"] += new_count
                stats["total_articles"] += len(articles)

                logger.info(f"  âœ… {name}: {len(articles)} ç¯‡, æ–°å¢ {new_count} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {name}: {e}")
                self._log_collection(db, name, "api", "error", 0, str(e))
                stats["sources_error"] += 1

        return stats

    def _save_article(self, db, article: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“

        Returns:
            True if new article, False if already exists
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return False

                # åˆ›å»ºæ–°æ–‡ç« 
                # å¯¹äºå®Œæ•´å†…å®¹ï¼Œä¸é™åˆ¶é•¿åº¦ï¼ˆä½¿ç”¨Textç±»å‹å¯ä»¥å­˜å‚¨å¤§é‡æ–‡æœ¬ï¼‰
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,  # ä¸é™åˆ¶é•¿åº¦ï¼Œä½¿ç”¨Textç±»å‹
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    def _save_article_and_get_id(self, db, article: Dict[str, Any]) -> int or None:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“å¹¶è¿”å›æ–‡ç« ID

        Returns:
            æ–‡ç« IDï¼ˆå¦‚æœæ–‡ç« å·²å­˜åœ¨è¿”å›Noneï¼‰
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return None

                # åˆ›å»ºæ–°æ–‡ç« 
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                # è¿”å›æ–°æ’å…¥çš„æ–‡ç« ID
                return new_article.id

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return None

    def _save_or_update_article_and_get_id(self, db, article: Dict[str, Any]) -> Dict[str, Any] or None:
        """
        ä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“å¹¶è¿”å›æ–‡ç« IDå’Œä¿¡æ¯

        Returns:
            {"id": int, "is_new": bool} - æ–‡ç« IDå’Œæ˜¯å¦ä¸ºæ–°æ–‡ç« 
            å¦‚æœä¿å­˜å¤±è´¥è¿”å›None
        """
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with db.get_session() as session:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = session.query(Article).filter(Article.url == article["url"]).first()

                    if existing:
                        # æ–‡ç« å·²å­˜åœ¨ï¼Œæ›´æ–°å†…å®¹ï¼ˆå¦‚æœæ–°å†…å®¹æ›´å®Œæ•´ï¼‰
                        content = article.get("content", "")
                        if content and content.strip():  # å¦‚æœæœ‰æ–°å†…å®¹
                            # åªåœ¨å†…å®¹ä¸ºç©ºæˆ–æ˜æ˜¾æ›´çŸ­æ—¶æ‰æ›´æ–°
                            if not existing.content or (existing.content and len(content) > len(existing.content)):
                                existing.content = content
                                # æ›´æ–°sourceå­—æ®µï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¢é˜…æºåç§°
                                existing.source = article.get("source", existing.source)

                                # å¦‚æœæ²¡æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œå°è¯•ç¿»è¯‘
                                if not existing.title_zh and self.ai_analyzer:
                                    self._translate_article_title_if_needed(existing)

                                session.commit()
                                return {"id": existing.id, "is_new": False}
                        return {"id": existing.id, "is_new": False}

                    # åˆ›å»ºæ–°æ–‡ç« 
                    content = article.get("content", "")
                    new_article = Article(
                        title=article.get("title"),
                        url=article.get("url"),
                        content=content,
                        source=article.get("source"),
                        category=article.get("category"),
                        author=article.get("author"),
                        published_at=article.get("published_at"),
                        extra_data=article.get("metadata"),
                    )

                    session.add(new_article)
                    session.commit()

                    # å¦‚æœæ²¡æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œå°è¯•ç¿»è¯‘
                    if not new_article.title_zh and self.ai_analyzer:
                        self._translate_article_title_if_needed(new_article)
                        session.commit()

                    # è¿”å›æ–°æ’å…¥çš„æ–‡ç« ID
                    return {"id": new_article.id, "is_new": True}

            except Exception as e:
                # å¦‚æœæ˜¯å”¯ä¸€æ€§çº¦æŸé”™è¯¯ï¼Œå¯èƒ½æ˜¯ç”±å¹¶å‘å¼•èµ·çš„ï¼Œé‡è¯•
                if "UNIQUE constraint failed" in str(e) and attempt < max_retries - 1:
                    logger.warning(f"âš ï¸  å¹¶å‘å†²çªï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {article.get('url', 'Unknown')}")
                    import time
                    time.sleep(0.1 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    logger.error(f"âŒ ä¿å­˜æˆ–æ›´æ–°æ–‡ç« å¤±è´¥: {e}")
                    return None

        return None

    def _translate_article_title_if_needed(self, article: Article):
        """
        å¦‚æœæ–‡ç« æ ‡é¢˜æ˜¯è‹±æ–‡ä¸”æ²¡æœ‰ä¸­æ–‡ç¿»è¯‘ï¼Œåˆ™ç¿»è¯‘ä¸ºä¸­æ–‡

        Args:
            article: æ–‡ç« å¯¹è±¡
        """
        import re

        # å¦‚æœå·²æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œè·³è¿‡
        if article.title_zh:
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºè‹±æ–‡ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        def is_english(text: str) -> bool:
            if not text:
                return False
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            return chinese_chars / len(text) < 0.3

        if is_english(article.title):
            try:
                article.title_zh = self.ai_analyzer.translate_title(article.title)
                logger.info(f"  ğŸŒ ç¿»è¯‘æ ‡é¢˜: {article.title[:50]}... â†’ {article.title_zh[:50]}...")
            except Exception as e:
                logger.warning(f"  âš ï¸  æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")

    def _analyze_articles(self, db, batch_size: int = 50, max_age_days: int = None, max_workers: int = 3) -> Dict[str, Any]:
        """
        AIåˆ†ææœªåˆ†æçš„æ–‡ç« ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            max_age_days: æœ€å¤§æ–‡ç« å¹´é¾„ï¼ˆå¤©æ•°ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ–‡ç« ä¸åˆ†æã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3
        """
        from config.settings import settings
        
        # å¦‚æœæœªæŒ‡å®šmax_age_daysï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼
        if max_age_days is None:
            max_age_days = settings.MAX_ANALYSIS_AGE_DAYS
        
        stats = {"analyzed_count": 0, "analysis_error": 0, "skipped_old": 0}

        with db.get_session() as session:
            # è®¡ç®—æ—¶é—´é˜ˆå€¼ï¼ˆåªåˆ†ææœ€è¿‘max_age_dayså¤©çš„æ–‡ç« ï¼‰
            # å¦‚æœmax_age_daysä¸º0ï¼Œè¡¨ç¤ºä¸é™åˆ¶ï¼Œåˆ†ææ‰€æœ‰æ–‡ç« 
            if max_age_days > 0:
                time_threshold = datetime.now() - timedelta(days=max_age_days)
            else:
                time_threshold = None
            
            # è·å–æœªåˆ†æçš„æ–‡ç« ï¼ˆåªåˆ†ææœ€è¿‘çš„æ–‡ç« ï¼‰
            query = session.query(Article).filter(
                Article.is_processed == False,
                Article.published_at.isnot(None)
            )
            
            # å¦‚æœé…ç½®äº†æ—¶é—´é™åˆ¶ï¼Œæ·»åŠ æ—¶é—´è¿‡æ»¤
            if time_threshold:
                query = query.filter(Article.published_at >= time_threshold)
            
            unanalyzed = query.order_by(Article.published_at.desc()).limit(batch_size).all()
            
            # ç»Ÿè®¡è·³è¿‡çš„æ—§æ–‡ç« ï¼ˆä»…åœ¨é…ç½®äº†æ—¶é—´é™åˆ¶æ—¶ï¼‰
            if time_threshold:
                skipped_count = (
                    session.query(Article)
                    .filter(
                        Article.is_processed == False,
                        Article.published_at.isnot(None),
                        Article.published_at < time_threshold
                    )
                    .count()
                )
            else:
                skipped_count = 0
            stats["skipped_old"] = skipped_count

            if not unanalyzed:
                if skipped_count > 0:
                    logger.info(f"  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æçš„æ–‡ç« ï¼ˆè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
                else:
                    logger.info("  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æçš„æ–‡ç« ")
                return stats

            logger.info(f"  ğŸ¤– å¼€å§‹å¹¶å‘åˆ†æ {len(unanalyzed)} ç¯‡æ–‡ç« ï¼ˆæŒ‰æ—¶é—´ä»æ–°åˆ°æ—§æ’åºï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼Œè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
            
            # æ˜¾ç¤ºå°†è¦åˆ†æçš„æ–‡ç« æ—¶é—´èŒƒå›´
            if unanalyzed:
                latest_date = unanalyzed[0].published_at
                oldest_date = unanalyzed[-1].published_at
                if latest_date and oldest_date:
                    logger.info(f"  ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: {oldest_date.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')}")

            # é¢„å…ˆåŠ è½½æ‰€æœ‰å±æ€§ï¼Œé¿å…åœ¨å¹¶å‘æ—¶å‡ºç°DetachedInstanceError
            for article in unanalyzed:
                _ = article.id
                _ = article.title
                _ = article.content
                _ = article.source
                _ = article.published_at
            
            session.expunge_all()

            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIAnalyzerå®ä¾‹ï¼Œé¿å…å¹¶å‘å†²çª
            # OpenAIå®¢æˆ·ç«¯å†…éƒ¨æœ‰è¿æ¥æ± ï¼Œå¤šçº¿ç¨‹å…±äº«ä¸å®‰å…¨
            from utils.factories import create_ai_analyzer

            # å¹¶å‘åˆ†ææ–‡ç« 
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è· article.idï¼Œé¿å…é—­åŒ…é™·é˜±å’Œ DetachedInstanceError
            def analyze_single_article(article_obj, article_id=None):
                """åˆ†æå•ç¯‡æ–‡ç« ï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰"""
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIåˆ†æå™¨å®ä¾‹
                thread_ai_analyzer = create_ai_analyzer()

                # å¦‚æœä¼ å…¥çš„æ˜¯ article å¯¹è±¡ï¼Œæå– IDï¼›å¦åˆ™ä½¿ç”¨ä¼ å…¥çš„ article_id
                if article_id is None:
                    article_id = article_obj.id if hasattr(article_obj, 'id') else None

                try:
                    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                    with db.get_session() as article_session:
                        # é‡æ–°æŸ¥è¯¢æ–‡ç« ï¼ˆé¿å…DetachedInstanceErrorï¼‰
                        article_obj = article_session.query(Article).filter(Article.id == article_id).first()
                        if not article_obj:
                            return {"success": False, "reason": "article_not_found"}

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘æ ‡é¢˜ï¼ˆè‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼‰
                        # æ”¾åœ¨is_processedæ£€æŸ¥ä¹‹å‰ï¼Œç¡®ä¿å³ä½¿æ˜¯å·²åˆ†æçš„æ–‡ç« ä¹Ÿèƒ½ç¿»è¯‘
                        if not article_obj.title_zh:
                            import re

                            # ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºè‹±æ–‡ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                            def is_english(text: str) -> bool:
                                if not text:
                                    return False
                                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
                                return chinese_chars / len(text) < 0.3

                            if is_english(article_obj.title):
                                logger.info(f"  ğŸŒ ç¿»è¯‘æ ‡é¢˜: {article_obj.title[:50]}...")
                                try:
                                    article_obj.title_zh = thread_ai_analyzer.translate_title(article_obj.title)
                                    article_session.commit()
                                except Exception as e:
                                    logger.warning(f"  âš ï¸  æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
                                    article_session.rollback()

                        # å¦‚æœå·²ç»åˆ†æè¿‡ï¼Œè·³è¿‡AIåˆ†æ
                        if article_obj.is_processed:
                            return {"success": False, "reason": "already_processed"}

                        # å‡†å¤‡æ–‡ç« æ•°æ®
                        article_dict = {
                            "title": article_obj.title,
                            "content": article_obj.content,
                            "source": article_obj.source,
                            "published_at": article_obj.published_at,
                        }

                        # AIåˆ†æï¼ˆä½¿ç”¨çº¿ç¨‹ç‹¬ç«‹çš„AIåˆ†æå™¨ï¼‰
                        result = thread_ai_analyzer.analyze_article(article_dict)

                        # æ›´æ–°æ–‡ç« 
                        article_obj.summary = result.get("summary")
                        article_obj.topics = result.get("topics")
                        article_obj.tags = result.get("tags")
                        article_obj.importance = result.get("importance")
                        article_obj.target_audience = result.get("target_audience")
                        article_obj.key_points = result.get("key_points")
                        article_obj.is_processed = True

                        article_session.commit()
                        return {"success": True, "article_id": article_obj.id}
                        
                except Exception as e:
                    logger.error(f"  âŒ åˆ†ææ–‡ç« å¤±è´¥ (ID={article_id}): {e}")
                    return {"success": False, "error": str(e)}

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è· article.idï¼Œé¿å…é—­åŒ…é™·é˜±
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_article = {
                    executor.submit(analyze_single_article, article, article.id): article
                    for article in unanalyzed
                }
                
                completed = 0
                for future in as_completed(future_to_article):
                    article = future_to_article[future]
                    article_id = article.id  # æå‰ä¿å­˜ IDï¼Œé¿å… DetachedInstanceError
                    completed += 1
                    
                    try:
                        result = future.result()
                        if result.get("success"):
                            stats["analyzed_count"] += 1
                            if completed % 5 == 0 or completed == len(unanalyzed):
                                logger.info(f"  âœ… [{completed}/{len(unanalyzed)}] AIåˆ†æè¿›åº¦")
                        else:
                            stats["analysis_error"] += 1
                    except Exception as e:
                        logger.error(f"  âŒ åˆ†ææ–‡ç« å¼‚å¸¸ (ID={article_id}): {e}")
                        stats["analysis_error"] += 1

        logger.info(f"  âœ… AIåˆ†æå®Œæˆ: {stats['analyzed_count']} ç¯‡æˆåŠŸ, {stats['analysis_error']} ç¯‡å¤±è´¥")
        return stats

    def _analyze_articles_by_ids(self, db, article_ids: List[int], max_workers: int = 3) -> int:
        """
        æ ¹æ®æ–‡ç« IDåˆ—è¡¨è¿›è¡Œå¹¶å‘AIåˆ†æ

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æˆåŠŸåˆ†æçš„æ–‡ç« æ•°é‡
        """
        if not article_ids or not self.ai_analyzer:
            return 0

        analyzed_count = 0

        # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIAnalyzerå®ä¾‹ï¼Œé¿å…å¹¶å‘å†²çª
        # OpenAIå®¢æˆ·ç«¯å†…éƒ¨æœ‰è¿æ¥æ± ï¼Œå¤šçº¿ç¨‹å…±äº«ä¸å®‰å…¨
        from utils.factories import create_ai_analyzer

        def analyze_single_article_id(article_id):
            """æ ¹æ®IDåˆ†æå•ç¯‡æ–‡ç« """
            try:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„AIåˆ†æå™¨å®ä¾‹
                thread_ai_analyzer = create_ai_analyzer()

                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                with db.get_session() as session:
                    # é‡æ–°æŸ¥è¯¢æ–‡ç« 
                    article_obj = session.query(Article).filter(Article.id == article_id).first()
                    if not article_obj or article_obj.is_processed:
                        return {"success": False, "reason": "already_processed"}

                    # å‡†å¤‡æ–‡ç« æ•°æ®
                    article_dict = {
                        "title": article_obj.title,
                        "content": article_obj.content,
                        "source": article_obj.source,
                        "published_at": article_obj.published_at,
                    }

                    # AIåˆ†æï¼ˆä½¿ç”¨çº¿ç¨‹ç‹¬ç«‹çš„AIåˆ†æå™¨ï¼‰
                    result = thread_ai_analyzer.analyze_article(article_dict)

                    # æ›´æ–°æ–‡ç« 
                    article_obj.summary = result.get("summary")
                    article_obj.topics = result.get("topics")
                    article_obj.tags = result.get("tags")
                    article_obj.importance = result.get("importance")
                    article_obj.target_audience = result.get("target_audience")
                    article_obj.key_points = result.get("key_points")
                    article_obj.is_processed = True

                    session.commit()
                    return {"success": True, "article_id": article_obj.id}

            except Exception as e:
                logger.error(f"  âŒ åˆ†ææ–‡ç« å¤±è´¥ (ID={article_id}): {e}")
                return {"success": False, "error": str(e)}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {
                executor.submit(analyze_single_article_id, article_id): article_id
                for article_id in article_ids
            }

            completed = 0
            for future in as_completed(future_to_id):
                article_id = future_to_id[future]
                completed += 1

                try:
                    result = future.result()
                    if result.get("success"):
                        analyzed_count += 1
                        if completed % 5 == 0 or completed == len(article_ids):
                            logger.info(f"  âœ… [{completed}/{len(article_ids)}] AIåˆ†æè¿›åº¦")
                except Exception as e:
                    logger.error(f"  âŒ åˆ†ææ–‡ç« å¼‚å¸¸ (ID={article_id}): {e}")

        logger.info(f"  âœ… AIåˆ†æå®Œæˆ: {analyzed_count} ç¯‡")
        return analyzed_count

    def _filter_unanalyzed_articles(self, db, article_ids: List[int]) -> List[int]:
        """
        è¿‡æ»¤å‡ºæœªåˆ†æçš„æ–‡ç« IDåˆ—è¡¨

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨

        Returns:
            æœªåˆ†æçš„æ–‡ç« IDåˆ—è¡¨
        """
        if not article_ids:
            return []

        try:
            with db.get_session() as session:
                # æŸ¥è¯¢æœªåˆ†æçš„æ–‡ç« 
                unanalyzed = session.query(Article.id).filter(
                    Article.id.in_(article_ids),
                    Article.is_processed == False
                ).all()

                return [row[0] for row in unanalyzed]
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æœªåˆ†ææ–‡ç« å¤±è´¥: {e}")
            return article_ids  # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›æ‰€æœ‰IDç»§ç»­å¤„ç†

    def _log_collection(self, db, source_name: str, source_type: str, status: str, count: int, error: str = None):
        """è®°å½•é‡‡é›†æ—¥å¿—"""
        try:
            with db.get_session() as session:
                log = CollectionLog(
                    source_name=source_name,
                    source_type=source_type,
                    status=status,
                    articles_count=count,
                    error_message=error,
                )
                session.add(log)
                session.commit()
        except Exception as e:
            logger.error(f"âŒ è®°å½•æ—¥å¿—å¤±è´¥: {e}")

    def get_recent_articles(self, db, limit: int = 100, hours: int = 24) -> List[Article]:
        """è·å–æœ€è¿‘çš„æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=hours)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold)
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles

    def get_daily_summary(self, db, limit: int = 10) -> List[Article]:
        """è·å–æ¯æ—¥é‡è¦æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=24)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold, Article.importance.in_(["high", "medium"]))
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles
