"""
ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡
"""
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path
import logging
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed

from sqlalchemy import desc
from sqlalchemy.orm import Session

from collector.rss_collector import RSSCollector
from collector.api_collector import ArXivCollector, HuggingFaceCollector, PapersWithCodeCollector
from database import get_db
from database.models import Article, CollectionLog, RSSSource
from analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class CollectionService:
    """ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self, config_path: str = "config/sources.json", ai_analyzer: AIAnalyzer = None):
        self.ai_analyzer = ai_analyzer
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–å„ä¸ªé‡‡é›†å™¨
        self.rss_collector = RSSCollector()
        self.arxiv_collector = ArXivCollector()
        self.hf_collector = HuggingFaceCollector()
        self.pwc_collector = PapersWithCodeCollector()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {"rss_sources": [], "api_sources": [], "web_sources": [], "social_sources": []}

    def collect_all(self, enable_ai_analysis: bool = True, task_id: int = None) -> Dict[str, Any]:
        """
        é‡‡é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº

        Args:
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æž
            task_id: ä»»åŠ¡IDï¼Œç”¨äºŽå®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ðŸš€ å¼€å§‹é‡‡é›†æ‰€æœ‰æ•°æ®æº")
        stats = {
            "total_articles": 0,
            "new_articles": 0,
            "sources_success": 0,
            "sources_error": 0,
            "start_time": datetime.now(),
        }

        db = get_db()

        # 1. é‡‡é›†RSSæº
        logger.info("\nðŸ“¡ é‡‡é›†RSSæº")
        rss_stats = self._collect_rss_sources(db, task_id=task_id)
        stats.update(rss_stats)
        
        # å®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        # 2. é‡‡é›†APIæºï¼ˆarXiv, Hugging Faceç­‰ï¼‰
        logger.info("\nðŸ“š é‡‡é›†è®ºæ–‡APIæº")
        api_stats = self._collect_api_sources(db, task_id=task_id)
        stats.update(api_stats)
        
        # å®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        # 3. AIåˆ†æžï¼ˆæŒ‰æ—¶é—´ä»Žæ–°åˆ°æ—§ï¼Œåªåˆ†æžæœ€è¿‘3å¤©çš„æ–‡ç« ï¼‰
        if enable_ai_analysis and self.ai_analyzer:
            logger.info("\nðŸ¤– å¼€å§‹AIåˆ†æžï¼ˆæŒ‰æ—¶é—´ä»Žæ–°åˆ°æ—§ï¼Œåªåˆ†æžæœ€è¿‘3å¤©çš„æ–‡ç« ï¼‰")
            ai_stats = self._analyze_articles(db, batch_size=50, max_age_days=3, max_workers=3)
            stats.update(ai_stats)
            
            # å®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if task_id:
                self._update_task_progress(db, task_id, stats)

        stats["end_time"] = datetime.now()
        stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

        logger.info(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        logger.info(f"   æ–°å¢žæ–‡ç« : {stats['new_articles']}")
        logger.info(f"   æˆåŠŸæºæ•°: {stats['sources_success']}")
        logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")

        return stats

    def _fetch_articles_full_content(self, articles: List[Dict[str, Any]], source_name: str, max_workers: int = 3) -> List[Dict[str, Any]]:
        """
        å¹¶å‘èŽ·å–æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            source_name: æºåç§°
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3ï¼ˆé¿å…å¯¹å•ä¸ªç½‘ç«™åŽ‹åŠ›è¿‡å¤§ï¼‰
        
        Returns:
            æ›´æ–°åŽçš„æ–‡ç« åˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦èŽ·å–å®Œæ•´å†…å®¹çš„æ–‡ç« ï¼ˆblogæ–‡ç« ï¼‰
        articles_to_fetch = [
            article for article in articles 
            if article.get("category") == "rss" and article.get("url")
        ]
        
        if not articles_to_fetch:
            return articles
        
        logger.info(f"  ðŸ“„ å¼€å§‹å¹¶å‘èŽ·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # å¹¶å‘èŽ·å–å®Œæ•´å†…å®¹
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_article = {
                executor.submit(self.rss_collector.fetch_full_content, article["url"]): article
                for article in articles_to_fetch
            }
            
            # æ”¶é›†ç»“æžœ
            completed = 0
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                completed += 1
                
                try:
                    full_content = future.result()
                    if full_content:
                        article["content"] = full_content
                        logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²èŽ·å–å®Œæ•´å†…å®¹: {article['title'][:50]}...")
                    else:
                        logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] æ— æ³•èŽ·å–å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨RSSæ‘˜è¦: {article['title'][:50]}...")
                except Exception as e:
                    logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] èŽ·å–å®Œæ•´å†…å®¹å¤±è´¥: {article['title'][:50]}... - {e}")
        
        logger.info(f"  âœ… å®Œæ•´å†…å®¹èŽ·å–å®Œæˆ: {len(articles_to_fetch)} ç¯‡æ–‡ç« ")
        return articles

    def _fix_source_by_feed_title(self, db, session, feed_title: str, correct_source_name: str):
        """
        æ ¹æ®feed titleä¿®æ­£æ•°æ®åº“ä¸­æ–‡ç« çš„sourceå­—æ®µ
        
        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            session: æ•°æ®åº“ä¼šè¯
            feed_title: RSS feedçš„title
            correct_source_name: æ­£ç¡®çš„è®¢é˜…æºåç§°
        """
        try:
            # æŸ¥æ‰¾sourceå­—æ®µç­‰äºŽfeed_titleçš„æ–‡ç« 
            articles_to_fix = session.query(Article).filter(
                Article.source == feed_title
            ).all()
            
            if articles_to_fix:
                fixed_count = 0
                for article in articles_to_fix:
                    article.source = correct_source_name
                    fixed_count += 1
                
                session.commit()
                logger.info(f"  ðŸ”§ å·²ä¿®æ­£ {fixed_count} ç¯‡æ–‡ç« çš„sourceå­—æ®µ: '{feed_title}' -> '{correct_source_name}'")
        except Exception as e:
            logger.warning(f"  âš ï¸  ä¿®æ­£sourceå­—æ®µå¤±è´¥: {e}")
            session.rollback()

    def _update_task_progress(self, db, task_id: int, stats: Dict[str, Any]):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            from database.models import CollectionTask
            with db.get_session() as session:
                task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                if task:
                    task.new_articles_count = stats.get('new_articles', 0)
                    task.total_sources = stats.get('sources_success', 0) + stats.get('sources_error', 0)
                    task.success_sources = stats.get('sources_success', 0)
                    task.failed_sources = stats.get('sources_error', 0)
                    task.ai_analyzed_count = stats.get('analyzed_count', 0)
                    session.commit()
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

    def _collect_rss_sources(self, db, task_id: int = None) -> Dict[str, Any]:
        """é‡‡é›†RSSæºï¼ˆä¼˜å…ˆä»Žæ•°æ®åº“è¯»å–ï¼Œå…¼å®¹é…ç½®æ–‡ä»¶ï¼‰"""
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0}

        # ä¼˜å…ˆä»Žæ•°æ®åº“è¯»å–RSSæº
        rss_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(RSSSource.enabled == True).order_by(RSSSource.priority.asc()).all()
            
            for source in db_sources:
                rss_configs.append({
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                    "max_articles": 20,  # é»˜è®¤å€¼
                    "category": source.category,
                    "tier": source.tier,
                })
                # é¢„å…ˆåŠ è½½å±žæ€§
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
                _ = source.last_collected_at
                _ = source.articles_count
            session.expunge_all()
        
        # å¦‚æžœæ•°æ®åº“ä¸­æ²¡æœ‰æºï¼Œåˆ™ä»Žé…ç½®æ–‡ä»¶è¯»å–ï¼ˆå‘åŽå…¼å®¹ï¼‰
        if not rss_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰RSSæºï¼Œä»Žé…ç½®æ–‡ä»¶è¯»å–")
            rss_configs = self.config.get("rss_sources", [])
        
        if not rss_configs:
            logger.warning("  âš ï¸  æ²¡æœ‰é…ç½®RSSæº")
            return stats

        results = self.rss_collector.fetch_multiple_feeds(rss_configs)

        # æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
        with db.get_session() as session:
            for source_name, feed_result in results.items():
                try:
                    articles = feed_result.get("articles", [])
                    feed_title = feed_result.get("feed_title")
                    
                    # å¦‚æžœfeed titleä¸Žè®¢é˜…æºåç§°ä¸ä¸€è‡´ï¼Œä¿®æ­£æ•°æ®åº“ä¸­å·²æœ‰çš„æ–‡ç« 
                    if feed_title and feed_title != source_name:
                        self._fix_source_by_feed_title(db, session, feed_title, source_name)
                    
                    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„sourceåç§°
                    for article in articles:
                        article["source"] = source_name
                    
                    # å¹¶å‘èŽ·å–å®Œæ•´å†…å®¹ï¼ˆä»…å¯¹blogæ–‡ç« ï¼‰
                    articles_with_full_content = self._fetch_articles_full_content(
                        articles, source_name, max_workers=3
                    )
                    
                    # ä¿å­˜æ–‡ç« 
                    new_count = 0
                    for article in articles_with_full_content:
                        if self._save_article(db, article):
                            new_count += 1

                    # æ›´æ–°RSSæºçš„ç»Ÿè®¡ä¿¡æ¯
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_collected_at = datetime.now()
                        source_obj.articles_count += len(articles)
                        source_obj.last_error = None
                        session.commit()

                    # è®°å½•æ—¥å¿—
                    self._log_collection(db, source_name, "rss", "success", len(articles))
                    stats["sources_success"] += 1
                    stats["new_articles"] += new_count
                    stats["total_articles"] += len(articles)

                    logger.info(f"  âœ… {source_name}: {len(articles)} ç¯‡, æ–°å¢ž {new_count} ç¯‡")

                except Exception as e:
                    logger.error(f"  âŒ {source_name}: {e}")
                    
                    # æ›´æ–°é”™è¯¯ä¿¡æ¯
                    source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                    if source_obj:
                        source_obj.last_error = str(e)
                        session.commit()
                    
                    self._log_collection(db, source_name, "rss", "error", 0, str(e))
                    stats["sources_error"] += 1

        return stats

    def _collect_api_sources(self, db, task_id: int = None) -> Dict[str, Any]:
        """é‡‡é›†APIæº"""
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0}

        api_configs = self.config.get("api_sources", [])

        for config in api_configs:
            if not config.get("enabled", True):
                continue

            name = config.get("name")
            source_type = config.get("category")

            try:
                articles = []
                if "arxiv" in name.lower():
                    query = config.get("query")
                    max_results = config.get("max_results", 20)
                    articles = self.arxiv_collector.fetch_papers(query, max_results)

                elif "hugging Face" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.hf_collector.fetch_trending_papers(limit)

                elif "papers with code" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.pwc_collector.fetch_trending_papers(limit)

                # ä¿å­˜æ–‡ç« 
                new_count = 0
                for article in articles:
                    if self._save_article(db, article):
                        new_count += 1

                # è®°å½•æ—¥å¿—
                self._log_collection(db, name, "api", "success", len(articles))
                stats["sources_success"] += 1
                stats["new_articles"] += new_count
                stats["total_articles"] += len(articles)

                logger.info(f"  âœ… {name}: {len(articles)} ç¯‡, æ–°å¢ž {new_count} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {name}: {e}")
                self._log_collection(db, name, "api", "error", 0, str(e))
                stats["sources_error"] += 1

        return stats

    def _save_article(self, db, article: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“

        Returns:
            True if new article, False if already exists
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return False

                # åˆ›å»ºæ–°æ–‡ç« 
                # å¯¹äºŽå®Œæ•´å†…å®¹ï¼Œä¸é™åˆ¶é•¿åº¦ï¼ˆä½¿ç”¨Textç±»åž‹å¯ä»¥å­˜å‚¨å¤§é‡æ–‡æœ¬ï¼‰
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,  # ä¸é™åˆ¶é•¿åº¦ï¼Œä½¿ç”¨Textç±»åž‹
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    def _analyze_articles(self, db, batch_size: int = 50, max_age_days: int = 3, max_workers: int = 3) -> Dict[str, Any]:
        """
        AIåˆ†æžæœªåˆ†æžçš„æ–‡ç« ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            max_age_days: æœ€å¤§æ–‡ç« å¹´é¾„ï¼ˆå¤©æ•°ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ–‡ç« ä¸åˆ†æžï¼Œé»˜è®¤3å¤©
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3
        """
        stats = {"analyzed_count": 0, "analysis_error": 0, "skipped_old": 0}

        with db.get_session() as session:
            # è®¡ç®—æ—¶é—´é˜ˆå€¼ï¼ˆåªåˆ†æžæœ€è¿‘max_age_dayså¤©çš„æ–‡ç« ï¼‰
            from datetime import timedelta
            time_threshold = datetime.now() - timedelta(days=max_age_days)
            
            # èŽ·å–æœªåˆ†æžçš„æ–‡ç« ï¼ˆåªåˆ†æžæœ€è¿‘çš„æ–‡ç« ï¼‰
            unanalyzed = (
                session.query(Article)
                .filter(
                    Article.is_processed == False,
                    Article.published_at.isnot(None),
                    Article.published_at >= time_threshold
                )
                .order_by(Article.published_at.desc())
                .limit(batch_size)
                .all()
            )
            
            # ç»Ÿè®¡è·³è¿‡çš„æ—§æ–‡ç« 
            skipped_count = (
                session.query(Article)
                .filter(
                    Article.is_processed == False,
                    Article.published_at.isnot(None),
                    Article.published_at < time_threshold
                )
                .count()
            )
            stats["skipped_old"] = skipped_count

            if not unanalyzed:
                if skipped_count > 0:
                    logger.info(f"  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æžçš„æ–‡ç« ï¼ˆè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
                else:
                    logger.info("  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æžçš„æ–‡ç« ")
                return stats

            logger.info(f"  ðŸ¤– å¼€å§‹å¹¶å‘åˆ†æž {len(unanalyzed)} ç¯‡æ–‡ç« ï¼ˆæŒ‰æ—¶é—´ä»Žæ–°åˆ°æ—§æŽ’åºï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼Œè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
            
            # æ˜¾ç¤ºå°†è¦åˆ†æžçš„æ–‡ç« æ—¶é—´èŒƒå›´
            if unanalyzed:
                latest_date = unanalyzed[0].published_at
                oldest_date = unanalyzed[-1].published_at
                if latest_date and oldest_date:
                    logger.info(f"  ðŸ“… åˆ†æžæ—¶é—´èŒƒå›´: {oldest_date.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')}")

            # é¢„å…ˆåŠ è½½æ‰€æœ‰å±žæ€§ï¼Œé¿å…åœ¨å¹¶å‘æ—¶å‡ºçŽ°DetachedInstanceError
            for article in unanalyzed:
                _ = article.id
                _ = article.title
                _ = article.content
                _ = article.source
                _ = article.published_at
            
            session.expunge_all()

            # å¹¶å‘åˆ†æžæ–‡ç« 
            def analyze_single_article(article):
                """åˆ†æžå•ç¯‡æ–‡ç« ï¼ˆç”¨äºŽå¹¶å‘æ‰§è¡Œï¼‰"""
                try:
                    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                    with db.get_session() as article_session:
                        # é‡æ–°æŸ¥è¯¢æ–‡ç« ï¼ˆé¿å…DetachedInstanceErrorï¼‰
                        article_obj = article_session.query(Article).filter(Article.id == article.id).first()
                        if not article_obj or article_obj.is_processed:
                            return {"success": False, "reason": "already_processed"}
                        
                        # å‡†å¤‡æ–‡ç« æ•°æ®
                        article_dict = {
                            "title": article_obj.title,
                            "content": article_obj.content,
                            "source": article_obj.source,
                            "published_at": article_obj.published_at,
                        }

                        # AIåˆ†æž
                        result = self.ai_analyzer.analyze_article(article_dict)

                        # æ›´æ–°æ–‡ç« 
                        article_obj.summary = result.get("summary")
                        article_obj.topics = result.get("topics")
                        article_obj.tags = result.get("tags")
                        article_obj.importance = result.get("importance")
                        article_obj.target_audience = result.get("target_audience")
                        article_obj.key_points = result.get("key_points")
                        article_obj.is_processed = True

                        article_session.commit()
                        return {"success": True, "article_id": article_obj.id}
                        
                except Exception as e:
                    logger.error(f"  âŒ åˆ†æžæ–‡ç« å¤±è´¥ (ID={article.id}): {e}")
                    return {"success": False, "error": str(e)}

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æž
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_article = {
                    executor.submit(analyze_single_article, article): article
                    for article in unanalyzed
                }
                
                completed = 0
                for future in as_completed(future_to_article):
                    article = future_to_article[future]
                    completed += 1
                    
                    try:
                        result = future.result()
                        if result.get("success"):
                            stats["analyzed_count"] += 1
                            if completed % 5 == 0 or completed == len(unanalyzed):
                                logger.info(f"  âœ… [{completed}/{len(unanalyzed)}] AIåˆ†æžè¿›åº¦")
                        else:
                            stats["analysis_error"] += 1
                    except Exception as e:
                        logger.error(f"  âŒ åˆ†æžæ–‡ç« å¼‚å¸¸ (ID={article.id}): {e}")
                        stats["analysis_error"] += 1

        logger.info(f"  âœ… AIåˆ†æžå®Œæˆ: {stats['analyzed_count']} ç¯‡æˆåŠŸ, {stats['analysis_error']} ç¯‡å¤±è´¥")
        return stats

    def _log_collection(self, db, source_name: str, source_type: str, status: str, count: int, error: str = None):
        """è®°å½•é‡‡é›†æ—¥å¿—"""
        try:
            with db.get_session() as session:
                log = CollectionLog(
                    source_name=source_name,
                    source_type=source_type,
                    status=status,
                    articles_count=count,
                    error_message=error,
                )
                session.add(log)
                session.commit()
        except Exception as e:
            logger.error(f"âŒ è®°å½•æ—¥å¿—å¤±è´¥: {e}")

    def get_recent_articles(self, db, limit: int = 100, hours: int = 24) -> List[Article]:
        """èŽ·å–æœ€è¿‘çš„æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=hours)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold)
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles

    def get_daily_summary(self, db, limit: int = 10) -> List[Article]:
        """èŽ·å–æ¯æ—¥é‡è¦æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=24)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold, Article.importance.in_(["high", "medium"]))
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles
