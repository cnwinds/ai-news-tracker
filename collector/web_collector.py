"""
é€šç”¨ç½‘é¡µé‡‡é›†å™¨
æ”¯æŒé€šè¿‡CSSé€‰æ‹©å™¨é…ç½®æ–‡ç« æå–è§„åˆ™
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any
import logging
import re

logger = logging.getLogger(__name__)


class WebCollector:
    """é€šç”¨ç½‘é¡µé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_articles(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»ç½‘é¡µè·å–æ–‡ç« 

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«:
                - url: ç½‘ç«™URL
                - name: æºåç§°
                - article_selector: æ–‡ç« åˆ—è¡¨çš„CSSé€‰æ‹©å™¨
                - title_selector: æ ‡é¢˜çš„CSSé€‰æ‹©å™¨
                - link_selector: é“¾æ¥çš„CSSé€‰æ‹©å™¨
                - date_selector: æ—¥æœŸçš„CSSé€‰æ‹©å™¨
                - content_selector: å†…å®¹çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - author_selector: ä½œè€…çš„CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
                - max_articles: æœ€å¤§æ–‡ç« æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        url = config.get("url")
        name = config.get("name", "Unknown")
        article_selector = config.get("article_selector")
        max_articles = config.get("max_articles", 20)

        if not url or not article_selector:
            logger.error(f"âŒ {name}: ç¼ºå°‘å¿…è¦çš„é…ç½® (url æˆ– article_selector)")
            return []

        try:
            logger.info(f"ğŸŒ æ­£åœ¨è·å–ç½‘é¡µ: {url}")

            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            articles = []
            article_elements = soup.select(article_selector)

            for i, element in enumerate(article_elements[:max_articles]):
                article = self._parse_article_element(element, config, name)
                if article:
                    articles.append(article)

            logger.info(f"âœ… {name}: æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æç½‘é¡µå¤±è´¥ {url}: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _parse_article_element(self, element: Any, config: Dict[str, Any], source_name: str) -> Dict[str, Any]:
        """
        è§£æå•ä¸ªæ–‡ç« å…ƒç´ 

        Args:
            element: BeautifulSoupå…ƒç´ 
            config: é…ç½®å­—å…¸
            source_name: æºåç§°

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            title_selector = config.get("title_selector")
            link_selector = config.get("link_selector")
            date_selector = config.get("date_selector")
            content_selector = config.get("content_selector")
            description_selector = config.get("description_selector")
            author_selector = config.get("author_selector")

            title = ""
            url = ""
            published_at = None
            author = ""
            content = ""

            if title_selector:
                title_elem = element.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)

            if link_selector:
                if link_selector == "self":
                    url = element.get("href", "")
                else:
                    link_elem = element.select_one(link_selector)
                    if link_elem:
                        url = link_elem.get("href", "")
                if url and not url.startswith("http"):
                    base_url = config.get("url")
                    url = self._resolve_url(url, base_url)

            if date_selector:
                date_elem = element.select_one(date_selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    published_at = self._parse_date(date_text)

            if author_selector:
                author_elem = element.select_one(author_selector)
                if author_elem:
                    author = author_elem.get_text(strip=True)

            if content_selector:
                content_elem = element.select_one(content_selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)

            if description_selector and not content:
                desc_elem = element.select_one(description_selector)
                if desc_elem:
                    content = desc_elem.get_text(strip=True)

            if not title or not url:
                logger.warning(f"âš ï¸  æ–‡ç« ç¼ºå°‘æ ‡é¢˜æˆ–URL: {title[:50] if title else 'N/A'}")
                return None

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source_name,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
            return None

    def _resolve_url(self, url: str, base_url: str) -> str:
        """
        è§£æç›¸å¯¹URLä¸ºç»å¯¹URL

        Args:
            url: å¯èƒ½æ˜¯ç›¸å¯¹çš„URL
            base_url: åŸºç¡€URL

        Returns:
            ç»å¯¹URL
        """
        if url.startswith("//"):
            return "https:" + url
        elif url.startswith("/"):
            from urllib.parse import urljoin
            return urljoin(base_url, url)
        else:
            return url

    def _parse_date(self, date_text: str) -> datetime or None:
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²

        Args:
            date_text: æ—¥æœŸæ–‡æœ¬

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        if not date_text:
            return None

        month_names = {
            "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
            "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12
        }

        date_patterns = [
            (r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}", "month_day_year"),
            r"\d{4}-\d{2}-\d{2}",
            r"\d{4}/\d{2}/\d{2}",
            r"\d{4}å¹´\d{2}æœˆ\d{2}æ—¥",
            r"\d{2}-\d{2}-\d{4}",
            r"\d{2}/\d{2}/\d{4}",
        ]

        for pattern in date_patterns:
            if isinstance(pattern, tuple):
                pattern_str, format_type = pattern
            else:
                pattern_str = pattern
                format_type = "default"

            match = re.search(pattern_str, date_text)
            if match:
                date_str = match.group(0)
                try:
                    if format_type == "month_day_year":
                        parts = date_str.replace(",", "").split()
                        month = month_names.get(parts[0])
                        day = int(parts[1])
                        year = int(parts[2])
                        return datetime(year, month, day)
                    elif "å¹´" in date_str:
                        parts = re.split(r"[å¹´æœˆæ—¥]", date_str)
                        return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                    elif "-" in date_str:
                        parts = date_str.split("-")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                    elif "/" in date_str:
                        parts = date_str.split("/")
                        if len(parts) == 3 and len(parts[0]) == 4:
                            return datetime(int(parts[0]), int(parts[1]), int(parts[2]))
                        elif len(parts) == 3 and len(parts[2]) == 4:
                            return datetime(int(parts[2]), int(parts[0]), int(parts[1]))
                except Exception:
                    continue

        return None

    def fetch_full_content(self, url: str) -> str:
        """
        è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹

        Args:
            url: æ–‡ç« URL

        Returns:
            å®Œæ•´å†…å®¹æ–‡æœ¬
        """
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨è·å–å®Œæ•´å†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main article',
                '[role="article"]',
                '.blog-post-content',
            ]

            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(separator=" ", strip=True)
                    if len(content) > 500:
                        break

            if not content or len(content) < 500:
                for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    tag.decompose()
                content = soup.get_text(separator=" ", strip=True)

            content = " ".join(content.split())

            logger.info(f"âœ… æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æå®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""

    def fetch_single_source(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªç½‘ç«™æºï¼ˆå…¬å¼€æ¥å£ï¼‰

        Args:
            config: ç½‘ç«™é…ç½®

        Returns:
            {"articles": [articles]}
        """
        articles = self.fetch_articles(config)
        return {"articles": articles}

    def fetch_multiple_sources(self, configs: List[Dict[str, Any]], max_workers: int = 5) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªç½‘ç«™æºï¼ˆå¹¶å‘ï¼‰

        Args:
            configs: ç½‘ç«™é…ç½®åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            {source_name: {"articles": [articles]}}
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed

        results = {}

        enabled_configs = [
            config for config in configs
            if config.get("enabled", True) and config.get("url") and config.get("article_selector")
        ]

        if not enabled_configs:
            logger.warning("âš ï¸  æ²¡æœ‰å¯ç”¨çš„ç½‘ç«™æº")
            return results

        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(enabled_configs)} ä¸ªç½‘ç«™æºï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_config = {
                executor.submit(self.fetch_single_source, config): config
                for config in enabled_configs
            }

            completed = 0
            for future in as_completed(future_to_config):
                config = future_to_config[future]
                name = config.get("name", "Unknown")
                completed += 1

                try:
                    source_result = future.result()
                    results[name] = source_result
                    logger.info(f"âœ… [{completed}/{len(enabled_configs)}] {name}: è·å– {len(source_result.get('articles', []))} ç¯‡æ–‡ç« ")
                except Exception as e:
                    logger.error(f"âŒ [{completed}/{len(enabled_configs)}] {name}: è·å–å¤±è´¥ - {e}")
                    results[name] = {"articles": []}

        logger.info(f"âœ… ç½‘ç«™æºè·å–å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in results.values() if r.get('articles')])}/{len(enabled_configs)}")
        return results
