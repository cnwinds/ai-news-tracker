"""
æ–‡ç« æ€»ç»“ç”Ÿæˆå™¨
ç”¨äºç”Ÿæˆæ¯æ—¥å’Œæ¯å‘¨çš„æ–‡ç« æ€»ç»“
"""
from datetime import datetime, timedelta
from typing import List, Dict, Any
from database import DatabaseManager
from database.models import Article, DailySummary
from analyzer.ai_analyzer import AIAnalyzer
import logging

logger = logging.getLogger(__name__)


class SummaryGenerator:
    """æ–‡ç« æ€»ç»“ç”Ÿæˆå™¨"""

    def __init__(self, ai_analyzer: AIAnalyzer):
        self.ai_analyzer = ai_analyzer

    def generate_daily_summary(self, db: DatabaseManager, date: datetime = None) -> DailySummary:
        """
        ç”Ÿæˆæ¯æ—¥æ€»ç»“

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            date: æ€»ç»“æ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰

        Returns:
            DailySummaryå¯¹è±¡
        """
        if date is None:
            date = datetime.now()

        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆè¿‡å»24å°æ—¶ï¼‰
        end_date = date
        start_date = date - timedelta(days=1)

        logger.info(f"ğŸ“ ç”Ÿæˆæ¯æ—¥æ€»ç»“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

        # ç›´æ¥åœ¨åŒä¸€ä¸ªsessionä¸­å¤„ç†æ‰€æœ‰é€»è¾‘
        return self._create_summary(db, start_date, end_date, "daily", date)

    def generate_weekly_summary(self, db: DatabaseManager, date: datetime = None) -> DailySummary:
        """
        ç”Ÿæˆæ¯å‘¨æ€»ç»“

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            date: æ€»ç»“æ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰

        Returns:
            DailySummaryå¯¹è±¡
        """
        if date is None:
            date = datetime.now()

        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆè¿‡å»7å¤©ï¼‰
        end_date = date
        start_date = date - timedelta(days=7)

        logger.info(f"ğŸ“ ç”Ÿæˆæ¯å‘¨æ€»ç»“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

        # ç›´æ¥åœ¨åŒä¸€ä¸ªsessionä¸­å¤„ç†æ‰€æœ‰é€»è¾‘
        return self._create_summary(db, start_date, end_date, "weekly", date)

    def _create_summary(
        self,
        db: DatabaseManager,
        start_date: datetime,
        end_date: datetime,
        summary_type: str,
        date: datetime
    ) -> DailySummary:
        """
        åˆ›å»ºæ€»ç»“

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            start_date: å¼€å§‹æ—¶é—´
            end_date: ç»“æŸæ—¶é—´
            summary_type: æ€»ç»“ç±»å‹ï¼ˆdaily/weeklyï¼‰
            date: æ€»ç»“æ—¥æœŸ

        Returns:
            DailySummaryå¯¹è±¡
        """
        start_time = datetime.now()

        # åœ¨åŒä¸€ä¸ªsessionä¸­æŸ¥è¯¢æ–‡ç« å¹¶æå–æ•°æ®
        with db.get_session() as session:
            # æŸ¥è¯¢å·²åˆ†æçš„æ–‡ç« ï¼ŒæŒ‰é‡è¦æ€§å’Œå‘å¸ƒæ—¶é—´æ’åº
            articles = session.query(Article).filter(
                Article.is_processed == True,
                Article.published_at >= start_date,
                Article.published_at <= end_date
            ).order_by(
                Article.importance.desc(),
                Article.published_at.desc()
            ).all()

            if not articles:
                logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
                return None

            # å‡†å¤‡æ–‡ç« æ•°æ®
            articles_data = []
            for article in articles:
                display_title = article.title_zh if article.title_zh else article.title
                articles_data.append({
                    "id": article.id,
                    "title": display_title,
                    "source": article.source,
                    "importance": article.importance,
                    "published_at": article.published_at,
                    "summary": article.summary,
                    "key_points": article.key_points,
                    "topics": article.topics,
                })

        # ç»Ÿè®¡ä¿¡æ¯
        high_count = sum(1 for a in articles_data if a.get("importance") == "high")
        medium_count = sum(1 for a in articles_data if a.get("importance") == "medium")

        logger.info(f"  æ–‡ç« æ€»æ•°: {len(articles_data)} (é«˜é‡è¦æ€§: {high_count}, ä¸­é‡è¦æ€§: {medium_count})")

        # è°ƒç”¨LLMç”Ÿæˆæ€»ç»“
        prompt = self._build_summary_prompt(articles_data, summary_type)
        summary_content = self.ai_analyzer.client.chat.completions.create(
            model=self.ai_analyzer.model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIé¢†åŸŸæ–°é—»åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¤§é‡æ–‡ç« ä¸­æç‚¼å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,
            max_tokens=2000
        )
        summary_text = summary_content.choices[0].message.content

        # æå–å…³é”®ä¸»é¢˜
        key_topics = self._extract_topics(articles_data)

        # æ¨èé‡è¦æ–‡ç« 
        recommended_articles = self._select_recommended_articles(articles_data)

        # è®¡ç®—è€—æ—¶
        generation_time = (datetime.now() - start_time).total_seconds()

        # ä¿å­˜åˆ°æ•°æ®åº“
        with db.get_session() as session:
            summary = DailySummary(
                summary_type=summary_type,
                summary_date=date,
                start_date=start_date,
                end_date=end_date,
                total_articles=len(articles_data),
                high_importance_count=high_count,
                medium_importance_count=medium_count,
                summary_content=summary_text,
                key_topics=key_topics,
                recommended_articles=recommended_articles,
                model_used=self.ai_analyzer.model,
                generation_time=generation_time
            )
            session.add(summary)
            session.flush()
            
            summary_id = summary.id
            logger.info(f"âœ… æ€»ç»“å·²ä¿å­˜ (ID: {summary_id})")
            
        # åœ¨sessionå¤–åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è±¡è¿”å›ï¼Œé¿å…detached instanceé—®é¢˜
        return DailySummary(
            id=summary_id,
            summary_type=summary_type,
            summary_date=date,
            start_date=start_date,
            end_date=end_date,
            total_articles=len(articles_data),
            high_importance_count=high_count,
            medium_importance_count=medium_count,
            summary_content=summary_text,
            key_topics=key_topics,
            recommended_articles=recommended_articles,
            model_used=self.ai_analyzer.model,
            generation_time=generation_time
        )

    def _build_summary_prompt(self, articles_data: List[Dict[str, Any]], summary_type: str) -> str:
        """
        æ„å»ºæ€»ç»“æç¤ºè¯

        Args:
            articles_data: æ–‡ç« æ•°æ®åˆ—è¡¨
            summary_type: æ€»ç»“ç±»å‹ï¼ˆdaily/weeklyï¼‰

        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        time_str = "è¿‡å»24å°æ—¶" if summary_type == "daily" else "è¿‡å»7å¤©"

        # é€‰æ‹©æœ€é‡è¦çš„æ–‡ç« ï¼ˆæœ€å¤š20ç¯‡ï¼‰
        important_articles = articles_data[:20]

        # æ„å»ºæ–‡ç« åˆ—è¡¨
        articles_str = ""
        for i, article in enumerate(important_articles, 1):
            importance_emoji = "ğŸ”´" if article.get("importance") == "high" else "ğŸŸ¡" if article.get("importance") == "medium" else "âšª"
            articles_str += f"""
{i}. {importance_emoji} [{article.get('source', 'Unknown')}] {article.get('title', 'N/A')}
   å‘å¸ƒæ—¶é—´: {article.get('published_at', datetime.now()).strftime('%Y-%m-%d %H:%M')}
   æ‘˜è¦: {article.get('summary', '')[:200]}...
"""

        prompt = f"""è¯·åŸºäº{time_str}é‡‡é›†çš„ä»¥ä¸‹AIé¢†åŸŸæ–‡ç« ï¼Œç”Ÿæˆä¸€ä»½{time_str}çš„æ–°é—»æ€»ç»“ã€‚

æ–‡ç« åˆ—è¡¨ï¼š
{articles_str}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ€»ç»“ï¼š

# ğŸ“Š {time_str}AIæ–°é—»æ€»ç»“

## ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ
- æ–‡ç« æ€»æ•°ï¼šXç¯‡
- é«˜é‡è¦æ€§ï¼šXç¯‡
- ä¸­é‡è¦æ€§ï¼šXç¯‡

## ğŸ”¥ é‡ç‚¹æ–‡ç« 
åˆ—å‡º3-5ç¯‡æœ€é‡è¦çš„æ–‡ç« ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡ç« æ ‡é¢˜å’Œæ¥æº
- æ ¸å¿ƒå†…å®¹
- ä¸ºä»€ä¹ˆé‡è¦

## ğŸ“Œ é‡è¦è¶‹åŠ¿
ä»è¿™äº›æ–‡ç« ä¸­æ€»ç»“å‡º2-3ä¸ªé‡è¦è¶‹åŠ¿æˆ–çƒ­ç‚¹è¯é¢˜

## ğŸ¯ æ¨èé˜…è¯»
æ ¹æ®æ–‡ç« çš„å…³è”æ€§å’Œé‡è¦æ€§ï¼Œæ¨è5-10ç¯‡å€¼å¾—æ·±å…¥é˜…è¯»çš„æ–‡ç« 

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šã€ç®€æ´çš„é£æ ¼ã€‚"""

        return prompt

    def _extract_topics(self, articles_data: List[Dict[str, Any]]) -> List[str]:
        """
        ä»æ–‡ç« ä¸­æå–å…³é”®ä¸»é¢˜

        Args:
            articles_data: æ–‡ç« æ•°æ®åˆ—è¡¨

        Returns:
            ä¸»é¢˜åˆ—è¡¨
        """
        topics_set = set()

        for article in articles_data:
            if article.get("topics"):
                for topic in article.get("topics", []):
                    if topic:
                        topics_set.add(topic)

        return list(topics_set)[:10]  # æœ€å¤šè¿”å›10ä¸ªä¸»é¢˜

    def _select_recommended_articles(self, articles_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        é€‰æ‹©æ¨èæ–‡ç« 

        Args:
            articles_data: æ–‡ç« æ•°æ®åˆ—è¡¨

        Returns:
            æ¨èæ–‡ç« åˆ—è¡¨
        """
        recommended = []

        # ä¼˜å…ˆé€‰æ‹©é«˜é‡è¦æ€§æ–‡ç« 
        high_importance = [a for a in articles_data if a.get("importance") == "high"]
        medium_importance = [a for a in articles_data if a.get("importance") == "medium"]

        # é€‰æ‹©æœ€å¤š10ç¯‡æ¨èæ–‡ç« 
        selected_articles = (high_importance + medium_importance)[:10]

        for article in selected_articles:
            reason = ""
            if article.get("importance") == "high":
                reason = "é«˜é‡è¦æ€§æ–‡ç« ï¼Œå€¼å¾—é‡ç‚¹å…³æ³¨"
            elif article.get("importance") == "medium":
                reason = "ä¸­ç­‰é‡è¦æ€§ï¼Œå»ºè®®é˜…è¯»"
            if article.get("key_points"):
                reason += f"ã€‚å…³é”®ç‚¹ï¼š{article.get('key_points')[0][:50]}..."

            recommended.append({
                "id": article.get("id"),
                "title": article.get("title"),
                "source": article.get("source"),
                "importance": article.get("importance"),
                "reason": reason
            })

        return recommended
