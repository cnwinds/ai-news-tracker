"""
APIæ•°æ®é‡‡é›†å™¨ï¼ˆarXiv, Hugging Faceç­‰ï¼‰
"""
import requests
import feedparser
from datetime import datetime
from typing import List, Dict, Any
import logging
import time

logger = logging.getLogger(__name__)


class ArXivCollector:
    """arXivè®ºæ–‡é‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "http://export.arxiv.org/api/query"

    def fetch_papers(self, query: str, max_results: int = 20) -> List[Dict[str, Any]]:
        """
        ä»arXivè·å–è®ºæ–‡

        Args:
            query: æŸ¥è¯¢æ¡ä»¶ (å¦‚: cat:cs.AI)
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“š æ­£åœ¨è·å–arXivè®ºæ–‡: {query}")

            params = {
                "search_query": query,
                "start": 0,
                "max_results": max_results,
                "sortBy": "submittedDate",
                "sortOrder": "descending",
            }

            response = requests.get(self.base_url, params=params, timeout=self.timeout)
            response.raise_for_status()

            # è§£æAtom feed
            feed = feedparser.parse(response.content)

            papers = []
            for entry in feed.entries:
                paper = self._parse_arxiv_entry(entry)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡arXivè®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–arXivè®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_arxiv_entry(self, entry: Any) -> Dict[str, Any]:
        """è§£æarXivè®ºæ–‡æ¡ç›®"""
        try:
            # æå–ä½œè€…
            authors = ", ".join([author.name for author in entry.authors[:5]]) if hasattr(entry, "authors") else ""

            # æå–æ‘˜è¦
            summary = entry.get("summary", "")

            # arXiv ID
            arxiv_id = entry.get("id", "").split("/abs/")[-1] if "/abs/" in entry.get("id", "") else ""

            # PDFé“¾æ¥
            pdf_url = entry.get("link", "").replace("/abs/", "/pdf/") + ".pdf" if "/abs/" in entry.get("link", "") else ""

            # å‘å¸ƒæ—¶é—´
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6])

            return {
                "title": entry.get("title", ""),
                "url": entry.get("id", ""),
                "content": summary,
                "source": "arXiv",
                "author": authors,
                "published_at": published_at,
                "category": "paper",
                "metadata": {
                    "arxiv_id": arxiv_id,
                    "pdf_url": pdf_url,
                    "primary_category": entry.get("arxiv_primary_category", {}).get("term", ""),
                    "categories": [tag.term for tag in entry.tags] if hasattr(entry, "tags") else [],
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æarXivè®ºæ–‡å¤±è´¥: {e}")
            return None


class HuggingFaceCollector:
    """Hugging Faceè®ºæ–‡é‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "https://huggingface.co/api/papers"

    def fetch_trending_papers(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–è¶‹åŠ¿è®ºæ–‡

        Args:
            limit: æœ€å¤§æ•°é‡

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ”¥ æ­£åœ¨è·å–Hugging Faceè¶‹åŠ¿è®ºæ–‡")

            url = f"{self.base_url}/trending"
            params = {"limit": limit}

            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()

            data = response.json()

            papers = []
            for item in data:
                paper = self._parse_hf_paper(item)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡Hugging Faceè¶‹åŠ¿è®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–Hugging Faceè®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_hf_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æHugging Faceè®ºæ–‡"""
        try:
            # æå–ä½œè€…
            authors = item.get("authors", [])
            author_str = ", ".join(authors[:5]) + ("..." if len(authors) > 5 else "")

            # å‘å¸ƒæ—¶é—´
            published_at = None
            if item.get("publishedAt"):
                try:
                    published_at = datetime.fromisoformat(item["publishedAt"].replace("Z", "+00:00"))
                except:
                    pass

            return {
                "title": item.get("title", ""),
                "url": item.get("paperUrl", ""),
                "content": item.get("summary", item.get("abstract", "")),
                "source": "Hugging Face",
                "author": author_str,
                "published_at": published_at,
                "category": "paper",
                "metadata": {
                    "hf_id": item.get("id", ""),
                    "likes": item.get("likesCount", 0),
                    "models": item.get("models", []),
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æHFè®ºæ–‡å¤±è´¥: {e}")
            return None


class PapersWithCodeCollector:
    """Papers with Codeé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.base_url = "https://paperswithcode.com/api/v1"

    def fetch_trending_papers(self, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–è¶‹åŠ¿è®ºæ–‡"""
        try:
            logger.info(f"ğŸ“ˆ æ­£åœ¨è·å–Papers with Codeè¶‹åŠ¿è®ºæ–‡")

            url = f"{self.base_url}/papers/"
            params = {"ordering": "-stars", "page_size": limit}

            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()

            data = response.json()

            papers = []
            for item in data.get("results", []):
                paper = self._parse_pwc_paper(item)
                if paper:
                    papers.append(paper)

            logger.info(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡Papers with Codeè®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"âŒ è·å–Papers with Codeå¤±è´¥: {e}")
            return []

    def _parse_pwc_paper(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æPapers with Codeè®ºæ–‡"""
        try:
            # æå–ä½œè€…
            authors = item.get("authors", [])
            author_str = ", ".join([a["name"] for a in authors[:5]]) + ("..." if len(authors) > 5 else "")

            return {
                "title": item.get("title", ""),
                "url": f"https://paperswithcode.com/paper/{item['id']}",
                "content": item.get("abstract", ""),
                "source": "Papers with Code",
                "author": author_str,
                "published_at": None,
                "category": "paper",
                "metadata": {
                    "stars": item.get("stars", 0),
                    "tasks": [t["name"] for t in item.get("tasks", [])],
                    "methods": [m["name"] for m in item.get("methods", [])],
                },
            }

        except Exception as e:
            logger.error(f"âŒ è§£æPWCè®ºæ–‡å¤±è´¥: {e}")
            return None
