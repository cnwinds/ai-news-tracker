"""
RSSæ•°æ®é‡‡é›†å™¨
"""
import feedparser
import requests
from datetime import datetime
from typing import List, Dict, Any
from urllib.parse import urljoin
import logging
from bs4 import BeautifulSoup
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)


class RSSCollector:
    """RSSé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_feed(self, url: str, max_articles: int = 20) -> List[Dict[str, Any]]:
        """
        ä»RSSæºè·å–æ–‡ç« 

        Args:
            url: RSS feed URL
            max_articles: æœ€å¤§æ–‡ç« æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}")

            # å‘é€è¯·æ±‚
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æRSS
            feed = feedparser.parse(response.content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # æå–æ–‡ç« ä¿¡æ¯
            articles = []
            for entry in feed.entries[:max_articles]:
                article = self._parse_entry(entry, feed.feed)
                if article:
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æRSSå¤±è´¥ {url}: {e}")
            return []

    def _parse_entry(self, entry: Any, feed_info: Any) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡æ–‡ç« 

        Args:
            entry: feedparser entry
            feed_info: feedä¿¡æ¯

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            # åŸºæœ¬å­—æ®µ
            title = entry.get("title", "æ— æ ‡é¢˜")
            url = entry.get("link", "")
            author = entry.get("author", "")

            # å‘å¸ƒæ—¶é—´
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published_at = datetime(*entry.updated_parsed[:6])

            # å†…å®¹æå–
            content = ""
            if hasattr(entry, "content") and entry.content:
                content = entry.content[0].value if isinstance(entry.content, list) else entry.content
            elif hasattr(entry, "summary"):
                content = entry.summary
            elif hasattr(entry, "description"):
                content = entry.description

            # æ¸…ç†HTMLæ ‡ç­¾
            content = self._clean_html(content)

            # æ¥æº
            source = feed_info.get("title", "Unknown")

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å¤±è´¥: {e}")
            return None

    def _clean_html(self, html: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬

        Args:
            html: HTMLå­—ç¬¦ä¸²

        Returns:
            çº¯æ–‡æœ¬
        """
        try:
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text(separator=" ", strip=True)
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç†HTMLå¤±è´¥: {e}")
            return html

    def fetch_full_content(self, url: str) -> str:
        """
        è·å–æ–‡ç« çš„å®Œæ•´é¡µé¢å†…å®¹

        Args:
            url: æ–‡ç« URL

        Returns:
            å®Œæ•´å†…å®¹æ–‡æœ¬
        """
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨è·å–å®Œæ•´å†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            # å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main article',
                '[role="article"]',
                '.blog-post-content',
            ]

            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
                    content = elements[0].get_text(separator=" ", strip=True)
                    if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        break

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–bodyå†…å®¹ï¼Œä½†ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰
            if not content or len(content) < 500:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    tag.decompose()
                content = soup.get_text(separator=" ", strip=True)

            # æ¸…ç†å¤šä½™ç©ºç™½
            content = " ".join(content.split())

            logger.info(f"âœ… æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æå®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return ""

    def fetch_multiple_feeds(self, feed_configs: List[Dict[str, Any]], max_workers: int = 5) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªRSSæºï¼ˆå¹¶å‘ï¼‰

        Args:
            feed_configs: RSSé…ç½®åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤5

        Returns:
            {source_name: {"articles": [articles], "feed_title": "feed title"}}
        """
        results = {}
        
        # è¿‡æ»¤å¯ç”¨çš„é…ç½®
        enabled_configs = [
            config for config in feed_configs 
            if config.get("enabled", True) and config.get("url")
        ]
        
        if not enabled_configs:
            logger.warning("âš ï¸  æ²¡æœ‰å¯ç”¨çš„RSSæº")
            return results
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(enabled_configs)} ä¸ªRSSæºï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_config = {
                executor.submit(self._fetch_single_feed_with_info, config): config 
                for config in enabled_configs
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_config):
                config = future_to_config[future]
                name = config.get("name", "Unknown")
                completed += 1
                
                try:
                    feed_result = future.result()
                    results[name] = feed_result
                    logger.info(f"âœ… [{completed}/{len(enabled_configs)}] {name}: è·å– {len(feed_result.get('articles', []))} ç¯‡æ–‡ç«  (feed title: {feed_result.get('feed_title', 'Unknown')})")
                except Exception as e:
                    logger.error(f"âŒ [{completed}/{len(enabled_configs)}] {name}: è·å–å¤±è´¥ - {e}")
                    results[name] = {"articles": [], "feed_title": None}
        
        logger.info(f"âœ… RSSæºè·å–å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in results.values() if r.get('articles')])}/{len(enabled_configs)}")
        return results
    
    def _fetch_single_feed_with_info(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆåŒ…å«feedä¿¡æ¯ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

        Args:
            config: RSSé…ç½®

        Returns:
            {"articles": [articles], "feed_title": "feed title"}
        """
        name = config.get("name", "Unknown")
        url = config.get("url")
        max_articles = config.get("max_articles", 20)
        
        if not url:
            logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
            return {"articles": [], "feed_title": None}
        
        try:
            # è·å–feedä¿¡æ¯ï¼ˆåªè¯·æ±‚ä¸€æ¬¡ï¼‰
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            # è·å–feed title
            feed_title = feed.feed.get("title", None) if hasattr(feed, 'feed') else None
            
            # æå–æ–‡ç« ä¿¡æ¯
            articles = []
            for entry in feed.entries[:max_articles]:
                article = self._parse_entry(entry, feed.feed)
                if article:
                    articles.append(article)
            
            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return {"articles": articles, "feed_title": feed_title}
            
        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return {"articles": [], "feed_title": None}
        except Exception as e:
            logger.error(f"âŒ è§£æRSSå¤±è´¥ {url}: {e}")
            return {"articles": [], "feed_title": None}
    
    def _fetch_single_feed(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

        Args:
            config: RSSé…ç½®

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        name = config.get("name", "Unknown")
        url = config.get("url")
        max_articles = config.get("max_articles", 20)
        
        if not url:
            logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
            return []
        
        return self.fetch_feed(url, max_articles)
