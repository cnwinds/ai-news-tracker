"""
RSSæ•°æ®é‡‡é›†å™¨
"""
import feedparser
import requests
from datetime import datetime
from typing import List, Dict, Any, Tuple
import logging
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

logger = logging.getLogger(__name__)


def _get_author_from_source(source_name: str = None, url: str = None) -> str:
    """
    æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…åç§°
    
    Args:
        source_name: è®¢é˜…æºåç§°
        url: æ–‡ç« URL
        
    Returns:
        ä½œè€…åç§°ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # æºåç§°åˆ°ä½œè€…çš„æ˜ å°„
    source_to_author = {
        "Paul Graham": "Paul Graham",
        "paulgraham.com": "Paul Graham",
        "Paul Graham's Essays": "Paul Graham",
    }
    
    # URLåˆ°ä½œè€…çš„æ˜ å°„
    url_to_author = {
        "paulgraham.com": "Paul Graham",
    }
    
    # é¦–å…ˆæ£€æŸ¥æºåç§°
    if source_name:
        # ç²¾ç¡®åŒ¹é…
        if source_name in source_to_author:
            return source_to_author[source_name]
        # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³é”®è¯ï¼‰
        for key, author in source_to_author.items():
            if key.lower() in source_name.lower():
                return author
    
    # ç„¶åæ£€æŸ¥URL
    if url:
        for key, author in url_to_author.items():
            if key in url.lower():
                return author
    
    return ""


class RSSCollector:
    """RSSé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_feed(self, url: str, max_articles: int = 20, source_name: str = None) -> List[Dict[str, Any]]:
        """
        ä»RSSæºè·å–æ–‡ç« 

        Args:
            url: RSS feed URL
            max_articles: æœ€å¤§æ–‡ç« æ•°
            source_name: è®¢é˜…æºåç§°ï¼ˆå°†ç”¨ä½œæ–‡ç« çš„sourceå­—æ®µï¼‰

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}")

            # å‘é€è¯·æ±‚
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æRSS
            feed = feedparser.parse(response.content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # æå–æ–‡ç« ä¿¡æ¯
            articles = []
            for entry in feed.entries[:max_articles]:
                article = self._parse_entry(entry, feed.feed, source_name=source_name)
                if article:
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æRSSå¤±è´¥ {url}: {e}")
            return []

    def _parse_entry(self, entry: Any, feed_info: Any, source_name: str = None) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡æ–‡ç« 

        Args:
            entry: feedparser entry
            feed_info: feedä¿¡æ¯
            source_name: è®¢é˜…æºåç§°ï¼ˆä¼˜å…ˆä½¿ç”¨æ­¤ä½œä¸ºsourceï¼Œè€Œä¸æ˜¯feed titleï¼‰

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            # åŸºæœ¬å­—æ®µ
            title = entry.get("title", "æ— æ ‡é¢˜")
            url = entry.get("link", "")
            author = entry.get("author", "")

            # å‘å¸ƒæ—¶é—´
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published_at = datetime(*entry.updated_parsed[:6])

            # å†…å®¹æå–
            content = ""
            if hasattr(entry, "content") and entry.content:
                content = entry.content[0].value if isinstance(entry.content, list) else entry.content
            elif hasattr(entry, "summary"):
                content = entry.summary
            elif hasattr(entry, "description"):
                content = entry.description

            # æ¸…ç†HTMLæ ‡ç­¾
            content = self._clean_html(content)

            # æ¥æºï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è®¢é˜…æºåç§°ï¼Œå¦åˆ™ä½¿ç”¨feed title
            source = source_name if source_name else feed_info.get("title", "Unknown")
            
            # æ ¹æ®æºåç§°æˆ–URLç¡®å®šæ­£ç¡®çš„ä½œè€…ï¼ˆå¦‚æœRSS feedä¸­çš„authorä¸å‡†ç¡®ï¼‰
            correct_author = _get_author_from_source(source_name, url)
            if correct_author:
                author = correct_author
            # å¦‚æœRSS feedä¸­æ²¡æœ‰authorï¼Œä½†å¯ä»¥æ ¹æ®æºåç§°ç¡®å®šï¼Œåˆ™ä½¿ç”¨ç¡®å®šçš„ä½œè€…
            elif not author and correct_author:
                author = correct_author

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å¤±è´¥: {e}")
            return None

    def _clean_html(self, html: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬

        Args:
            html: HTMLå­—ç¬¦ä¸²

        Returns:
            çº¯æ–‡æœ¬
        """
        try:
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text(separator=" ", strip=True)
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç†HTMLå¤±è´¥: {e}")
            return html

    def _extract_date_from_page(self, soup: BeautifulSoup, url: str) -> datetime or None:
        """
        ä»é¡µé¢HTMLä¸­æå–å‘å¸ƒæ—¥æœŸ

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: é¡µé¢URLï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        try:
            text = soup.get_text()

            month_names = [
                'January', 'February', 'March', 'April', 'May', 'June',
                'July', 'August', 'September', 'October', 'November', 'December'
            ]

            month_to_num = {
                'January': 1, 'February': 2, 'March': 3, 'April': 4,
                'May': 5, 'June': 6, 'July': 7, 'August': 8,
                'September': 9, 'October': 10, 'November': 11, 'December': 12
            }

            # Paul Grahamçš„æ—¥æœŸæ ¼å¼: "Month YYYY" (å¦‚ "October 2023")
            if 'paulgraham.com' in url:
                date_pattern = r'(' + '|'.join(month_names) + r')\s+(\d{4})'
                match = re.search(date_pattern, text, re.IGNORECASE)
                if match:
                    month_str = match.group(1)
                    year = int(match.group(2))
                    month_num = month_to_num.get(month_str.capitalize())
                    if month_num:
                        return datetime(year, month_num, 1)

            return None

        except Exception as e:
            logger.warning(f"âš ï¸  æå–æ—¥æœŸå¤±è´¥: {e}")
            return None

    def fetch_full_content(self, url: str) -> Tuple[str, datetime or None]:
        """
        è·å–æ–‡ç« çš„å®Œæ•´é¡µé¢å†…å®¹å’Œå‘å¸ƒæ—¥æœŸ

        Args:
            url: æ–‡ç« URL

        Returns:
            (å®Œæ•´å†…å®¹æ–‡æœ¬, å‘å¸ƒæ—¶é—´) çš„å…ƒç»„
        """
        try:
            logger.info(f"ğŸ“„ æ­£åœ¨è·å–å®Œæ•´å†…å®¹: {url}")
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.content, "html.parser")

            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            # å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main article',
                '[role="article"]',
                '.blog-post-content',
            ]

            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
                    content = elements[0].get_text(separator=" ", strip=True)
                    if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        break

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–bodyå†…å®¹ï¼Œä½†ç§»é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰
            if not content or len(content) < 500:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in soup.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    tag.decompose()
                content = soup.get_text(separator=" ", strip=True)

            # æ¸…ç†å¤šä½™ç©ºç™½
            content = " ".join(content.split())

            # å°è¯•ä»é¡µé¢æå–å‘å¸ƒæ—¥æœŸ
            published_at = self._extract_date_from_page(soup, url)

            logger.info(f"âœ… æˆåŠŸè·å–å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦" + (f"ï¼Œæ—¥æœŸ: {published_at}" if published_at else ""))
            return content, published_at

        except requests.RequestException as e:
            logger.warning(f"âš ï¸  è·å–å®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return "", None
        except Exception as e:
            logger.warning(f"âš ï¸  è§£æå®Œæ•´å†…å®¹å¤±è´¥ {url}: {e}")
            return "", None

    def fetch_single_feed(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆå…¬å¼€æ¥å£ï¼‰

        Args:
            config: RSSé…ç½®ï¼ŒåŒ…å« name, url, max_articles ç­‰å­—æ®µ

        Returns:
            {"articles": [articles], "feed_title": "feed title"}
        """
        return self._fetch_single_feed_with_info(config)

    def fetch_multiple_feeds(self, feed_configs: List[Dict[str, Any]], max_workers: int = 5) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªRSSæºï¼ˆå¹¶å‘ï¼‰

        Args:
            feed_configs: RSSé…ç½®åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤5

        Returns:
            {source_name: {"articles": [articles], "feed_title": "feed title"}}
        """
        results = {}
        
        # è¿‡æ»¤å¯ç”¨çš„é…ç½®
        enabled_configs = [
            config for config in feed_configs 
            if config.get("enabled", True) and config.get("url")
        ]
        
        if not enabled_configs:
            logger.warning("âš ï¸  æ²¡æœ‰å¯ç”¨çš„RSSæº")
            return results
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(enabled_configs)} ä¸ªRSSæºï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_config = {
                executor.submit(self._fetch_single_feed_with_info, config): config 
                for config in enabled_configs
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_config):
                config = future_to_config[future]
                name = config.get("name", "Unknown")
                completed += 1
                
                try:
                    feed_result = future.result()
                    results[name] = feed_result
                    logger.info(f"âœ… [{completed}/{len(enabled_configs)}] {name}: è·å– {len(feed_result.get('articles', []))} ç¯‡æ–‡ç«  (feed title: {feed_result.get('feed_title', 'Unknown')})")
                except Exception as e:
                    logger.error(f"âŒ [{completed}/{len(enabled_configs)}] {name}: è·å–å¤±è´¥ - {e}")
                    results[name] = {"articles": [], "feed_title": None}
        
        logger.info(f"âœ… RSSæºè·å–å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in results.values() if r.get('articles')])}/{len(enabled_configs)}")
        return results
    
    def _fetch_single_feed_with_info(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆåŒ…å«feedä¿¡æ¯ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

        Args:
            config: RSSé…ç½®

        Returns:
            {"articles": [articles], "feed_title": "feed title"}
        """
        name = config.get("name", "Unknown")
        url = config.get("url")
        max_articles = config.get("max_articles", 20)

        if not url:
            logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
            return {"articles": [], "feed_title": None}

        try:
            # è·å–feedä¿¡æ¯ï¼ˆåªè¯·æ±‚ä¸€æ¬¡ï¼‰
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æRSS
            feed = feedparser.parse(response.content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # è·å–feed title
            feed_title = feed.feed.get("title", None) if hasattr(feed, 'feed') else None

            # æå–æ–‡ç« ä¿¡æ¯ï¼ˆä½¿ç”¨è®¢é˜…æºåç§°ä½œä¸ºsourceï¼‰
            # æ³¨æ„ï¼šæ¯ä¸ªçº¿ç¨‹éƒ½ä¼šåˆ›å»ºç‹¬ç«‹çš„feedå¯¹è±¡ï¼Œä¸ä¼šå…±äº«
            articles = []
            for entry in feed.entries[:max_articles]:
                # ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„source_nameï¼Œé˜²æ­¢å¹¶å‘æ—¶ä½¿ç”¨é”™è¯¯çš„åç§°
                article = self._parse_entry(entry, feed.feed, source_name=name)
                if article:
                    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿articleçš„sourceå­—æ®µä¸ä¼ å…¥çš„nameä¸€è‡´
                    if article.get("source") != name:
                        logger.warning(f"  âš ï¸  RSSè§£ææ—¶sourceä¸åŒ¹é…: æœŸæœ›={name}, å®é™…={article.get('source')}, URL={article.get('url', '')[:50]}")
                        article["source"] = name  # å¼ºåˆ¶ä¿®æ­£
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return {"articles": articles, "feed_title": feed_title}

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return {"articles": [], "feed_title": None}
        except Exception as e:
            logger.error(f"âŒ è§£æRSSå¤±è´¥ {url}: {e}")
            return {"articles": [], "feed_title": None}
    
    def _fetch_single_feed(self, config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–å•ä¸ªRSSæºï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

        Args:
            config: RSSé…ç½®

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        name = config.get("name", "Unknown")
        url = config.get("url")
        max_articles = config.get("max_articles", 20)

        if not url:
            logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
            return []

        return self.fetch_feed(url, max_articles, source_name=name)
