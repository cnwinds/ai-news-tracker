"""
RSSæ•°æ®é‡‡é›†å™¨
"""
import feedparser
import requests
from datetime import datetime
from typing import List, Dict, Any
from urllib.parse import urljoin
import logging
from bs4 import BeautifulSoup
from time import sleep

logger = logging.getLogger(__name__)


class RSSCollector:
    """RSSé‡‡é›†å™¨"""

    def __init__(self, timeout: int = 30, user_agent: str = None):
        self.timeout = timeout
        self.user_agent = user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

    def fetch_feed(self, url: str, max_articles: int = 20) -> List[Dict[str, Any]]:
        """
        ä»RSSæºè·å–æ–‡ç« 

        Args:
            url: RSS feed URL
            max_articles: æœ€å¤§æ–‡ç« æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}")

            # å‘é€è¯·æ±‚
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # è§£æRSS
            feed = feedparser.parse(response.content)

            if feed.bozo:
                logger.warning(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            # æå–æ–‡ç« ä¿¡æ¯
            articles = []
            for entry in feed.entries[:max_articles]:
                article = self._parse_entry(entry, feed.feed)
                if article:
                    articles.append(article)

            logger.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç«  from {url}")
            return articles

        except requests.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥ {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æRSSå¤±è´¥ {url}: {e}")
            return []

    def _parse_entry(self, entry: Any, feed_info: Any) -> Dict[str, Any]:
        """
        è§£æå•ç¯‡æ–‡ç« 

        Args:
            entry: feedparser entry
            feed_info: feedä¿¡æ¯

        Returns:
            æ–‡ç« å­—å…¸
        """
        try:
            # åŸºæœ¬å­—æ®µ
            title = entry.get("title", "æ— æ ‡é¢˜")
            url = entry.get("link", "")
            author = entry.get("author", "")

            # å‘å¸ƒæ—¶é—´
            published_at = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published_at = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published_at = datetime(*entry.updated_parsed[:6])

            # å†…å®¹æå–
            content = ""
            if hasattr(entry, "content") and entry.content:
                content = entry.content[0].value if isinstance(entry.content, list) else entry.content
            elif hasattr(entry, "summary"):
                content = entry.summary
            elif hasattr(entry, "description"):
                content = entry.description

            # æ¸…ç†HTMLæ ‡ç­¾
            content = self._clean_html(content)

            # æ¥æº
            source = feed_info.get("title", "Unknown")

            return {
                "title": title,
                "url": url,
                "content": content,
                "source": source,
                "author": author,
                "published_at": published_at,
                "category": "rss",
            }

        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« å¤±è´¥: {e}")
            return None

    def _clean_html(self, html: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬

        Args:
            html: HTMLå­—ç¬¦ä¸²

        Returns:
            çº¯æ–‡æœ¬
        """
        try:
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text(separator=" ", strip=True)
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç†HTMLå¤±è´¥: {e}")
            return html

    def fetch_multiple_feeds(self, feed_configs: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªRSSæº

        Args:
            feed_configs: RSSé…ç½®åˆ—è¡¨

        Returns:
            {source_name: [articles]}
        """
        results = {}

        for config in feed_configs:
            if not config.get("enabled", True):
                continue

            name = config.get("name", "Unknown")
            url = config.get("url")
            max_articles = config.get("max_articles", 20)

            if not url:
                logger.warning(f"âš ï¸  {name} æ²¡æœ‰é…ç½®URL")
                continue

            articles = self.fetch_feed(url, max_articles)
            results[name] = articles

            # é¿å…è¯·æ±‚è¿‡å¿«
            sleep(1)

        return results
