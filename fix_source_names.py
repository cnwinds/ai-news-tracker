"""
ä¿®å¤æ•°æ®åº“ä¸­æ–‡ç« çš„sourceå­—æ®µï¼Œä½¿å…¶ä¸RSSè®¢é˜…æºåç§°åŒ¹é…
"""
import sys
import os
from pathlib import Path

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8ï¼ˆWindowsï¼‰
if sys.platform == 'win32':
    os.system('chcp 65001 >nul 2>&1')
    sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from database import get_db
from database.models import Article, RSSSource
from sqlalchemy import func

def fix_source_names():
    """ä¿®å¤æ–‡ç« çš„sourceå­—æ®µï¼Œä½¿å…¶ä¸è®¢é˜…æºåç§°åŒ¹é…"""
    print("=" * 60)
    print("ğŸ”§ å¼€å§‹ä¿®å¤æ–‡ç« sourceå­—æ®µ...")
    print("=" * 60)
    
    db = get_db()
    
    with db.get_session() as session:
        # è·å–æ‰€æœ‰è®¢é˜…æº
        sources = session.query(RSSSource).all()
        source_url_map = {source.url: source.name for source in sources}
        
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(sources)} ä¸ªè®¢é˜…æº")
        
        # è·å–æ‰€æœ‰æ–‡ç« 
        articles = session.query(Article).all()
        print(f"ğŸ“° æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # ç»Ÿè®¡éœ€è¦ä¿®å¤çš„æ–‡ç« 
        fixed_count = 0
        not_found_count = 0
        
        # æ„å»ºURLåˆ°sourceåç§°çš„æ˜ å°„ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
        url_to_source = {}
        for source in sources:
            if source.url:
                try:
                    from urllib.parse import urlparse
                    domain = urlparse(source.url).netloc
                    # ç§»é™¤wwwå‰ç¼€
                    if domain.startswith('www.'):
                        domain = domain[4:]
                    url_to_source[domain] = source.name
                    # ä¹Ÿä¿å­˜å®Œæ•´URL
                    url_to_source[source.url] = source.name
                except:
                    pass
        
        for article in articles:
            matched_source = None
            
            # æ–¹æ³•1: é€šè¿‡æ–‡ç« URLçš„åŸŸååŒ¹é…è®¢é˜…æº
            if article.url:
                try:
                    from urllib.parse import urlparse
                    article_domain = urlparse(article.url).netloc
                    # ç§»é™¤wwwå‰ç¼€
                    if article_domain.startswith('www.'):
                        article_domain = article_domain[4:]
                    
                    # ç²¾ç¡®åŒ¹é…åŸŸå
                    if article_domain in url_to_source:
                        matched_source = url_to_source[article_domain]
                    else:
                        # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ aws.amazon.com åŒ¹é… amazon.comï¼‰
                        for domain, source_name in url_to_source.items():
                            if '.' in domain and (domain in article_domain or article_domain in domain):
                                matched_source = source_name
                                break
                except Exception as e:
                    pass
            
            # æ–¹æ³•2: é€šè¿‡sourceå­—æ®µåŒ¹é…ï¼ˆå¤„ç†å·²çŸ¥çš„ä¸åŒ¹é…æƒ…å†µï¼‰
            if not matched_source and article.source:
                article_source_lower = article.source.lower().strip()
                
                # å·²çŸ¥çš„æ˜ å°„å…³ç³»ï¼ˆRSS feed title -> è®¢é˜…æºåç§°ï¼‰
                known_mappings = {
                    'artificial intelligence': 'AWS Machine Learning',
                    'aws machine learning blog': 'AWS Machine Learning',
                    'aws machine learning': 'AWS Machine Learning',
                    'openai news': 'OpenAI',
                    'openai blog': 'OpenAI',
                }
                
                if article_source_lower in known_mappings:
                    matched_source = known_mappings[article_source_lower]
                else:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…
                    for source in sources:
                        source_name_lower = source.name.lower().strip()
                        # å®Œå…¨åŒ¹é…
                        if article_source_lower == source_name_lower:
                            matched_source = source.name
                            break
                        # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚æœsourceåç§°åŒ…å«åœ¨æ–‡ç« sourceä¸­ï¼Œæˆ–ç›¸åï¼‰
                        elif (source_name_lower in article_source_lower or 
                              article_source_lower in source_name_lower):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆç†çš„åŒ¹é…ï¼ˆé¿å…è¯¯åŒ¹é…ï¼‰
                            if len(source_name_lower) > 3 and len(article_source_lower) > 3:
                                matched_source = source.name
                                break
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è®¢é˜…æºï¼Œæ›´æ–°sourceå­—æ®µ
            if matched_source and article.source != matched_source:
                old_source = article.source
                article.source = matched_source
                fixed_count += 1
                if fixed_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"  âœ… ä¿®å¤: {article.title[:50]}...")
                    print(f"     æ—§source: {old_source}")
                    print(f"     æ–°source: {matched_source}")
            elif not matched_source:
                not_found_count += 1
                if not_found_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  âš ï¸  æœªæ‰¾åˆ°åŒ¹é…: {article.title[:50]}... (source: {article.source}, url: {article.url[:50] if article.url else 'N/A'})")
        
        # æäº¤æ›´æ”¹
        session.commit()
        
        print("\n" + "=" * 60)
        print(f"âœ… ä¿®å¤å®Œæˆï¼")
        print(f"   ä¿®å¤æ–‡ç« æ•°: {fixed_count}")
        print(f"   æœªæ‰¾åˆ°åŒ¹é…: {not_found_count}")
        print("=" * 60)

if __name__ == "__main__":
    try:
        fix_source_names()
    except Exception as e:
        print(f"\nâŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
