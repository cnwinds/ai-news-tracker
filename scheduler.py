"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - ä½¿ç”¨APScheduler
"""
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from datetime import datetime
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from collector import CollectionService
from analyzer.ai_analyzer import AIAnalyzer
from notification import NotificationService
from database import get_db
from dotenv import load_dotenv
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = logging.getLogger(__name__)


class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self):
        self.scheduler = BlockingScheduler()

        # åˆå§‹åŒ–æœåŠ¡
        self._init_services()

    def _init_services(self):
        """åˆå§‹åŒ–å„ä¸ªæœåŠ¡"""
        # AIåˆ†æå™¨
        if os.getenv("OPENAI_API_KEY"):
            self.ai_analyzer = AIAnalyzer(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1"),
                model=os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview"),
            )
            logger.info("âœ… AIåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.ai_analyzer = None
            logger.warning("âš ï¸  æœªé…ç½®OPENAI_API_KEYï¼ŒAIåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

        # é‡‡é›†æœåŠ¡
        self.collector = CollectionService(ai_analyzer=self.ai_analyzer)
        logger.info("âœ… é‡‡é›†æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        # é€šçŸ¥æœåŠ¡
        feishu_webhook = os.getenv("FEISHU_BOT_WEBHOOK")
        if feishu_webhook:
            self.notifier = NotificationService(feishu_webhook=feishu_webhook)
            logger.info("âœ… é€šçŸ¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.notifier = None
            logger.warning("âš ï¸  æœªé…ç½®FEISHU_BOT_WEBHOOKï¼Œæ¨é€åŠŸèƒ½å°†ä¸å¯ç”¨")

        # æ•°æ®åº“
        self.db = get_db()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")

    def add_collection_job(self, cron_expression: str = "0 */1 * * *"):
        """
        æ·»åŠ å®šæ—¶é‡‡é›†ä»»åŠ¡

        Args:
            cron_expression: cronè¡¨è¾¾å¼ï¼Œé»˜è®¤æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        """
        try:
            # è§£æcronè¡¨è¾¾å¼
            # æ ¼å¼: åˆ† æ—¶ æ—¥ æœˆ å‘¨
            parts = cron_expression.split()
            if len(parts) != 5:
                raise ValueError(f"æ— æ•ˆçš„cronè¡¨è¾¾å¼: {cron_expression}")

            self.scheduler.add_job(
                func=self._run_collection,
                trigger=CronTrigger.from_crontab(cron_expression),
                id="collection_job",
                name="å®šæ—¶æ•°æ®é‡‡é›†",
                replace_existing=True,
            )

            logger.info(f"âœ… å®šæ—¶é‡‡é›†ä»»åŠ¡å·²æ·»åŠ : {cron_expression}")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®šæ—¶é‡‡é›†ä»»åŠ¡å¤±è´¥: {e}")

    def add_daily_summary_job(self, cron_expression: str = "0 9 * * *"):
        """
        æ·»åŠ æ¯æ—¥æ‘˜è¦ä»»åŠ¡

        Args:
            cron_expression: cronè¡¨è¾¾å¼ï¼Œé»˜è®¤æ¯å¤©9ç‚¹æ‰§è¡Œ
        """
        try:
            self.scheduler.add_job(
                func=self._run_daily_summary,
                trigger=CronTrigger.from_crontab(cron_expression),
                id="daily_summary_job",
                name="æ¯æ—¥æ‘˜è¦æ¨é€",
                replace_existing=True,
            )

            logger.info(f"âœ… æ¯æ—¥æ‘˜è¦ä»»åŠ¡å·²æ·»åŠ : {cron_expression}")

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ¯æ—¥æ‘˜è¦ä»»åŠ¡å¤±è´¥: {e}")

    def _run_collection(self):
        """æ‰§è¡Œé‡‡é›†ä»»åŠ¡"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®šæ—¶é‡‡é›†ä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")

            stats = self.collector.collect_all(enable_ai_analysis=True)

            logger.info(f"âœ… é‡‡é›†å®Œæˆ:")
            logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
            logger.info(f"   æ–°å¢æ–‡ç« : {stats['new_articles']}")
            logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")
            logger.info("=" * 60)

            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜é‡è¦æ€§æ–‡ç« éœ€è¦å³æ—¶æ¨é€
            if self.notifier:
                self._send_instant_alerts()

        except Exception as e:
            logger.error(f"âŒ é‡‡é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

    def _run_daily_summary(self):
        """æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“ å¼€å§‹æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡")
            logger.info(f"â° æ—¶é—´: {datetime.now()}")

            if not self.ai_analyzer:
                logger.warning("âš ï¸  AIåˆ†æå™¨æœªé…ç½®ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
                return

            if not self.notifier:
                logger.warning("âš ï¸  é€šçŸ¥æœåŠ¡æœªé…ç½®ï¼Œè·³è¿‡æ¨é€")
                return

            # è·å–é‡è¦æ–‡ç« 
            articles = self.collector.get_daily_summary(self.db, limit=20)

            if not articles:
                logger.info("ğŸ“­ ä»Šæ—¥æš‚æ— é‡è¦æ–‡ç« ")
                return

            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(articles)} ç¯‡é‡è¦æ–‡ç« ")

            # å‡†å¤‡æ–‡ç« æ•°æ®
            articles_data = []
            for article in articles:
                articles_data.append(
                    {
                        "title": article.title,
                        "content": article.content,
                        "source": article.source,
                        "published_at": article.published_at,
                        "summary": article.summary,
                        "importance": article.importance,
                    }
                )

            # ç”Ÿæˆæ‘˜è¦
            summary = self.ai_analyzer.generate_daily_summary(articles_data, max_count=15)

            logger.info("ğŸ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            logger.info(f"\n{summary}\n")

            # æ¨é€åˆ°é£ä¹¦
            success = self.notifier.send_daily_summary(summary, self.db, limit=20)

            if success:
                logger.info("âœ… æ¯æ—¥æ‘˜è¦æ¨é€æˆåŠŸ")
            else:
                logger.error("âŒ æ¯æ—¥æ‘˜è¦æ¨é€å¤±è´¥")

            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥æ‘˜è¦ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

    def _send_instant_alerts(self):
        """å‘é€å³æ—¶æé†’ï¼ˆé«˜é‡è¦æ€§æ–‡ç« ï¼‰"""
        try:
            from datetime import timedelta

            with self.db.get_session() as session:
                # è·å–æœ€è¿‘1å°æ—¶çš„é«˜é‡è¦æ€§æ–‡ç« ä¸”æœªæ¨é€çš„
                time_threshold = datetime.now() - timedelta(hours=1)

                articles = (
                    session.query(Article)
                    .filter(Article.published_at >= time_threshold, Article.importance == "high", Article.is_sent == False)
                    .all()
                )

                if not articles:
                    return

                logger.info(f"ğŸš¨ å‘ç° {len(articles)} ç¯‡é«˜é‡è¦æ€§æ–‡ç« ï¼Œå‡†å¤‡æ¨é€")

                for article in articles:
                    success = self.notifier.send_instant_alert(article)

                    if success:
                        article.is_sent = True
                        logger.info(f"âœ… å·²æ¨é€: {article.title[:50]}...")
                    else:
                        logger.error(f"âŒ æ¨é€å¤±è´¥: {article.title[:50]}...")

                session.commit()

        except Exception as e:
            logger.error(f"âŒ å‘é€å³æ—¶æé†’å¤±è´¥: {e}")

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        try:
            logger.info("ğŸš€ ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨ä¸­...")
            logger.info(f"ğŸ“… å½“å‰æ—¶é—´: {datetime.now()}")

            # æ·»åŠ ä»»åŠ¡
            collection_cron = os.getenv("COLLECTION_CRON", "0 */1 * * *")  # é»˜è®¤æ¯å°æ—¶
            summary_cron = os.getenv("DAILY_SUMMARY_CRON", "0 9 * * *")  # é»˜è®¤æ¯å¤©9ç‚¹

            self.add_collection_job(collection_cron)
            self.add_daily_summary_job(summary_cron)

            # æ˜¾ç¤ºå³å°†æ‰§è¡Œçš„ä»»åŠ¡
            self.scheduler.print_jobs()

            logger.info("âœ… ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")

            # å¯åŠ¨è°ƒåº¦å™¨
            self.scheduler.start()

        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸  æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­è°ƒåº¦å™¨...")
            self.scheduler.shutdown()
            logger.info("âœ… è°ƒåº¦å™¨å·²åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}")
            self.scheduler.shutdown()


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(), logging.FileHandler("logs/scheduler.log", encoding="utf-8")],
    )

    logger.info("=" * 60)
    logger.info("ğŸ¤– AI News Tracker - ä»»åŠ¡è°ƒåº¦å™¨")
    logger.info("=" * 60)

    # åˆ›å»ºå¹¶å¯åŠ¨è°ƒåº¦å™¨
    scheduler = TaskScheduler()
    scheduler.start()


if __name__ == "__main__":
    main()
